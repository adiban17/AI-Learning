{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47def858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e93dbc25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>car_name</th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>vehicle_age</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>seller_type</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>transmission_type</th>\n",
       "      <th>mileage</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "      <th>seats</th>\n",
       "      <th>selling_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Maruti Alto</td>\n",
       "      <td>Maruti</td>\n",
       "      <td>Alto</td>\n",
       "      <td>9</td>\n",
       "      <td>120000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>19.70</td>\n",
       "      <td>796</td>\n",
       "      <td>46.30</td>\n",
       "      <td>5</td>\n",
       "      <td>120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Hyundai Grand</td>\n",
       "      <td>Hyundai</td>\n",
       "      <td>Grand</td>\n",
       "      <td>5</td>\n",
       "      <td>20000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>18.90</td>\n",
       "      <td>1197</td>\n",
       "      <td>82.00</td>\n",
       "      <td>5</td>\n",
       "      <td>550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hyundai i20</td>\n",
       "      <td>Hyundai</td>\n",
       "      <td>i20</td>\n",
       "      <td>11</td>\n",
       "      <td>60000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>17.00</td>\n",
       "      <td>1197</td>\n",
       "      <td>80.00</td>\n",
       "      <td>5</td>\n",
       "      <td>215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Maruti Alto</td>\n",
       "      <td>Maruti</td>\n",
       "      <td>Alto</td>\n",
       "      <td>9</td>\n",
       "      <td>37000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>20.92</td>\n",
       "      <td>998</td>\n",
       "      <td>67.10</td>\n",
       "      <td>5</td>\n",
       "      <td>226000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Ford Ecosport</td>\n",
       "      <td>Ford</td>\n",
       "      <td>Ecosport</td>\n",
       "      <td>6</td>\n",
       "      <td>30000</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Manual</td>\n",
       "      <td>22.77</td>\n",
       "      <td>1498</td>\n",
       "      <td>98.59</td>\n",
       "      <td>5</td>\n",
       "      <td>570000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       car_name    brand     model  vehicle_age  km_driven  \\\n",
       "0           0    Maruti Alto   Maruti      Alto            9     120000   \n",
       "1           1  Hyundai Grand  Hyundai     Grand            5      20000   \n",
       "2           2    Hyundai i20  Hyundai       i20           11      60000   \n",
       "3           3    Maruti Alto   Maruti      Alto            9      37000   \n",
       "4           4  Ford Ecosport     Ford  Ecosport            6      30000   \n",
       "\n",
       "  seller_type fuel_type transmission_type  mileage  engine  max_power  seats  \\\n",
       "0  Individual    Petrol            Manual    19.70     796      46.30      5   \n",
       "1  Individual    Petrol            Manual    18.90    1197      82.00      5   \n",
       "2  Individual    Petrol            Manual    17.00    1197      80.00      5   \n",
       "3  Individual    Petrol            Manual    20.92     998      67.10      5   \n",
       "4      Dealer    Diesel            Manual    22.77    1498      98.59      5   \n",
       "\n",
       "   selling_price  \n",
       "0         120000  \n",
       "1         550000  \n",
       "2         215000  \n",
       "3         226000  \n",
       "4         570000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing our data\n",
    "\n",
    "df=pd.read_csv('cardekho_imputated.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86f3bbe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car_name</th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>vehicle_age</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>seller_type</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>transmission_type</th>\n",
       "      <th>mileage</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "      <th>seats</th>\n",
       "      <th>selling_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maruti Alto</td>\n",
       "      <td>Maruti</td>\n",
       "      <td>Alto</td>\n",
       "      <td>9</td>\n",
       "      <td>120000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>19.70</td>\n",
       "      <td>796</td>\n",
       "      <td>46.30</td>\n",
       "      <td>5</td>\n",
       "      <td>120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hyundai Grand</td>\n",
       "      <td>Hyundai</td>\n",
       "      <td>Grand</td>\n",
       "      <td>5</td>\n",
       "      <td>20000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>18.90</td>\n",
       "      <td>1197</td>\n",
       "      <td>82.00</td>\n",
       "      <td>5</td>\n",
       "      <td>550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hyundai i20</td>\n",
       "      <td>Hyundai</td>\n",
       "      <td>i20</td>\n",
       "      <td>11</td>\n",
       "      <td>60000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>17.00</td>\n",
       "      <td>1197</td>\n",
       "      <td>80.00</td>\n",
       "      <td>5</td>\n",
       "      <td>215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Maruti Alto</td>\n",
       "      <td>Maruti</td>\n",
       "      <td>Alto</td>\n",
       "      <td>9</td>\n",
       "      <td>37000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>20.92</td>\n",
       "      <td>998</td>\n",
       "      <td>67.10</td>\n",
       "      <td>5</td>\n",
       "      <td>226000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ford Ecosport</td>\n",
       "      <td>Ford</td>\n",
       "      <td>Ecosport</td>\n",
       "      <td>6</td>\n",
       "      <td>30000</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Manual</td>\n",
       "      <td>22.77</td>\n",
       "      <td>1498</td>\n",
       "      <td>98.59</td>\n",
       "      <td>5</td>\n",
       "      <td>570000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        car_name    brand     model  vehicle_age  km_driven seller_type  \\\n",
       "0    Maruti Alto   Maruti      Alto            9     120000  Individual   \n",
       "1  Hyundai Grand  Hyundai     Grand            5      20000  Individual   \n",
       "2    Hyundai i20  Hyundai       i20           11      60000  Individual   \n",
       "3    Maruti Alto   Maruti      Alto            9      37000  Individual   \n",
       "4  Ford Ecosport     Ford  Ecosport            6      30000      Dealer   \n",
       "\n",
       "  fuel_type transmission_type  mileage  engine  max_power  seats  \\\n",
       "0    Petrol            Manual    19.70     796      46.30      5   \n",
       "1    Petrol            Manual    18.90    1197      82.00      5   \n",
       "2    Petrol            Manual    17.00    1197      80.00      5   \n",
       "3    Petrol            Manual    20.92     998      67.10      5   \n",
       "4    Diesel            Manual    22.77    1498      98.59      5   \n",
       "\n",
       "   selling_price  \n",
       "0         120000  \n",
       "1         550000  \n",
       "2         215000  \n",
       "3         226000  \n",
       "4         570000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing the 'Unnamed: 0' column\n",
    "\n",
    "df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1171ef42",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cee1998",
   "metadata": {},
   "source": [
    "Data Cleaning:\n",
    "- Handling Missing Values\n",
    "- Handling Duplicates\n",
    "- Check Data Type\n",
    "- Understand the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b697a05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "car_name             0\n",
       "brand                0\n",
       "model                0\n",
       "vehicle_age          0\n",
       "km_driven            0\n",
       "seller_type          0\n",
       "fuel_type            0\n",
       "transmission_type    0\n",
       "mileage              0\n",
       "engine               0\n",
       "max_power            0\n",
       "seats                0\n",
       "selling_price        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for null values\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da1d985f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>vehicle_age</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>seller_type</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>transmission_type</th>\n",
       "      <th>mileage</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "      <th>seats</th>\n",
       "      <th>selling_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alto</td>\n",
       "      <td>9</td>\n",
       "      <td>120000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>19.70</td>\n",
       "      <td>796</td>\n",
       "      <td>46.30</td>\n",
       "      <td>5</td>\n",
       "      <td>120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Grand</td>\n",
       "      <td>5</td>\n",
       "      <td>20000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>18.90</td>\n",
       "      <td>1197</td>\n",
       "      <td>82.00</td>\n",
       "      <td>5</td>\n",
       "      <td>550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i20</td>\n",
       "      <td>11</td>\n",
       "      <td>60000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>17.00</td>\n",
       "      <td>1197</td>\n",
       "      <td>80.00</td>\n",
       "      <td>5</td>\n",
       "      <td>215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alto</td>\n",
       "      <td>9</td>\n",
       "      <td>37000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>20.92</td>\n",
       "      <td>998</td>\n",
       "      <td>67.10</td>\n",
       "      <td>5</td>\n",
       "      <td>226000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ecosport</td>\n",
       "      <td>6</td>\n",
       "      <td>30000</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Manual</td>\n",
       "      <td>22.77</td>\n",
       "      <td>1498</td>\n",
       "      <td>98.59</td>\n",
       "      <td>5</td>\n",
       "      <td>570000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  vehicle_age  km_driven seller_type fuel_type transmission_type  \\\n",
       "0      Alto            9     120000  Individual    Petrol            Manual   \n",
       "1     Grand            5      20000  Individual    Petrol            Manual   \n",
       "2       i20           11      60000  Individual    Petrol            Manual   \n",
       "3      Alto            9      37000  Individual    Petrol            Manual   \n",
       "4  Ecosport            6      30000      Dealer    Diesel            Manual   \n",
       "\n",
       "   mileage  engine  max_power  seats  selling_price  \n",
       "0    19.70     796      46.30      5         120000  \n",
       "1    18.90    1197      82.00      5         550000  \n",
       "2    17.00    1197      80.00      5         215000  \n",
       "3    20.92     998      67.10      5         226000  \n",
       "4    22.77    1498      98.59      5         570000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing the 'car_name' and 'brand' columns as they are pretty much unnecessary\n",
    "\n",
    "df.drop(columns=['car_name','brand'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f89412f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Alto', 'Grand', 'i20', 'Ecosport', 'Wagon R', 'i10', 'Venue',\n",
       "       'Swift', 'Verna', 'Duster', 'Cooper', 'Ciaz', 'C-Class', 'Innova',\n",
       "       'Baleno', 'Swift Dzire', 'Vento', 'Creta', 'City', 'Bolero',\n",
       "       'Fortuner', 'KWID', 'Amaze', 'Santro', 'XUV500', 'KUV100', 'Ignis',\n",
       "       'RediGO', 'Scorpio', 'Marazzo', 'Aspire', 'Figo', 'Vitara',\n",
       "       'Tiago', 'Polo', 'Seltos', 'Celerio', 'GO', '5', 'CR-V',\n",
       "       'Endeavour', 'KUV', 'Jazz', '3', 'A4', 'Tigor', 'Ertiga', 'Safari',\n",
       "       'Thar', 'Hexa', 'Rover', 'Eeco', 'A6', 'E-Class', 'Q7', 'Z4', '6',\n",
       "       'XF', 'X5', 'Hector', 'Civic', 'D-Max', 'Cayenne', 'X1', 'Rapid',\n",
       "       'Freestyle', 'Superb', 'Nexon', 'XUV300', 'Dzire VXI', 'S90',\n",
       "       'WR-V', 'XL6', 'Triber', 'ES', 'Wrangler', 'Camry', 'Elantra',\n",
       "       'Yaris', 'GL-Class', '7', 'S-Presso', 'Dzire LXI', 'Aura', 'XC',\n",
       "       'Ghibli', 'Continental', 'CR', 'Kicks', 'S-Class', 'Tucson',\n",
       "       'Harrier', 'X3', 'Octavia', 'Compass', 'CLS', 'redi-GO', 'Glanza',\n",
       "       'Macan', 'X4', 'Dzire ZXI', 'XC90', 'F-PACE', 'A8', 'MUX',\n",
       "       'GTC4Lusso', 'GLS', 'X-Trail', 'XE', 'XC60', 'Panamera', 'Alturas',\n",
       "       'Altroz', 'NX', 'Carnival', 'C', 'RX', 'Ghost', 'Quattroporte',\n",
       "       'Gurkha'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['model'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27837a92",
   "metadata": {},
   "source": [
    "#### Dofferent Types of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e02e26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Numeric Features : 7\n",
      "Numeric Features are :\n",
      "['vehicle_age', 'km_driven', 'mileage', 'engine', 'max_power', 'seats', 'selling_price']\n"
     ]
    }
   ],
   "source": [
    "# Numeric Features\n",
    "\n",
    "numeric_features=[feature for feature in df.columns if df[feature].dtype!='O']\n",
    "print(f'Number of Numeric Features : {len(numeric_features)}')\n",
    "print(f'Numeric Features are :\\n{numeric_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f01215a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Categorical Features : 4\n",
      "Categorical Features are :\n",
      "['model', 'seller_type', 'fuel_type', 'transmission_type']\n"
     ]
    }
   ],
   "source": [
    "# Categorical Features\n",
    "\n",
    "categorical_features=[feature for feature in df.columns if df[feature].dtype=='O']\n",
    "print(f\"Number of Categorical Features : {len(categorical_features)}\")\n",
    "print(f\"Categorical Features are :\\n{categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b9aa1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Discrete Features is 2\n",
      "Discrete Features are:\n",
      "['vehicle_age', 'seats']\n"
     ]
    }
   ],
   "source": [
    "# Discrete Features\n",
    "\n",
    "discrete_features=[feature for feature in numeric_features if len(df[feature].unique())<=25]\n",
    "print(f\"Number of Discrete Features is {len(discrete_features)}\")\n",
    "print(f\"Discrete Features are:\\n{discrete_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bc2c135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Continuous Features : 5\n",
      "Continuous Features are \n",
      "['km_driven', 'mileage', 'engine', 'max_power', 'selling_price']\n"
     ]
    }
   ],
   "source": [
    "# Continuous Features\n",
    "\n",
    "continuous_features=[feature for feature in numeric_features if len(df[feature].unique())>25]\n",
    "print(f\"Number of Continuous Features : {len(continuous_features)}\")\n",
    "print(f\"Continuous Features are \\n{continuous_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a26e1d",
   "metadata": {},
   "source": [
    "#### Dividing our Data into Independent and Dependent Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f3ff096",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop(columns=['selling_price'], axis=1)\n",
    "y=df['selling_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b2be654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>vehicle_age</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>seller_type</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>transmission_type</th>\n",
       "      <th>mileage</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "      <th>seats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alto</td>\n",
       "      <td>9</td>\n",
       "      <td>120000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>19.70</td>\n",
       "      <td>796</td>\n",
       "      <td>46.30</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Grand</td>\n",
       "      <td>5</td>\n",
       "      <td>20000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>18.90</td>\n",
       "      <td>1197</td>\n",
       "      <td>82.00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i20</td>\n",
       "      <td>11</td>\n",
       "      <td>60000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>17.00</td>\n",
       "      <td>1197</td>\n",
       "      <td>80.00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alto</td>\n",
       "      <td>9</td>\n",
       "      <td>37000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>20.92</td>\n",
       "      <td>998</td>\n",
       "      <td>67.10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ecosport</td>\n",
       "      <td>6</td>\n",
       "      <td>30000</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Manual</td>\n",
       "      <td>22.77</td>\n",
       "      <td>1498</td>\n",
       "      <td>98.59</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  vehicle_age  km_driven seller_type fuel_type transmission_type  \\\n",
       "0      Alto            9     120000  Individual    Petrol            Manual   \n",
       "1     Grand            5      20000  Individual    Petrol            Manual   \n",
       "2       i20           11      60000  Individual    Petrol            Manual   \n",
       "3      Alto            9      37000  Individual    Petrol            Manual   \n",
       "4  Ecosport            6      30000      Dealer    Diesel            Manual   \n",
       "\n",
       "   mileage  engine  max_power  seats  \n",
       "0    19.70     796      46.30      5  \n",
       "1    18.90    1197      82.00      5  \n",
       "2    17.00    1197      80.00      5  \n",
       "3    20.92     998      67.10      5  \n",
       "4    22.77    1498      98.59      5  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "315e6b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    120000\n",
       "1    550000\n",
       "2    215000\n",
       "3    226000\n",
       "4    570000\n",
       "Name: selling_price, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e78871",
   "metadata": {},
   "source": [
    "### Feature Encoding and Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f406ef",
   "metadata": {},
   "source": [
    "OHE for columns which had lesser unique values and not ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e2a148f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['model'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2c0cb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le1=LabelEncoder()\n",
    "X['model']=le1.fit_transform(X['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31ba299d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>vehicle_age</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>seller_type</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>transmission_type</th>\n",
       "      <th>mileage</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "      <th>seats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>120000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>19.70</td>\n",
       "      <td>796</td>\n",
       "      <td>46.30</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54</td>\n",
       "      <td>5</td>\n",
       "      <td>20000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>18.90</td>\n",
       "      <td>1197</td>\n",
       "      <td>82.00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118</td>\n",
       "      <td>11</td>\n",
       "      <td>60000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>17.00</td>\n",
       "      <td>1197</td>\n",
       "      <td>80.00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>37000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>20.92</td>\n",
       "      <td>998</td>\n",
       "      <td>67.10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>6</td>\n",
       "      <td>30000</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Manual</td>\n",
       "      <td>22.77</td>\n",
       "      <td>1498</td>\n",
       "      <td>98.59</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model  vehicle_age  km_driven seller_type fuel_type transmission_type  \\\n",
       "0      7            9     120000  Individual    Petrol            Manual   \n",
       "1     54            5      20000  Individual    Petrol            Manual   \n",
       "2    118           11      60000  Individual    Petrol            Manual   \n",
       "3      7            9      37000  Individual    Petrol            Manual   \n",
       "4     38            6      30000      Dealer    Diesel            Manual   \n",
       "\n",
       "   mileage  engine  max_power  seats  \n",
       "0    19.70     796      46.30      5  \n",
       "1    18.90    1197      82.00      5  \n",
       "2    17.00    1197      80.00      5  \n",
       "3    20.92     998      67.10      5  \n",
       "4    22.77    1498      98.59      5  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9540c2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['seller_type'].unique()), len(df['fuel_type'].unique()), len(df['transmission_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed12a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating column transformer with 3 types of transformers\n",
    "\n",
    "numeric_features=X.select_dtypes(exclude='object').columns\n",
    "onehot_columns=['seller_type', 'fuel_type','transmission_type']\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "numeric_transformer=StandardScaler()\n",
    "oh_transformer=OneHotEncoder(drop='first')\n",
    "\n",
    "preprocessor=ColumnTransformer([\n",
    "    (\"OneHotEncoder\",oh_transformer, onehot_columns),\n",
    "    (\"StandardScaler\",numeric_transformer, numeric_features)\n",
    "], remainder='passthrough' # remainder = 'passthrough' means that all other features should be kept as they are\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db31c208",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f976bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.519714</td>\n",
       "      <td>0.983562</td>\n",
       "      <td>1.247335</td>\n",
       "      <td>-0.000276</td>\n",
       "      <td>-1.324259</td>\n",
       "      <td>-1.263352</td>\n",
       "      <td>-0.403022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.225693</td>\n",
       "      <td>-0.343933</td>\n",
       "      <td>-0.690016</td>\n",
       "      <td>-0.192071</td>\n",
       "      <td>-0.554718</td>\n",
       "      <td>-0.432571</td>\n",
       "      <td>-0.403022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.536377</td>\n",
       "      <td>1.647309</td>\n",
       "      <td>0.084924</td>\n",
       "      <td>-0.647583</td>\n",
       "      <td>-0.554718</td>\n",
       "      <td>-0.479113</td>\n",
       "      <td>-0.403022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.519714</td>\n",
       "      <td>0.983562</td>\n",
       "      <td>-0.360667</td>\n",
       "      <td>0.292211</td>\n",
       "      <td>-0.936610</td>\n",
       "      <td>-0.779312</td>\n",
       "      <td>-0.403022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.666211</td>\n",
       "      <td>-0.012060</td>\n",
       "      <td>-0.496281</td>\n",
       "      <td>0.735736</td>\n",
       "      <td>0.022918</td>\n",
       "      <td>-0.046502</td>\n",
       "      <td>-0.403022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2    3    4    5    6         7         8         9         10  \\\n",
       "0  1.0  0.0  0.0  0.0  0.0  1.0  1.0 -1.519714  0.983562  1.247335 -0.000276   \n",
       "1  1.0  0.0  0.0  0.0  0.0  1.0  1.0 -0.225693 -0.343933 -0.690016 -0.192071   \n",
       "2  1.0  0.0  0.0  0.0  0.0  1.0  1.0  1.536377  1.647309  0.084924 -0.647583   \n",
       "3  1.0  0.0  0.0  0.0  0.0  1.0  1.0 -1.519714  0.983562 -0.360667  0.292211   \n",
       "4  0.0  0.0  1.0  0.0  0.0  0.0  1.0 -0.666211 -0.012060 -0.496281  0.735736   \n",
       "\n",
       "         11        12        13  \n",
       "0 -1.324259 -1.263352 -0.403022  \n",
       "1 -0.554718 -0.432571 -0.403022  \n",
       "2 -0.554718 -0.479113 -0.403022  \n",
       "3 -0.936610 -0.779312 -0.403022  \n",
       "4  0.022918 -0.046502 -0.403022  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3d5026",
   "metadata": {},
   "source": [
    "### Model Training and Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9c7f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating and testing splits\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d40d53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5377a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to evaluate the model\n",
    "\n",
    "def evaluate_model(true, predicted):\n",
    "    mae=mean_absolute_error(y_true=true, y_pred=predicted)\n",
    "    mse=mean_squared_error(y_true=true, y_pred=predicted)\n",
    "    rmse=np.sqrt(mse)\n",
    "    score=r2_score(y_pred=predicted, y_true=true)\n",
    "    return mae, mse, rmse, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7112378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor()\n",
      "Model Performance for Training Set:\n",
      "MSE: 19430893396.99378\n",
      "MAE: 40164.833115640395\n",
      "RMSE: 139394.73948823815\n",
      "R2 Score: 0.9752258826100773\n",
      "------------------------------------------------------------------------------------------\n",
      "Model Performance for Testing Set:\n",
      "MSE: 79977741949.76947\n",
      "MAE: 101513.93564116766\n",
      "RMSE: 282803.3626917641\n",
      "R2 Score: 0.905312901016563\n",
      "==================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LinearRegression()\n",
      "Model Performance for Training Set:\n",
      "MSE: 298376743063.129\n",
      "MAE: 266703.0780207068\n",
      "RMSE: 546238.7235111851\n",
      "R2 Score: 0.6195738246285458\n",
      "------------------------------------------------------------------------------------------\n",
      "Model Performance for Testing Set:\n",
      "MSE: 288350900910.69727\n",
      "MAE: 267345.243047681\n",
      "RMSE: 536983.1476971109\n",
      "R2 Score: 0.6586161395548988\n",
      "==================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ridge()\n",
      "Model Performance for Training Set:\n",
      "MSE: 298377080879.87726\n",
      "MAE: 266668.86338791624\n",
      "RMSE: 546239.0327318959\n",
      "R2 Score: 0.6195733939169152\n",
      "------------------------------------------------------------------------------------------\n",
      "Model Performance for Testing Set:\n",
      "MSE: 288348648240.843\n",
      "MAE: 267292.88890119817\n",
      "RMSE: 536981.0501692244\n",
      "R2 Score: 0.6586188065315886\n",
      "==================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lasso()\n",
      "Model Performance for Training Set:\n",
      "MSE: 298376749329.12537\n",
      "MAE: 266702.41986961453\n",
      "RMSE: 546238.7292467693\n",
      "R2 Score: 0.619573816639488\n",
      "------------------------------------------------------------------------------------------\n",
      "Model Performance for Testing Set:\n",
      "MSE: 288349964890.1156\n",
      "MAE: 267341.2205742393\n",
      "RMSE: 536982.2761415088\n",
      "R2 Score: 0.658617247726639\n",
      "==================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "KNeighborsRegressor()\n",
      "Model Performance for Training Set:\n",
      "MSE: 93375252377.7362\n",
      "MAE: 90680.14146046029\n",
      "RMSE: 305573.64476953214\n",
      "R2 Score: 0.8809478588319773\n",
      "------------------------------------------------------------------------------------------\n",
      "Model Performance for Testing Set:\n",
      "MSE: 121751438318.19362\n",
      "MAE: 111654.71061510511\n",
      "RMSE: 348928.98750059964\n",
      "R2 Score: 0.8558562643760181\n",
      "==================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DecisionTreeRegressor()\n",
      "Model Performance for Training Set:\n",
      "MSE: 387824541.44315624\n",
      "MAE: 4608.54242371806\n",
      "RMSE: 19693.261320643574\n",
      "R2 Score: 0.9995055291323922\n",
      "------------------------------------------------------------------------------------------\n",
      "Model Performance for Testing Set:\n",
      "MSE: 145667681184.1422\n",
      "MAE: 131255.06964270267\n",
      "RMSE: 381664.3567116822\n",
      "R2 Score: 0.8275413907580275\n",
      "==================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AdaBoostRegressor()\n",
      "Model Performance for Training Set:\n",
      "MSE: 155583356420.54922\n",
      "MAE: 268656.69480580254\n",
      "RMSE: 394440.56132774835\n",
      "R2 Score: 0.8016333959982912\n",
      "------------------------------------------------------------------------------------------\n",
      "Model Performance for Testing Set:\n",
      "MSE: 243755351812.32083\n",
      "MAE: 280849.55694902525\n",
      "RMSE: 493715.8614145598\n",
      "R2 Score: 0.7114136188129496\n",
      "==================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "GradientBoostingRegressor()\n",
      "Model Performance for Training Set:\n",
      "MSE: 40627262175.599846\n",
      "MAE: 111131.3113668909\n",
      "RMSE: 201562.05539634646\n",
      "R2 Score: 0.9482008087942472\n",
      "------------------------------------------------------------------------------------------\n",
      "Model Performance for Testing Set:\n",
      "MSE: 88098021515.99655\n",
      "MAE: 122523.24579963853\n",
      "RMSE: 296813.1087334192\n",
      "R2 Score: 0.8956991547877257\n",
      "==================================================\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# beginning model training\n",
    "\n",
    "models={\n",
    "    \"Random Forest\":RandomForestRegressor(),\n",
    "    \"Linear Regression\":LinearRegression(),\n",
    "    \"Ridge\":Ridge(),\n",
    "    \"Lasso\":Lasso(),\n",
    "    \"KNN\":KNeighborsRegressor(),\n",
    "    \"Decision Trees\":DecisionTreeRegressor(),\n",
    "    \"Adaboost\":AdaBoostRegressor(),\n",
    "    \"GradientBoostingRegressor\":GradientBoostingRegressor()\n",
    "}\n",
    "\n",
    "for i in range(len(list(models))):\n",
    "    model=list(models.values())[i]\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions\n",
    "    y_train_pred=model.predict(X_train)\n",
    "    y_test_pred=model.predict(X_test)\n",
    "\n",
    "    # evaluate train and test dataset\n",
    "    model_train_mae, model_train_mse, model_train_rmse, model_train_score = evaluate_model(true=y_train, predicted=y_train_pred)\n",
    "    model_test_mae, model_test_mse, model_test_rmse, model_test_score = evaluate_model(true=y_test, predicted=y_test_pred)\n",
    "\n",
    "    # prinitng our results\n",
    "    print(list(models.values())[i])\n",
    "    print(\"Model Performance for Training Set:\")\n",
    "    print(f\"MSE: {model_train_mse}\")\n",
    "    print(f\"MAE: {model_train_mae}\")\n",
    "    print(f\"RMSE: {model_train_rmse}\")\n",
    "    print(f\"R2 Score: {model_train_score}\")\n",
    "    print('------------------------------------------------------------------------------------------')\n",
    "    print(\"Model Performance for Testing Set:\")\n",
    "    print(f\"MSE: {model_test_mse}\")\n",
    "    print(f\"MAE: {model_test_mae}\")\n",
    "    print(f\"RMSE: {model_test_rmse}\")\n",
    "    print(f\"R2 Score: {model_test_score}\")\n",
    "    print(\"=\"*50)\n",
    "    print('\\n'*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c89d18",
   "metadata": {},
   "source": [
    "### Hyper-Parameter Tuning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "809ac9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = {\"n_neighbors\": [2, 3, 10, 20, 40, 50]}\n",
    "rf_params = {\"max_depth\": [5, 8, 15, None, 10],\n",
    "             \"max_features\": [5, 7, \"auto\", 8],\n",
    "             \"min_samples_split\": [2, 8, 15, 20],\n",
    "             \"n_estimators\": [100, 200, 500, 1000]}\n",
    "ab_params={\n",
    "    \"n_estimators\":[50, 60, 70, 80],\n",
    "    \"loss\":['linear', 'square', 'exponential']\n",
    "}\n",
    "\n",
    "gb_params = {\n",
    "    'loss': ['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'criterion': ['friedman_mse', 'squared_error'],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_weight_fraction_leaf': [0.0, 0.01],\n",
    "    'max_depth': [3, 5, 7, None],\n",
    "    'min_impurity_decrease': [0.0, 0.01, 0.1],\n",
    "    'init': [None, 'zero'],\n",
    "    'random_state': [42],  # Set to your preferred seed or remove for non-reproducibility\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'alpha': [0.8, 0.9, 0.95],  # Used only for 'huber' or 'quantile' loss\n",
    "    'verbose': [0, 1],\n",
    "    'max_leaf_nodes': [None, 10, 20, 30],\n",
    "    'warm_start': [False, True],\n",
    "    'validation_fraction': [0.1, 0.2],  # Only when early stopping is used\n",
    "    'n_iter_no_change': [None, 5, 10],  # Only when early stopping is used\n",
    "    'tol': [1e-4, 1e-3],  # For early stopping tolerance\n",
    "    'ccp_alpha': [0.0, 0.01, 0.1]  # Tree pruning\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85133338",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomcv_models = [#('KNN', KNeighborsRegressor(), knn_params),\n",
    "                   #(\"RF\", RandomForestRegressor(), rf_params),\n",
    "                   #(\"AB\", AdaBoostRegressor(), ab_params),\n",
    "                   (\"GB\", GradientBoostingRegressor(), gb_params)\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53eca0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      381814.2927       18205.6457            0.56s\n",
      "         2      355632.1950       -5674.6593            0.44s\n",
      "         3      347123.5748       57851.4374            0.38s\n",
      "         4      329713.3084        3240.1288            0.34s\n",
      "         5      319943.3409       45799.7322            0.35s\n",
      "         6      304609.2036       16944.3949            0.32s\n",
      "         7      288219.9100        9059.6199            0.29s\n",
      "         8      281921.8059       20757.6085            0.28s\n",
      "         9      269610.8123       -4634.6099            0.27s\n",
      "        10      259306.8570        -829.7123            0.27s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      372623.5886       20305.2563            0.53s\n",
      "         2      358960.0235       30419.8864            0.35s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         3      340155.0274        6907.1817            0.43s\n",
      "         4      321112.6757        4638.6687            0.37s\n",
      "         1      376048.2317       24531.7769            0.58s\n",
      "         2      353977.2388       -4576.5305            0.38s\n",
      "         5      306859.8854       26911.3628            0.37s\n",
      "         3      340083.0860       27620.9955            0.34s\n",
      "        20      187792.1233       -7977.3050            0.23s\n",
      "         4      312065.6321      -19668.4274            0.30s\n",
      "         6      284671.5077      -20679.2098            0.38s\n",
      "         5      301465.2690       22998.0007            0.27s\n",
      "         6      289266.5851       15376.2044            0.25s\n",
      "         7      280001.7152       46976.9939            0.37s\n",
      "         7      287132.0738       18028.5108            0.27s\n",
      "         8      275132.1232       21764.7836            0.25s\n",
      "         8      269568.7494        3633.2724            0.39s\n",
      "         9      256188.8334      -21780.2880            0.38s\n",
      "         9      259576.6042       -6248.0518            0.28s\n",
      "        10      250494.0212       24743.8641            0.36s\n",
      "        10      245117.6374          89.5259            0.27s\n",
      "        30      160313.8889      -27409.6184            0.15s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      479567.9327      142503.2444            0.20s\n",
      "         2      380600.3846      113935.2559            0.14s\n",
      "         3      331242.7404       94139.6251            0.14s\n",
      "         4      279336.1154       13574.4110            0.14s\n",
      "         5      254862.9923       11300.7602            0.13s\n",
      "         6      250470.5362       23068.4657            0.12s\n",
      "         7      229596.0867      -14086.7095            0.11s\n",
      "         8      234742.5214       17437.7403            0.10s\n",
      "         9      232654.7237        2514.0875            0.10s\n",
      "        10      230384.9436        -267.3757            0.09s\n",
      "        20      231945.9124        -240.6489            0.05s\n",
      "        40      152216.0347        6736.7994            0.07s\n",
      "        30      227397.0192       -3358.1096            0.04s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.01, criterion=squared_error, init=zero, learning_rate=0.2, loss=quantile, max_depth=7, max_features=log2, max_leaf_nodes=None, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   0.1s\n",
      "        20      184207.6059       23071.4780            0.33s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "        20      184936.0665       42573.7487            0.31s\n",
      "        50      144008.4482        6948.8530            0.00s\n",
      "         1      469374.8558      142824.3962            0.27s\n",
      "         2      375341.0096      124088.6806            0.17s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.1, loss=absolute_error, max_depth=5, max_features=log2, max_leaf_nodes=20, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.3s\n",
      "         3      313893.6118       73357.9182            0.14s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         4      280249.4675       44808.2307            0.12s\n",
      "         5      241725.6522       -9110.8055            0.10s\n",
      "         1      478642.5379      142715.8886            0.10s\n",
      "         2      364437.7373       89676.9376            0.08s\n",
      "         6      228235.4072        8049.9425            0.10s\n",
      "         3      317502.4141       94436.2365            0.08s\n",
      "         4      264685.2682       11512.2192            0.07s\n",
      "         7      228366.5629       17960.6501            0.11s\n",
      "         5      253986.1913       36687.0298            0.07s\n",
      "         6      245479.4368       19433.9275            0.07s\n",
      "         8      229427.8380       12121.6235            0.11s\n",
      "         7      219565.0571      -22338.8030            0.07s\n",
      "         9      228950.8769        5734.7296            0.11s\n",
      "         8      235024.1074       34437.7256            0.08s\n",
      "        10      215356.6697      -17305.1393            0.10s\n",
      "         9      229891.6350       -1156.8125            0.09s\n",
      "        10      218409.6677      -13311.9458            0.08s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=quantile, max_depth=None, max_features=sqrt, max_leaf_nodes=None, min_impurity_decrease=0.01, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   0.2s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=quantile, max_depth=None, max_features=sqrt, max_leaf_nodes=None, min_impurity_decrease=0.01, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   0.2s\n",
      "        20      216983.0236        5548.9869            0.08s\n",
      "        20      222737.5578       -4619.5674            0.06s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "[CV] END alpha=0.8, ccp_alpha=0.01, criterion=squared_error, init=zero, learning_rate=0.2, loss=quantile, max_depth=7, max_features=log2, max_leaf_nodes=None, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   0.1s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.01, criterion=squared_error, init=zero, learning_rate=0.2, loss=quantile, max_depth=7, max_features=log2, max_leaf_nodes=None, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   0.1s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 347278839218.2270            1.08s\n",
      "         1 357381343999.8001            2.66s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=quantile, max_depth=None, max_features=sqrt, max_leaf_nodes=None, min_impurity_decrease=0.01, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   0.2s\n",
      "         1 348679609777.8669            2.19s\n",
      "         2 317519407299.2156            1.14s\n",
      "         3 289278529808.7551            1.14s\n",
      "         2 324411958454.0291            2.40s\n",
      "        30      151576.1561      -11301.5566            0.23s\n",
      "        30      159713.5210       53267.6026            0.21s\n",
      "         2 316305566069.3151            2.29s\n",
      "         3 295354986931.6493            2.29s\n",
      "         4 263588856564.1591            1.58s\n",
      "         5 239047332663.1987            1.46s\n",
      "         4 270074656938.8514            2.13s\n",
      "         3 288085655378.3561            2.71s\n",
      "         5 245918521246.1042            1.89s\n",
      "        40      142039.2923       -4631.4152            0.10s\n",
      "         6 217487940264.6765            1.47s\n",
      "         7 198085273166.2559            1.41s\n",
      "        40      141623.3712       -8847.7051            0.10s\n",
      "         6 224649441770.3632            1.94s\n",
      "         4 262519152153.2122            2.90s\n",
      "         7 204801384864.5093            1.83s\n",
      "         8 180729940499.3536            1.54s\n",
      "        50      135192.4169        3035.9765            0.00s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.1, loss=absolute_error, max_depth=5, max_features=log2, max_leaf_nodes=20, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.4s\n",
      "         8 186040223323.9994            1.78s\n",
      "         9 164865800277.4767            1.46s\n",
      "         5 239402925589.4716            2.95s\n",
      "        10 151332836089.8801            1.44s\n",
      "        50      131257.7135      -15742.8882            0.00s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.1, loss=absolute_error, max_depth=5, max_features=log2, max_leaf_nodes=20, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.4s\n",
      "         9 170033412968.2941            1.79s\n",
      "        10 156136639103.9261            1.70s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 1597991073649.6570 92010149664.7026            0.28s\n",
      "         2 1415380795080.5562 -217352463610.1086            0.27s\n",
      "         6 217401590823.2220            2.97s\n",
      "         3 1314315052914.5559  4548219707.0188            0.34s\n",
      "         4 929301067151.4861 -1144630619176.3203            0.38s\n",
      "         5 1216781974404.0808 1576091951213.7190            0.36s\n",
      "         6 1055237667289.5642 -291626552907.9899            0.40s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         7 1065421707218.1693 381109275268.2593            0.41s\n",
      "         1 1191190238236.8770 94228244363.3966            0.67s\n",
      "         8 1015117009501.7786 106854271298.0669            0.42s\n",
      "         2 1066353871191.0057 -32433433851.1797            0.46s\n",
      "         7 198029963579.9387            2.93s\n",
      "         9 904866104267.2626 -177040268169.4449            0.40s\n",
      "         3 991112164398.4332 79185226632.8828            0.38s\n",
      "         4 877349871184.7498 -108756922339.2577            0.34s\n",
      "        10 882505415726.9513 140890149875.1672            0.39s\n",
      "         5 872760048726.2472 322347161574.4536            0.32s\n",
      "         6 738086938787.6874 -217432645090.4457            0.30s\n",
      "         7 738174723663.0513 308314691970.5514            0.29s\n",
      "         8 694838445439.1315 110314167118.7937            0.28s\n",
      "         9 594729405020.1589 -160921502990.4573            0.29s\n",
      "        10 548853906505.7539 26953510957.1589            0.29s\n",
      "         8 180378942978.7173            2.86s\n",
      "        20 611245215700.3373 51685393155.7830            0.30s\n",
      "        20 321827621221.4136 76668817677.8902            0.27s\n",
      "        30 202621553890.5977  7435162173.6741            0.25s\n",
      "         9 164841080681.1685            2.99s\n",
      "        30 188764812091.1925 -33766189358.1795            0.23s\n",
      "        40 402248044909.9968 -5676396760.5568            0.20s\n",
      "        20 64111999749.8562            1.27s\n",
      "        10 150706583529.0483            2.89s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.01, loss=absolute_error, max_depth=3, max_features=sqrt, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.3s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "        50 355300696874.9582 -66456114922.3529            0.16s\n",
      "         1 1422871626520.4546 100570663799.7637            0.45s\n",
      "         2 1351048765982.6851 206678361293.1595            0.35s\n",
      "        20 67226633762.9342            1.44s\n",
      "         3 975468353953.6108 -1095284233059.4391            0.35s\n",
      "        40 141205648965.9344   123556358.0492            0.21s\n",
      "         4 1161210885410.4033 1120958967326.2886            0.32s\n",
      "         5 1115385630332.7708 134518886497.5708            0.30s\n",
      "         6 1084668430942.1156 196935750273.1229            0.30s\n",
      "         7 965332167848.7997 -179531851216.5009            0.32s\n",
      "         8 934398032845.3472 149617607135.5332            0.34s\n",
      "        60 341158893801.0042 45517916040.9655            0.13s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.01, loss=absolute_error, max_depth=3, max_features=sqrt, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.3s\n",
      "         9 893865844889.7969 55881556927.0231            0.35s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        10 834200653137.4154 -6013411180.2521            0.35s\n",
      "         1      217626.3268            0.81s\n",
      "        50 109177996367.4265 -115227269959.1906            0.17s\n",
      "         2      208041.0193            0.65s\n",
      "         3      199650.5283            0.72s\n",
      "         4      199635.9784            0.70s\n",
      "         5      195421.0170            0.72s\n",
      "        70 326599581423.2897 28669743390.2166            0.09s\n",
      "         6      190268.3159            0.65s\n",
      "         7      183153.4919            0.62s\n",
      "         8      179194.5198            0.59s\n",
      "         9      179138.2379            0.55s\n",
      "        10      179085.7610            0.51s\n",
      "        20 572667161792.9119 78199067501.0450            0.28s\n",
      "        60 104301906060.2415 35459733945.1226            0.14s\n",
      "        80 337366820855.5714 24249205489.6023            0.06s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.01, loss=absolute_error, max_depth=3, max_features=sqrt, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.4s\n",
      "        30 446816839107.5714 26548959606.7267            0.22s\n",
      "        20      152195.3510            0.54s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        70 100809240822.7217 29966673458.4950            0.10s\n",
      "         1      207695.9006            0.98s\n",
      "         2      198631.4793            0.97s\n",
      "         3      190631.8401            1.05s\n",
      "         4      190620.3271            0.99s\n",
      "        90 106084884960.4364 -828840019859.2706            0.03s\n",
      "         5      186474.0923            0.97s\n",
      "        40 391912235960.3242 30313763841.4969            0.19s\n",
      "         6      181487.9366            0.93s\n",
      "         7      174813.9192            0.86s\n",
      "        30      132737.5232            0.54s\n",
      "         8      170817.5339            0.80s\n",
      "        20 64095473438.0801            1.91s\n",
      "         9      170767.0288            0.74s\n",
      "        10      170720.7140            0.68s\n",
      "        80 98602025738.7753 10461040477.9769            0.07s\n",
      "        40      129815.5210            0.44s\n",
      "       100 294191808182.1879 -34270668987.3805            0.00s\n",
      "        20      144588.3847            0.55s\n",
      "        50 351403701193.3049 -22808501200.0725            0.16s\n",
      "        50      120290.5337            0.39s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.05, loss=squared_error, max_depth=7, max_features=log2, max_leaf_nodes=None, min_impurity_decrease=0.01, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   0.3s\n",
      "        30 32199591671.8707            1.24s\n",
      "        90 95329433896.5209 88324261157.6711            0.03s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      213411.7758            0.65s\n",
      "        60      115204.6192            0.35s\n",
      "        30      129211.6880            0.50s\n",
      "         2      203814.2862            0.72s\n",
      "        60 99457958129.3412 -958672928019.8317            0.13s\n",
      "         3      195473.4413            0.76s\n",
      "         4      189609.0139            0.69s\n",
      "         5      189596.8646            0.66s\n",
      "        40      126214.6514            0.42s\n",
      "         6      182451.6034            0.66s\n",
      "         7      175727.1166            0.70s\n",
      "        70      110100.0310            0.32s\n",
      "         8      171628.8579            0.66s\n",
      "        30 30260963877.0806            1.29s\n",
      "         9      169512.9915            0.63s\n",
      "        10      169466.7417            0.62s\n",
      "       100 69081770611.7150 -1271850924.7745            0.00s\n",
      "        70 333190604120.9839 83492841774.2902            0.10s\n",
      "        50      116440.6888            0.38s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.05, loss=squared_error, max_depth=7, max_features=log2, max_leaf_nodes=None, min_impurity_decrease=0.01, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   0.4s\n",
      "        80      109467.1591            0.29s\n",
      "        60      113505.6191            0.33s\n",
      "        20      144986.1860            0.59s\n",
      "        90      108549.3476            0.26s\n",
      "        80 319314039951.1457 930994648761.3036            0.06s\n",
      "        70      105795.4079            0.31s\n",
      "       100      108511.8738            0.24s\n",
      "        30      130466.7218            0.51s\n",
      "        80      103102.6879            0.27s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.05, loss=squared_error, max_depth=7, max_features=log2, max_leaf_nodes=None, min_impurity_decrease=0.01, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   0.3s\n",
      "        30 30300907567.4703            1.48s\n",
      "        40      128113.6783            0.47s\n",
      "        90      100413.8110            0.25s\n",
      "        50      118446.9496            0.42s\n",
      "       100       98051.9223            0.23s\n",
      "        60      116590.8094            0.39s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.05, loss=quantile, max_depth=3, max_features=sqrt, max_leaf_nodes=10, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   0.3s\n",
      "        40 18350461571.3387            1.07s\n",
      "        70      107621.4566            0.35s\n",
      "        40 17022515760.1443            1.08s\n",
      "        80      106580.0626            0.32s\n",
      "        90      104993.5259            0.29s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.05, loss=quantile, max_depth=None, max_features=sqrt, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.2s\n",
      "       100      101749.5339            0.26s\n",
      "        40 16970538799.3243            1.21s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.05, loss=quantile, max_depth=None, max_features=sqrt, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.2s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.05, loss=quantile, max_depth=None, max_features=sqrt, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.2s\n",
      "        50 11596749131.0197            0.87s\n",
      "        50 12538063873.5902            0.90s\n",
      "       200       97154.5253            0.00s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.05, loss=quantile, max_depth=3, max_features=sqrt, max_leaf_nodes=10, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   0.5s\n",
      "        50 11351111250.8870            0.96s\n",
      "       200       96041.5143            0.00s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.05, loss=quantile, max_depth=3, max_features=sqrt, max_leaf_nodes=10, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   0.5s\n",
      "        60  9044066610.8815            0.69s\n",
      "        60  9574774753.8274            0.72s\n",
      "        60  8719943121.2808            0.74s\n",
      "        70  7765114340.7297            0.54s\n",
      "        70  7394695528.0604            0.56s\n",
      "        70  7949658696.9856            0.57s\n",
      "        80  6951182866.5304            0.36s\n",
      "        80  6610070022.0504            0.36s\n",
      "        80  7043356164.5937            0.37s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=quantile, max_depth=3, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   0.7s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=quantile, max_depth=3, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   0.7s\n",
      "        90  6138651921.3848            0.18s\n",
      "        90  6523145525.3070            0.18s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=quantile, max_depth=3, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   0.8s\n",
      "        90  6392953137.8640            0.18s\n",
      "       100  5742647078.4600            0.00s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=squared_error, init=zero, learning_rate=0.05, loss=huber, max_depth=7, max_features=sqrt, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   1.7s\n",
      "       100  6102031852.3484            0.00s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=squared_error, init=zero, learning_rate=0.05, loss=huber, max_depth=7, max_features=sqrt, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   1.8s\n",
      "       100  6001703865.5160            0.00s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=squared_error, init=zero, learning_rate=0.05, loss=huber, max_depth=7, max_features=sqrt, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   1.8s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.2, loss=huber, max_depth=None, max_features=log2, max_leaf_nodes=10, min_impurity_decrease=0.1, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.3s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.2, loss=huber, max_depth=None, max_features=log2, max_leaf_nodes=10, min_impurity_decrease=0.1, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.3s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      405682.0190            0.46s\n",
      "         2      403347.7520            0.42s\n",
      "         3      401022.9954            0.38s\n",
      "         4      398784.5719            0.33s\n",
      "         5      396833.0034            0.33s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.2, loss=huber, max_depth=None, max_features=log2, max_leaf_nodes=10, min_impurity_decrease=0.1, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.3s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         6      395090.5457            0.35s\n",
      "         1      393295.2877            0.27s\n",
      "         2      390638.1059            0.25s\n",
      "         7      392648.0781            0.36s\n",
      "         3      388534.6428            0.28s\n",
      "         8      390525.5570            0.33s\n",
      "         9      388493.3180            0.31s\n",
      "         4      386008.9724            0.31s\n",
      "        10      386400.6179            0.29s\n",
      "         5      383997.6529            0.27s\n",
      "         6      381779.0897            0.25s\n",
      "         7      379363.0259            0.24s\n",
      "         8      377529.2778            0.25s\n",
      "         9      375184.0519            0.27s\n",
      "        10      372854.5505            0.27s\n",
      "        20      365499.8385            0.21s\n",
      "        20      351724.1858            0.18s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.0, criterion=squared_error, init=None, learning_rate=0.2, loss=huber, max_depth=7, max_features=log2, max_leaf_nodes=10, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   1.2s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        30      345497.0240            0.14s\n",
      "         1      395617.8061            0.30s\n",
      "         2      393156.0674            0.27s\n",
      "        30      331995.9176            0.12s\n",
      "         3      390838.3869            0.28s\n",
      "         4      388361.7432            0.30s\n",
      "         5      386270.6547            0.27s\n",
      "         6      383956.2955            0.31s\n",
      "         7      381891.7888            0.29s\n",
      "         8      380080.4367            0.31s\n",
      "         9      377972.0068            0.29s\n",
      "        10      375624.8852            0.27s\n",
      "        40      316716.7464            0.06s\n",
      "        40      330161.7341            0.07s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.0, criterion=squared_error, init=None, learning_rate=0.2, loss=huber, max_depth=7, max_features=log2, max_leaf_nodes=10, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   1.3s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      200096.2683           0.0000            3.53s\n",
      "        20      354330.1593            0.21s\n",
      "         2      196782.3761      -13253.8140            2.64s\n",
      "         3      202126.1045       21370.2743            1.91s\n",
      "         4      194062.1750      -32249.1285            1.56s\n",
      "        50      300300.9774            0.00s\n",
      "         5      199612.7047       22197.9649            1.40s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.01, loss=absolute_error, max_depth=5, max_features=log2, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.3s\n",
      "         6      201875.9658        9050.9447            1.36s\n",
      "        50      314620.4515            0.00s\n",
      "         7      194178.4986      -30785.0995            1.28s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=friedman_mse, init=None, learning_rate=0.01, loss=quantile, max_depth=5, max_features=log2, max_leaf_nodes=30, min_impurity_decrease=0.1, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   0.1s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.01, loss=absolute_error, max_depth=5, max_features=log2, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.4s\n",
      "         1      182245.4309          -3.3562            1.12s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         2      187022.0159       19103.0736            0.84s\n",
      "         1      187092.0606          -0.2725            0.65s\n",
      "         2      180783.7030      -25233.5418            0.56s\n",
      "         3      192785.4594       48007.0956            0.52s\n",
      "         3      191070.1139       16189.2403            1.02s\n",
      "         4      188022.9932      -19049.9762            0.48s\n",
      "         5      189766.3597        6973.4464            0.54s\n",
      "         4      186774.3877      -17179.9546            1.06s\n",
      "         5      187237.1477        1850.9646            0.96s\n",
      "         6      191174.3409        5631.9952            0.62s\n",
      "         6      189897.9555       10641.1650            0.88s\n",
      "         7      181773.8115      -37602.0516            0.63s\n",
      "         7      186200.6540      -14786.4849            0.83s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=friedman_mse, init=None, learning_rate=0.01, loss=quantile, max_depth=5, max_features=log2, max_leaf_nodes=30, min_impurity_decrease=0.1, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   0.0s\n",
      "         8      185515.2158       -2741.7588            0.77s\n",
      "        30      334659.6119            0.14s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=friedman_mse, init=None, learning_rate=0.01, loss=quantile, max_depth=5, max_features=log2, max_leaf_nodes=30, min_impurity_decrease=0.1, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   0.0s\n",
      "        40      320207.7907            0.07s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=friedman_mse, init=zero, learning_rate=0.01, loss=quantile, max_depth=None, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   0.7s\n",
      "        50      304242.2212            0.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.01, loss=absolute_error, max_depth=5, max_features=log2, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.4s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=friedman_mse, init=zero, learning_rate=0.01, loss=quantile, max_depth=None, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   0.7s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=friedman_mse, init=zero, learning_rate=0.01, loss=quantile, max_depth=None, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   0.7s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 350717756413.3989 -365457510333.1506            1.03s\n",
      "         2 325074452604.6008 42421992168.4389            0.90s\n",
      "         3 312336196102.5213 54222190478.8287            0.80s\n",
      "         4 271628839258.4438  3945818355.1776            0.82s\n",
      "         5 242160490523.5190 17082164905.2588            0.79s\n",
      "         6 227525312514.5242 33037101800.2611            0.77s\n",
      "         7 195118747637.3258   539434404.2149            0.76s\n",
      "         8 184388073297.3120 28292357317.8635            0.71s\n",
      "         9 167243650664.0338 17034899245.7871            0.72s\n",
      "        10 152587246305.0201 14603532227.0375            0.71s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.0, criterion=squared_error, init=None, learning_rate=0.2, loss=huber, max_depth=7, max_features=log2, max_leaf_nodes=10, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   1.3s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 340204967650.9653 -359717508065.0800            1.05s\n",
      "         2 318998062194.6328 52297747436.3875            0.91s\n",
      "         3 294830963452.6586 34382144138.5733            0.90s\n",
      "        20 65697956963.9551  5933972529.1067            0.54s\n",
      "         4 271596613332.7798 30772566504.9579            0.89s\n",
      "         5 232009653306.9928   -18747331.9827            0.81s\n",
      "         6 208750757945.2744 19144604176.2949            0.84s\n",
      "         7 194427145937.6936 27175202752.3741            0.81s\n",
      "         8 180872210262.9962 23574465773.6615            0.80s\n",
      "         9 165924123283.6713 17134011493.4441            0.85s\n",
      "        10 142807818751.9746   208611818.7786            0.79s\n",
      "        30 30800567167.0541  2391797598.2720            0.35s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=squared_error, init=None, learning_rate=0.1, loss=squared_error, max_depth=5, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   0.7s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "        20 60441436096.9071  6262670636.6732            0.61s\n",
      "         1 348717387439.4384 -345693638375.7560            1.02s\n",
      "        40 18157983583.7534  4330151916.4050            0.18s\n",
      "         2 307214412225.5772 16217881587.7581            0.82s\n",
      "         3 297913666766.5864 56626462527.4279            0.93s\n",
      "         4 257448796181.6316  3627233101.4851            0.90s\n",
      "         5 241939332623.3578 37124074275.9041            0.89s\n",
      "         6 223799560075.6754 26464874626.4517            0.81s\n",
      "         7 188772778559.8093 -5986154055.5618            0.82s\n",
      "        50 12686968676.0056  -296821395.1557            0.00s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=squared_error, init=zero, learning_rate=0.05, loss=huber, max_depth=7, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.1, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.8s\n",
      "         8 186625644217.8658 41282729809.9011            0.80s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         9 168836762099.8122 13497503899.4437            0.75s\n",
      "         1 238435878100.3606 -272780994766.7554            0.68s\n",
      "         2 229302058031.6568 -5339836901.0468            0.52s\n",
      "         3 248780865105.2981 37731412244.4293            0.47s\n",
      "        10 147332618748.9893  4457154825.5460            0.71s\n",
      "         4 233071909419.1053 -12995689035.3593            0.43s\n",
      "        30 29059499833.8215  1620848267.0401            0.39s\n",
      "         5 251225193025.9654 36994034921.7996            0.41s\n",
      "         6 207426712197.9287 -58644719783.8235            0.40s\n",
      "         7 232814917359.6560 46525429894.4662            0.39s\n",
      "         8 242369963147.5997 22504648509.7755            0.38s\n",
      "         9 221561265043.1337 -23529079428.0186            0.37s\n",
      "        10 218164648583.3035  2420359944.2652            0.36s\n",
      "        20 193438737467.9624  4820711311.2339            0.33s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=squared_error, init=None, learning_rate=0.1, loss=squared_error, max_depth=5, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   1.0s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 228965332481.2987 -215779044337.7590            0.71s\n",
      "         2 222038320351.9744 -1048055022.1693            0.54s\n",
      "         3 227441576576.4826 15939463236.2247            0.49s\n",
      "        30 163013289496.7746 -2678613360.0650            0.33s\n",
      "         4 213046345225.9764 -14015333758.3633            0.45s\n",
      "         5 217393629826.7573 14045495079.1997            0.42s\n",
      "         6 188823171449.1947 -37250072984.7457            0.48s\n",
      "         7 202247405828.6800 29118820583.5867            0.52s\n",
      "         8 206882059372.4458 11539927389.1536            0.52s\n",
      "        20 64215245077.8959  3205308693.8595            0.51s\n",
      "         9 193817923902.4649 -13881821667.3540            0.57s\n",
      "        10 182148362665.0891 -10590445922.8723            0.54s\n",
      "        40 147807044045.0397 -14015220196.8279            0.30s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=squared_error, init=None, learning_rate=0.1, loss=squared_error, max_depth=5, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   1.1s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 249312389879.3879 -185021583842.3599            0.67s\n",
      "         2 232008536639.6659 -18987598605.3969            0.71s\n",
      "         3 230175989920.8179  5512184035.4359            0.59s\n",
      "        40 15820001575.2473  1754232749.0932            0.20s\n",
      "        20 184172002072.2913 17226096105.5221            0.46s\n",
      "         4 209580406494.6881 -22864544546.1613            0.56s\n",
      "         5 212436981981.3961 11045165721.4690            0.52s\n",
      "         6 192443008527.9469 -21586017667.4095            0.50s\n",
      "         7 191657578273.0431  4199154394.9546            0.48s\n",
      "        50 144004857406.8938  7158535968.6879            0.26s\n",
      "         8 207932942670.9760 30975697651.2709            0.51s\n",
      "         9 210039403226.8940  8608297853.3892            0.53s\n",
      "        10 196526197741.9224 -13334084631.2196            0.53s\n",
      "        60 132503249240.7788 25675678557.6329            0.20s\n",
      "        30 147980714578.2473 -5996877413.7583            0.42s\n",
      "        20 175768422643.6655 21016046152.7734            0.48s\n",
      "        30 30149345212.9599  2418945379.9394            0.35s\n",
      "        70 106187729517.6557 -5835246801.0700            0.16s\n",
      "        40 134541236124.7875 -9849275526.3151            0.37s\n",
      "        30 158105783350.3716 26582119967.1535            0.44s\n",
      "        50 127310860308.2068 10904289584.0845            0.31s\n",
      "        40 137399454761.2750 -7310526179.6249            0.36s\n",
      "        50 12687805632.8600   583202217.6503            0.00s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=squared_error, init=zero, learning_rate=0.05, loss=huber, max_depth=7, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.1, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   1.0s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "        80 101155230769.3242 -6231047420.9360            0.12s\n",
      "         1 931652978201.5498 471096919875.3484            0.39s\n",
      "        60 113379110549.5550 12244644771.3235            0.25s\n",
      "         2 896471074753.3323 711372654291.1908            0.38s\n",
      "         3 729525437331.5806 289448002446.3561            0.50s\n",
      "        50 122003502633.6411 -19532098436.2848            0.30s\n",
      "        40 16421239069.9171   185091378.2796            0.17s\n",
      "         4 520533767036.3254 81269596840.5567            0.55s\n",
      "         5 294478520489.2789 -107881221118.5514            0.57s\n",
      "         6 327053773136.5649 222136006195.0625            0.63s\n",
      "         7 168241516971.3582 -128295865749.4388            0.59s\n",
      "         8 141015195190.7165 39503979352.3466            0.60s\n",
      "         9 133824970699.5860 53165124711.3258            0.57s\n",
      "        10 184715048864.7398 130607695670.1414            0.55s\n",
      "        90 85807760698.9870  1155997548.3741            0.06s\n",
      "        60 114204700138.1906 24770818765.0801            0.24s\n",
      "        70 96738183243.3791  8984398845.6055            0.19s\n",
      "        20 115679306461.2191 11139551567.0825            0.50s\n",
      "       100 79691055052.3032  9842126199.2713            0.00s\n",
      "        70 99961605816.0329  8433513479.3904            0.18s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.01, loss=huber, max_depth=None, max_features=log2, max_leaf_nodes=10, min_impurity_decrease=0.1, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   0.6s\n",
      "        80 87167703831.0812 -5076999086.3066            0.13s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 841654266488.8732 395961186799.9683            0.58s\n",
      "         2 611754387953.9039 296870954313.9916            0.47s\n",
      "         3 482218719421.7876 237188768525.1901            0.43s\n",
      "         4 357568126010.3256 115418504306.4766            0.47s\n",
      "        50 11902030133.5507 -2693171759.8613            0.00s\n",
      "        30 76028128144.6012  9672356311.7959            0.44s\n",
      "         5 238841430486.0061 11242552333.3888            0.49s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=squared_error, init=zero, learning_rate=0.05, loss=huber, max_depth=7, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.1, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.9s\n",
      "         6 197394240965.0361 87233946797.2463            0.46s\n",
      "         7 161264113684.7215 30535424010.3641            0.46s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         8 132923092872.1853 23959857658.5364            0.45s\n",
      "         9 111996693909.8454 11845446164.1026            0.43s\n",
      "        10 94585526402.3599  5900605492.7590            0.41s\n",
      "         1 1108864303191.3113 370560494837.2587            0.78s\n",
      "        80 92975844926.0981 13070467361.1767            0.12s\n",
      "         2 565484933549.3091 -165761838977.4249            0.90s\n",
      "        90 82831379653.7607  5489985760.3069            0.07s\n",
      "         3 654129334189.6467 636783406331.5315            0.74s\n",
      "         4 480238299656.0787 55187214113.4960            0.64s\n",
      "         5 384013377976.3810 68468880813.9683            0.70s\n",
      "         6 315627649683.1289 59357477615.4273            0.64s\n",
      "         7 147272640570.3647 -158281262186.5223            0.60s\n",
      "         8 228099796653.9692 199519821175.4656            0.57s\n",
      "        20 55329092430.8852   139027057.2244            0.35s\n",
      "         9 200939646830.0453 24613043530.9446            0.55s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.01, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=squared_error, max_depth=3, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.1s\n",
      "        40 67215119610.3339 24079359768.1694            0.40s\n",
      "       100 73027661453.4586  5940730530.5419            0.00s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.01, loss=huber, max_depth=None, max_features=log2, max_leaf_nodes=10, min_impurity_decrease=0.1, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   0.7s\n",
      "        30 51640359302.3153  1315772079.2792            0.32s\n",
      "        90 77088412362.1613  9294564568.6111            0.06s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.01, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=squared_error, max_depth=3, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.3s\n",
      "        40 40096168442.3072  -879601806.3380            0.30s\n",
      "       100 76953168313.0240 11710461271.2038            0.00s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.01, loss=huber, max_depth=None, max_features=log2, max_leaf_nodes=10, min_impurity_decrease=0.1, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   0.7s\n",
      "        50 40582171510.6402  3894327317.5745            0.25s\n",
      "        60 37073403722.6054  -374680886.3264            0.20s\n",
      "        70 32340363523.0195  2918304033.7950            0.15s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.05, loss=absolute_error, max_depth=5, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   2.1s\n",
      "        80 32336958838.1301 -1262970251.4420            0.10s\n",
      "        90 29198538376.0171   477239501.3315            0.05s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.05, loss=absolute_error, max_depth=5, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   2.2s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.05, loss=absolute_error, max_depth=5, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   2.2s\n",
      "       100 26979401137.3909  -389147046.3008            0.00s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.01, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=squared_error, max_depth=3, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.5s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.05, loss=quantile, max_depth=3, max_features=sqrt, max_leaf_nodes=20, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   0.6s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.05, loss=quantile, max_depth=3, max_features=sqrt, max_leaf_nodes=20, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   0.6s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=squared_error, init=zero, learning_rate=0.1, loss=squared_error, max_depth=3, max_features=log2, max_leaf_nodes=10, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   0.5s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.05, loss=quantile, max_depth=3, max_features=sqrt, max_leaf_nodes=20, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   0.7s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=squared_error, init=zero, learning_rate=0.1, loss=squared_error, max_depth=3, max_features=log2, max_leaf_nodes=10, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   0.5s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=absolute_error, max_depth=None, max_features=log2, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=4, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.2s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=squared_error, init=zero, learning_rate=0.1, loss=squared_error, max_depth=3, max_features=log2, max_leaf_nodes=10, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   0.6s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=absolute_error, max_depth=None, max_features=log2, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=4, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.5s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=absolute_error, max_depth=None, max_features=log2, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=4, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.5s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=squared_error, init=None, learning_rate=0.05, loss=absolute_error, max_depth=None, max_features=None, max_leaf_nodes=10, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.9s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=squared_error, init=None, learning_rate=0.05, loss=absolute_error, max_depth=None, max_features=None, max_leaf_nodes=10, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.9s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.1, criterion=friedman_mse, init=None, learning_rate=0.2, loss=absolute_error, max_depth=5, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   0.2s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.1, criterion=friedman_mse, init=None, learning_rate=0.2, loss=absolute_error, max_depth=5, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   0.3s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.1, criterion=friedman_mse, init=None, learning_rate=0.2, loss=absolute_error, max_depth=5, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   0.3s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=squared_error, init=None, learning_rate=0.05, loss=absolute_error, max_depth=None, max_features=None, max_leaf_nodes=10, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.9s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.1, loss=quantile, max_depth=5, max_features=sqrt, max_leaf_nodes=10, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.2s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.1, loss=squared_error, max_depth=None, max_features=None, max_leaf_nodes=10, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.8s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.1, loss=squared_error, max_depth=None, max_features=None, max_leaf_nodes=10, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.8s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.1, loss=squared_error, max_depth=None, max_features=None, max_leaf_nodes=10, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.8s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.1, loss=quantile, max_depth=5, max_features=sqrt, max_leaf_nodes=10, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.3s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.1, loss=quantile, max_depth=5, max_features=sqrt, max_leaf_nodes=10, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.3s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 393654378748.1821            2.57s\n",
      "         1 375265883836.1721            1.48s\n",
      "         2 324548316208.6411            2.06s\n",
      "         2 311471593404.2474            1.49s\n",
      "         3 257717263078.9378            1.47s\n",
      "         3 273587752893.8972            2.57s\n",
      "         4 216452292986.1300            1.77s\n",
      "         4 228534762580.3851            2.75s\n",
      "         5 181071470796.5349            1.80s\n",
      "         6 154110365991.7904            1.73s\n",
      "         5 192507039057.8422            2.77s\n",
      "         7 130024850891.0576            1.75s\n",
      "         6 161781156348.5513            2.67s\n",
      "         8 111489923723.9174            1.73s\n",
      "         7 139403283182.5553            2.48s\n",
      "         9 96831837275.0971            1.72s\n",
      "        10 84657687458.6904            1.71s\n",
      "         8 121255553155.2557            2.46s\n",
      "         9 104817657782.2532            2.34s\n",
      "        10 91093144502.1237            2.23s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.1, criterion=squared_error, init=None, learning_rate=0.1, loss=absolute_error, max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.1, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   0.2s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 377220284553.7463            1.61s\n",
      "         2 312722055958.7961            1.94s\n",
      "         3 259179417251.6629            2.10s\n",
      "         4 217425232888.1434            2.07s\n",
      "         5 184386030885.5768            1.96s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.1, criterion=squared_error, init=None, learning_rate=0.1, loss=absolute_error, max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.1, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   0.2s\n",
      "         6 155725452775.0580            1.99s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.1, criterion=squared_error, init=None, learning_rate=0.1, loss=absolute_error, max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.1, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   0.2s\n",
      "         7 131609876135.2695            1.96s\n",
      "         8 113690718182.6441            1.89s\n",
      "         9 97435768194.7507            1.92s\n",
      "        20 32607585800.9246            2.05s\n",
      "        10 84826871861.1854            1.90s\n",
      "        20 30905446045.2061            2.30s\n",
      "        30 21069897527.7211            1.96s\n",
      "        20 31197375617.1538            2.02s\n",
      "        30 19869753167.7997            2.21s\n",
      "        40 17628500126.4088            1.86s\n",
      "        30 20102930727.1046            1.91s\n",
      "        40 16233064389.5821            2.05s\n",
      "        50 15970352517.8190            1.69s\n",
      "        40 16820967286.4521            1.79s\n",
      "        50 14526773491.2324            1.84s\n",
      "        60 15059689067.5295            1.59s\n",
      "        50 15190835782.6063            1.71s\n",
      "        60 13706292287.9340            1.70s\n",
      "        70 14128863791.8027            1.44s\n",
      "        60 14285638773.7769            1.59s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=friedman_mse, init=zero, learning_rate=0.01, loss=quantile, max_depth=7, max_features=sqrt, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   1.1s\n",
      "        70 12937573670.0104            1.55s\n",
      "        80 13596190058.4371            1.32s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=friedman_mse, init=zero, learning_rate=0.01, loss=quantile, max_depth=7, max_features=sqrt, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   1.1s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=friedman_mse, init=zero, learning_rate=0.01, loss=quantile, max_depth=7, max_features=sqrt, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   1.1s\n",
      "        70 13712850620.8868            1.45s\n",
      "        80 12413189478.4215            1.43s\n",
      "        90 12972545727.3406            1.19s\n",
      "        80 13135098503.6793            1.33s\n",
      "        90 11952092190.4157            1.30s\n",
      "       100 12456780851.4533            1.08s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.05, loss=huber, max_depth=3, max_features=sqrt, max_leaf_nodes=None, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.2s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.05, loss=huber, max_depth=3, max_features=sqrt, max_leaf_nodes=None, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.2s\n",
      "        90 12504446308.4258            1.23s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      663731.7759      110506.8138            0.29s\n",
      "         2      574271.9446       79672.8099            0.60s\n",
      "         3      518420.3850       80839.2083            0.68s\n",
      "         4      456645.2831       46548.9085            0.76s\n",
      "       100 11553605093.4926            1.17s\n",
      "         5      433309.5898       69183.3163            1.05s\n",
      "         6      367132.9258      -22997.5008            1.07s\n",
      "         7      360309.6434       77668.8967            1.16s\n",
      "         8      339856.9812       44661.2121            1.29s\n",
      "         9      308076.0574       -2042.0675            1.35s\n",
      "        10      284070.9686       21125.2921            1.32s\n",
      "       100 11976617575.2772            1.11s\n",
      "        20      182040.6153        8642.9056            1.37s\n",
      "        30      165859.7552       -5997.2129            1.09s\n",
      "        40      165344.4081      -16507.7932            0.88s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.05, loss=huber, max_depth=3, max_features=sqrt, max_leaf_nodes=None, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.2s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      661766.1692      109567.7320            0.60s\n",
      "         2      576952.6552       87944.5571            0.68s\n",
      "         3      514142.3318       74924.0347            0.75s\n",
      "         4      447970.3061       38185.9588            1.01s\n",
      "         5      413005.4867       53488.8903            1.03s\n",
      "        50      173416.9730        3560.5772            0.79s\n",
      "         6      351271.0320        7478.2012            1.14s\n",
      "         7      340024.6133       48162.2821            1.31s\n",
      "        60      179843.1543       22287.9557            0.68s\n",
      "         8      316373.3673       40964.0143            1.41s\n",
      "         9      286394.3257        6438.3757            1.43s\n",
      "        10      255646.6808       13855.1708            1.40s\n",
      "        70      166836.0678       -6652.8344            0.61s\n",
      "        80      171632.2146       -3232.9320            0.53s\n",
      "        90      164303.0233        4624.5486            0.47s\n",
      "        20      181140.9859       13301.1054            1.41s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=huber, max_depth=7, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.7s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      678412.3297      109869.0723            0.32s\n",
      "         2      582228.6567       67459.0723            0.42s\n",
      "         3      517612.3746       71254.8946            0.58s\n",
      "       100      167082.5857       13811.8399            0.42s\n",
      "        30      160869.1962       -8211.7286            1.11s\n",
      "         4      446760.7898       30277.8375            0.95s\n",
      "         5      413087.0370       47098.4679            1.09s\n",
      "         6      359212.9963       16624.5498            1.07s\n",
      "         7      332510.6507       32807.2770            1.16s\n",
      "        40      165061.2312       -9966.0035            0.88s\n",
      "         8      318767.3468       42722.8263            1.13s\n",
      "         9      300007.0060       34517.7746            1.13s\n",
      "        10      276961.4566        7176.5383            1.23s\n",
      "        50      171692.1061        6278.9451            0.77s\n",
      "        60      172341.3054       13523.7265            0.70s\n",
      "        20      175729.7140       16442.5883            1.23s\n",
      "        70      164217.1882        8522.6217            0.61s\n",
      "        80      165443.9004       -5547.9874            0.54s\n",
      "        30      167193.5845       18245.3681            1.05s\n",
      "        90      169662.6228       11602.7575            0.47s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=huber, max_depth=7, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   1.6s\n",
      "        40      164186.8089       -5356.4927            0.89s\n",
      "       100      166545.3122        6781.7932            0.42s\n",
      "        50      167227.2263      -13836.8609            0.75s\n",
      "        60      166217.6762       21906.7110            0.66s\n",
      "        70      160775.7500        5122.6919            0.57s\n",
      "       200      163248.3713       -6574.2953            0.00s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=squared_error, init=zero, learning_rate=0.2, loss=absolute_error, max_depth=5, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   0.7s\n",
      "        80      164841.7751       16521.0450            0.50s\n",
      "        90      158450.2746       11494.1648            0.43s\n",
      "       100      161295.5286       12551.8355            0.38s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=friedman_mse, init=zero, learning_rate=0.1, loss=squared_error, max_depth=7, max_features=log2, max_leaf_nodes=10, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.2s\n",
      "       200  9982795849.0366            0.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.1, loss=huber, max_depth=3, max_features=None, max_leaf_nodes=10, min_impurity_decrease=0.0, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   2.1s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      676931.1339      109593.3670            0.10s\n",
      "         2      588425.8158       66678.4427            0.11s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=friedman_mse, init=zero, learning_rate=0.1, loss=squared_error, max_depth=7, max_features=log2, max_leaf_nodes=10, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.1s\n",
      "         3      528913.0341      113893.1507            0.11s\n",
      "         4      463030.3299        1015.8039            0.11s\n",
      "         5      422808.0332       69302.9156            0.14s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      656455.9221      110686.9142            0.14s\n",
      "         6      391127.5757       54211.0350            0.14s\n",
      "         7      354358.3307       -1210.1879            0.14s\n",
      "         2      580480.7509      124327.1269            0.16s\n",
      "         8      331254.9440       33044.1570            0.13s\n",
      "         9      297462.8348       -3198.7477            0.13s\n",
      "         3      516829.5809      102035.8472            0.19s\n",
      "        10      280724.0387       28085.5604            0.12s\n",
      "         4      462885.2866       22055.8846            0.17s\n",
      "         5      411780.9662       26761.2539            0.16s\n",
      "       200      167841.0809        6309.5161            0.00s\n",
      "         6      382251.5582       58395.8033            0.15s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=squared_error, init=zero, learning_rate=0.2, loss=absolute_error, max_depth=5, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   0.8s\n",
      "         7      354624.6974        4019.8693            0.15s\n",
      "         8      325396.2798       33788.8609            0.16s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      662767.6640      110675.3785            0.07s\n",
      "         9      294731.4284       44917.1650            0.18s\n",
      "         2      568476.1941       52251.4239            0.08s\n",
      "        10      275239.4054      -25335.8898            0.16s\n",
      "         3      521560.1018      136082.4513            0.11s\n",
      "       200  9149454585.3804            0.00s\n",
      "         4      457174.4767       11416.8133            0.14s\n",
      "         5      420179.0160       60549.6652            0.15s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.1, loss=huber, max_depth=3, max_features=None, max_leaf_nodes=10, min_impurity_decrease=0.0, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   2.2s\n",
      "        20      217169.1592      -19642.1499            0.12s\n",
      "         6      385394.2911       61493.0545            0.18s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=huber, max_depth=7, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   2.0s\n",
      "         7      347422.9439      -24221.9649            0.18s\n",
      "         8      334462.7777       58018.6794            0.18s\n",
      "         9      320855.9534       29661.5182            0.16s\n",
      "        10      298281.6319      -17994.6666            0.18s\n",
      "        20      199321.5911      -20235.4712            0.14s\n",
      "        30      194400.5640      -33429.7251            0.08s\n",
      "       200      168050.9811         162.7580            0.00s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=squared_error, init=zero, learning_rate=0.2, loss=absolute_error, max_depth=5, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   0.7s\n",
      "        20      210129.1551       46916.0204            0.12s\n",
      "        30      187983.7922      -11008.1410            0.09s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=squared_error, init=zero, learning_rate=0.2, loss=absolute_error, max_depth=None, max_features=sqrt, max_leaf_nodes=10, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.1s\n",
      "       200  9898271481.3669            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "        30      177325.3584       11864.0626            0.07s\n",
      "        40      177800.6159       -1271.5172            0.04s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.1, loss=huber, max_depth=3, max_features=None, max_leaf_nodes=10, min_impurity_decrease=0.0, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   2.1s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=squared_error, init=zero, learning_rate=0.2, loss=absolute_error, max_depth=None, max_features=sqrt, max_leaf_nodes=10, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.1s\n",
      "         1 264083568070.8041 -231896015598.1087            1.18s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         2 255072791976.3589 -4852250335.9229            1.00s\n",
      "         1 242979922328.6033 -206089079374.6774            0.47s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         2 230200336067.5601 -8107478854.5197            0.66s\n",
      "         1 241056593497.9563 -237310102970.3388            0.72s\n",
      "         3 252079162412.7206  4652551292.5706            1.00s\n",
      "         2 228556470963.9621 -8693431282.4726            0.59s\n",
      "         3 221932506977.9868 -3358302005.4802            0.70s\n",
      "        50      175490.5824        -320.5897            0.00s\n",
      "         3 230817502686.4072 13298607236.4255            0.54s\n",
      "         4 228605649986.7918 -25200093176.5151            0.98s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=squared_error, init=zero, learning_rate=0.2, loss=absolute_error, max_depth=None, max_features=sqrt, max_leaf_nodes=10, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.2s\n",
      "         4 225323642953.9886  1276899544.3509            0.52s\n",
      "         4 202902441920.2433 -20787238788.9274            0.73s\n",
      "         5 230712985358.2782 11581361446.0072            0.88s\n",
      "         5 220861402155.0217  3422679367.4901            0.51s\n",
      "         5 213979549302.8637 23881865942.2388            0.66s\n",
      "         6 222912224841.1041 -1692585962.5430            0.80s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         6 240923529564.3315 37777331731.7351            0.50s\n",
      "         6 203527903309.3909 -6808415705.7157            0.62s\n",
      "         7 215420466620.9168 -28276691221.7006            0.49s\n",
      "         7 209479755918.9379 18654466619.5525            0.60s\n",
      "         7 220313356082.5113  6653565184.8265            0.84s\n",
      "         8 216953166767.5610 11926551641.4663            0.48s\n",
      "         1      398210.4036       19396.9992            1.12s\n",
      "         8 215194708288.8461 16316266467.1547            0.58s\n",
      "         9 208232037881.0671 -5400168928.7169            0.47s\n",
      "         8 249847977487.8444 54606959709.6776            0.82s\n",
      "        10 199348460884.0970 -4426541961.8115            0.45s\n",
      "         2      366343.5104      -40525.6285            1.01s\n",
      "         9 190143369115.7866 -28736131980.2304            0.62s\n",
      "         9 215579338920.4508 -41268636737.3260            0.79s\n",
      "         3      349658.8552       19419.8135            0.96s\n",
      "        10 183709442772.5212 -2170019173.7386            0.61s\n",
      "        10 217036720693.9102 11335666681.2411            0.78s\n",
      "         4      322403.1565      -20865.9161            0.94s\n",
      "         5      325992.4608       87917.8831            0.95s\n",
      "         6      305849.9815      -23189.6404            1.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=friedman_mse, init=zero, learning_rate=0.1, loss=squared_error, max_depth=7, max_features=log2, max_leaf_nodes=10, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.5s\n",
      "         7      296896.8529       29621.2177            0.94s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         8      292442.9805       29095.9438            0.93s\n",
      "        20 189351144094.5428 48489950736.6004            0.46s\n",
      "         1      379222.8757       19544.6067            1.10s\n",
      "         9      272370.2196      -25580.3844            0.94s\n",
      "         2      355103.5693       -8041.4842            0.91s\n",
      "        10      264672.1082       15302.1005            0.97s\n",
      "         3      339648.8278       24046.4599            1.15s\n",
      "         4      316620.8560      -11500.7932            1.03s\n",
      "        20 178480615221.0114 -32777267932.9270            0.71s\n",
      "         5      313117.0638       54059.6045            1.00s\n",
      "         6      290231.2401      -16659.7268            0.99s\n",
      "        20 165649345603.7651 -13024865136.1675            0.71s\n",
      "        30 157466182946.5286 34171432894.2684            0.41s\n",
      "         7      285510.0425       37037.2571            0.98s\n",
      "         8      276591.0731       25075.5229            0.95s\n",
      "         9      256958.8391      -31583.3066            0.90s\n",
      "        10      245129.8924       -1946.3404            0.90s\n",
      "        30 139720196204.5327 -5959603622.1405            0.60s\n",
      "        30 137851452103.2913 -2167428711.5659            0.58s\n",
      "        40 137660869168.6199 15487143555.8276            0.37s\n",
      "        20      199208.8394        4708.5854            0.89s\n",
      "        40 118839300892.8018  7712804293.8570            0.45s\n",
      "        40 138475245706.6320 12779542312.7265            0.51s\n",
      "        20      192405.2835       23230.9484            0.86s\n",
      "        50 116697387369.8246 -6867233747.4689            0.33s\n",
      "        30      154642.4048       19656.7808            0.76s\n",
      "        50 118548556317.2280  7847703763.2380            0.40s\n",
      "        50 104537928795.4032 -20967424189.2790            0.39s\n",
      "        60 91937442879.0432 -11588712511.1638            0.26s\n",
      "        30      152348.5982       -3472.0850            0.77s\n",
      "        60 87989412412.8491 -10919196563.1115            0.31s\n",
      "        40      145214.6881        4046.0475            0.63s\n",
      "        70 91517389340.7832 22399443495.8066            0.20s\n",
      "        60 94472259736.3044 -16560531776.0788            0.33s\n",
      "        70 82892852070.2817  9858588911.0537            0.23s\n",
      "        80 71056917683.7579  2990212801.2836            0.14s\n",
      "        50      131459.6597       -5069.2956            0.52s\n",
      "        70 90750071509.3263  3740261587.6249            0.25s\n",
      "        40      137176.6583       18376.6130            0.68s\n",
      "        80 76193162923.0697  5372132683.8055            0.15s\n",
      "        80 84591682493.4192  2258167839.1917            0.16s\n",
      "        90 62131289265.1963 -2767486443.9063            0.07s\n",
      "        60      122855.0708       -7130.5216            0.42s\n",
      "        50      127066.3750      -17306.4375            0.56s\n",
      "        90 67024906932.1080  1822628171.5731            0.08s\n",
      "        90 73042313799.2637 -2371579821.1283            0.08s\n",
      "       100 65328329460.3209 -3677477259.4191            0.00s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.01, loss=huber, max_depth=None, max_features=sqrt, max_leaf_nodes=30, min_impurity_decrease=0.1, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   0.7s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      381523.4232       19004.5418            1.23s        70      120362.1630       -7341.2578            0.32s\n",
      "\n",
      "         2      364346.8363       18354.5012            1.00s\n",
      "       100 60077670613.0627  1720452890.3607            0.00s\n",
      "         3      344748.0389        5898.9132            1.02s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.01, loss=huber, max_depth=None, max_features=sqrt, max_leaf_nodes=30, min_impurity_decrease=0.1, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   0.8s\n",
      "        60      118072.5829       -2012.7431            0.46s\n",
      "       100 66715107051.5600  7668000002.7868            0.00s\n",
      "         4      327105.2243       17535.6034            1.02s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      777167.5860            0.07s\n",
      "         2      771673.0860            0.06s\n",
      "         3      766233.5310            0.05s\n",
      "         5      312379.4317       28419.3373            0.99s\n",
      "         4      760848.3715            0.05s\n",
      "         5      755517.0637            0.05s\n",
      "         6      750239.0689            0.04s\n",
      "         7      745013.8541            0.04s\n",
      "         8      739840.8914            0.04s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.01, loss=huber, max_depth=None, max_features=sqrt, max_leaf_nodes=30, min_impurity_decrease=0.1, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   0.8s\n",
      "         9      734720.4365            0.05s\n",
      "        10      729654.1209            0.05s\n",
      "         6      303249.6967       25954.3773            1.02s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      763283.6957            0.13s\n",
      "         7      280223.4174      -30748.5519            0.98s\n",
      "         2      757739.6957            0.12s\n",
      "         3      752251.1357            0.10s\n",
      "         4      746817.4613            0.10s\n",
      "         5      741438.1236            0.10s\n",
      "         8      277167.9062       30891.7371            0.95s\n",
      "         6      736112.5793            0.10s\n",
      "         7      730840.2905            0.10s\n",
      "         8      725620.7245            0.11s\n",
      "         9      720454.2447            0.10s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.2, loss=huber, max_depth=None, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   0.9s\n",
      "         9      266217.7815       11878.3142            0.95s\n",
      "        20      681931.3953            0.07s\n",
      "        10      715341.7169            0.10s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.2, loss=huber, max_depth=None, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   1.0s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.2, loss=huber, max_depth=None, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   1.0s\n",
      "         1      762942.3118            0.05s\n",
      "         2      757428.0118            0.05s\n",
      "         3      751968.8548            0.05s\n",
      "        80      123139.3301         587.8121            0.21s\n",
      "         4      746564.2894            0.05s\n",
      "         5      741213.7696            0.05s\n",
      "         6      735916.7551            0.05s\n",
      "         7      730672.7106            0.04s\n",
      "         8      725481.1066            0.04s\n",
      "        10      255271.7794       11646.2486            0.95s\n",
      "         9      720341.4187            0.04s\n",
      "        10      715253.9735            0.04s\n",
      "        20      667167.4847            0.07s\n",
      "        30      639519.4439            0.05s\n",
      "        20      667281.7618            0.06s\n",
      "        70      115950.0857        3602.1682            0.34s\n",
      "        30      624301.1533            0.06s\n",
      "        30      624612.2590            0.05s\n",
      "        40      602250.5421            0.03s\n",
      "        40      586726.0098            0.03s\n",
      "        40      587078.8446            0.03s\n",
      "        90      113577.2749      -17425.1605            0.11s\n",
      "        50      569660.2324            0.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.01, loss=absolute_error, max_depth=None, max_features=sqrt, max_leaf_nodes=10, min_impurity_decrease=0.01, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   0.2s\n",
      "        50      554007.6370            0.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.01, loss=absolute_error, max_depth=None, max_features=sqrt, max_leaf_nodes=10, min_impurity_decrease=0.01, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   0.2s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time         50      554463.5818            0.00s\n",
      "\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.01, loss=absolute_error, max_depth=None, max_features=sqrt, max_leaf_nodes=10, min_impurity_decrease=0.01, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   0.1s\n",
      "        20      185310.9565       15719.4961            0.96s\n",
      "         1      463121.6742      236113.5626            0.44s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         2      313131.7975      113603.7638            0.52s\n",
      "         1      455372.7093      236408.0824            0.65s\n",
      "         3      256296.9241      105430.2012            0.57s\n",
      "         2      315127.8417      164783.5149            0.62s\n",
      "        80      114438.1946       -1118.3757            0.23s\n",
      "         4      223300.1480       19931.0396            0.60s\n",
      "         5      215842.2164       42743.1626            0.61s\n",
      "         3      246242.6826       55014.4576            0.75s\n",
      "         6      207864.7467        5466.3697            0.61s\n",
      "         4      213430.0531       14949.5684            0.82s\n",
      "         7      203091.3990         917.5655            0.67s\n",
      "         5      201688.7244       26972.0846            0.79s\n",
      "         8      202924.6909       12186.8895            0.66s\n",
      "         6      185577.9703      -27133.0468            0.78s\n",
      "         9      196075.9548      -20421.6901            0.66s\n",
      "         7      189900.1738       38111.7229            0.83s\n",
      "       100      113616.9782       -6828.3080            0.00s\n",
      "        10      195944.7766        3686.7335            0.67s\n",
      "         8      185511.5109       -5542.4345            0.84s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.1, loss=absolute_error, max_depth=7, max_features=None, max_leaf_nodes=10, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   1.1s\n",
      "         9      178085.9449      -22490.9569            0.81s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "        10      180123.9508       12497.8465            0.79s\n",
      "         1      454556.4406      244341.0344            0.76s\n",
      "        30      155572.4739       13944.4743            0.80s\n",
      "         2      308203.3452      129511.8288            0.77s\n",
      "         3      245389.4563       73967.4997            0.74s\n",
      "        90      112420.1346        8291.5582            0.11s\n",
      "         4      209478.3344       -7379.1014            0.85s\n",
      "         5      197860.3947       20698.8844            0.80s\n",
      "         6      191557.7418        9651.0861            0.77s\n",
      "         7      191063.8987       17036.1161            0.83s\n",
      "         8      190647.1735        7219.6382            0.84s\n",
      "        20      190687.4932       -8667.0664            0.67s\n",
      "         9      185216.0600      -16212.3438            0.83s\n",
      "        20      182693.9932       19832.5550            0.69s\n",
      "        10      182776.0768       -5762.5213            0.81s\n",
      "        40      142835.5303       16721.2290            0.65s\n",
      "       100      106843.1522        2208.3855            0.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.1, loss=absolute_error, max_depth=7, max_features=None, max_leaf_nodes=10, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   1.1s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      590497.7489      124623.9254            0.50s\n",
      "        30      179276.9102      -13262.7714            0.56s\n",
      "         2      467964.4818       49068.5645            1.11s\n",
      "        30      191449.6111      -26127.8276            0.60s\n",
      "         3      374491.0604       92860.3268            1.37s\n",
      "         4      287045.7531       46076.0865            1.47s\n",
      "        20      184925.0188       39478.8144            0.73s\n",
      "         5      242193.9356      128684.5797            1.84s\n",
      "         6      193108.9258       24646.7332            1.79s\n",
      "         7      157967.5896       49751.1617            1.84s\n",
      "         8      134117.9340       37848.5623            1.80s\n",
      "        40      183911.9630       -6364.2116            0.46s\n",
      "        50      132330.5242        4554.7490            0.56s\n",
      "        40      201023.7436       13192.7882            0.49s\n",
      "         9      107682.7036       -1151.8999            1.92s\n",
      "        10       94432.4989       26333.2306            1.95s\n",
      "        30      188713.6760       58367.2321            0.60s\n",
      "        50      188540.8228       15125.5381            0.38s\n",
      "        50      202230.7606       13596.5039            0.41s\n",
      "        60      118672.5717      -22700.3856            0.44s\n",
      "        40      183657.0691      -12726.6980            0.51s\n",
      "        20       40671.9996         758.0465            2.01s\n",
      "        60      184823.9657       13765.8240            0.31s\n",
      "        60      201327.9864       24087.1764            0.34s\n",
      "        50      180864.3910      -24930.3953            0.41s\n",
      "        70      125573.5478       25092.7916            0.32s\n",
      "        70      182415.2721       24065.6991            0.23s\n",
      "        30       28940.5620        3187.9084            1.88s\n",
      "        70      196040.8498       10311.0723            0.25s\n",
      "        60      182458.0132       17581.6512            0.34s\n",
      "        80      118575.0538       27507.5751            0.21s\n",
      "        80      183601.6764       -5715.5895            0.15s\n",
      "        80      198013.5848      -18265.2718            0.16s\n",
      "        40       29423.0698        5563.0475            1.66s\n",
      "        70      181126.2765       20201.2678            0.26s\n",
      "        90      116390.0916        1408.6320            0.11s\n",
      "        90      192905.0070      -11051.3160            0.08s\n",
      "        90      181861.7086        5908.2822            0.08s\n",
      "        50       27384.5368         358.1994            1.55s\n",
      "       100      195789.4055      -12675.9412            0.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=quantile, max_depth=7, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   0.8s\n",
      "        80      185833.6982       23067.0365            0.17s\n",
      "       100      113937.8282        4585.6758            0.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.1, loss=absolute_error, max_depth=7, max_features=None, max_leaf_nodes=10, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   1.0s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      573716.4267      123767.7210            0.57s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      576295.6297      123460.4623            0.22s\n",
      "         2      458233.7315       93699.8702            1.69s\n",
      "         2      465726.7278      112723.6212            1.04s\n",
      "         3      366361.1576      102044.8954            1.73s\n",
      "         3      369036.5519       90070.0020            1.47s\n",
      "       100      181650.8906      -13613.0816            0.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=quantile, max_depth=7, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   0.8s\n",
      "         4      282186.9576       47125.8681            1.89s\n",
      "         4      290004.8276       80614.4369            1.48s\n",
      "         5      230708.8240       97563.3996            1.83s\n",
      "         5      235594.3281       63323.8075            1.49s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         6      183388.4513       19113.9672            1.77s\n",
      "         6      194584.4990       58031.2645            1.49s\n",
      "         1      561693.4699       73961.0706            0.13s\n",
      "         2      485493.2772       12073.3982            0.14s\n",
      "         3      430072.4113       51596.0827            0.13s\n",
      "         7      153229.6496       51116.0450            1.73s\n",
      "         4      377207.3665       14530.6634            0.12s\n",
      "         5      355446.7924       98548.8740            0.11s\n",
      "         7      153512.2895       -4864.9179            1.65s\n",
      "         6      317150.4258       22035.6014            0.12s\n",
      "         8      128005.9515       39164.4664            1.70s\n",
      "        60       25756.2662        1154.0317            1.46s\n",
      "         7      288573.3787       29308.2495            0.13s\n",
      "         8      131601.5297       46617.6188            1.64s\n",
      "         9      103000.9214       -2585.5697            1.67s\n",
      "         8      261488.3640       46459.1722            0.14s\n",
      "         9      227656.3095       -2172.6271            0.13s\n",
      "        90      184818.4580        5123.4278            0.09s\n",
      "        10      213248.9980       23464.5864            0.13s\n",
      "         9      111757.7407       18038.1892            1.61s\n",
      "        10       86107.5259        5105.4907            1.64s\n",
      "        10       96835.7466       18977.3353            1.60s\n",
      "        20      138355.2989        3264.3231            0.11s\n",
      "        70       24589.2537          34.5089            1.33s\n",
      "        30      101588.0176       14765.1920            0.07s\n",
      "       100      182942.3338        1426.0952            0.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=quantile, max_depth=7, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   0.9s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.01, criterion=squared_error, init=zero, learning_rate=0.1, loss=quantile, max_depth=None, max_features=sqrt, max_leaf_nodes=20, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   0.1s\n",
      "        20       38300.6203        4059.9555            1.67s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      546042.2430       73968.7753            0.06s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         2      477755.1815       49828.6294            0.07s\n",
      "        20       42155.0239        4833.7659            1.69s\n",
      "         3      423144.8144       62596.9870            0.07s\n",
      "         1      547920.3001       73979.7242            0.12s\n",
      "         4      370675.5789       15840.5953            0.07s\n",
      "         5      328640.3912       87006.1199            0.08s\n",
      "         2      483719.4646       66285.0041            0.16s\n",
      "         6      290978.8523       14655.8833            0.09s\n",
      "         7      266435.5891       40832.5230            0.09s\n",
      "         3      426285.6666       51967.2944            0.20s\n",
      "         8      238949.1081       35824.1168            0.09s\n",
      "         9      205028.0462       -1658.9770            0.09s\n",
      "         4      382349.6165       45978.8367            0.21s\n",
      "        10      187233.0109        7344.2717            0.09s\n",
      "         5      347938.8997       47286.5116            0.22s\n",
      "         6      315724.2343       44327.1657            0.20s\n",
      "         7      276091.8956      -13550.1095            0.19s\n",
      "         8      250183.1579       50360.6394            0.18s\n",
      "         9      223785.6165       24437.1980            0.17s\n",
      "        10      207773.3094       16104.7902            0.16s\n",
      "        20      129945.2479       23756.1819            0.09s\n",
      "        80       25042.9089        1359.8586            1.25s\n",
      "        20      134402.0689       23023.8737            0.13s\n",
      "        30       27827.6105       -2742.4011            1.67s\n",
      "        30       30951.8342       -6397.9489            1.62s\n",
      "        30      102360.0910       -5112.4435            0.08s\n",
      "        30      109849.1927        9625.7272            0.09s\n",
      "        40      102376.5101       16751.5889            0.04s\n",
      "        40      104328.4874       14732.5725            0.04s\n",
      "        40       31264.5430        6091.2973            1.48s\n",
      "        40       26068.1420        1553.5892            1.52s\n",
      "        90       23862.7175       -2153.6814            1.15s\n",
      "        50       97793.3461      -24132.7825            0.00s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.01, criterion=squared_error, init=zero, learning_rate=0.1, loss=quantile, max_depth=None, max_features=sqrt, max_leaf_nodes=20, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   0.2s\n",
      "        50      100932.2640       -8874.5259            0.00s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.01, criterion=squared_error, init=zero, learning_rate=0.1, loss=quantile, max_depth=None, max_features=sqrt, max_leaf_nodes=20, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   0.2s\n",
      "        50       29129.6556        4020.7589            1.38s\n",
      "       100       23062.8401        -467.5000            1.03s\n",
      "        50       24572.6558       -3997.4610            1.44s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=squared_error, init=zero, learning_rate=0.01, loss=huber, max_depth=7, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   1.5s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=squared_error, init=zero, learning_rate=0.01, loss=huber, max_depth=7, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   1.3s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      469846.0545            0.48s\n",
      "         2      327395.2148            0.52s\n",
      "         3      260586.0278            0.67s\n",
      "         4      229700.9172            0.65s\n",
      "         5      214875.5622            0.70s\n",
      "         6      207293.1649            0.72s\n",
      "        60       26063.0006        -813.1388            1.31s\n",
      "         7      203230.4453            0.75s\n",
      "         8      200831.1808            0.70s\n",
      "         9      199383.0459            0.67s\n",
      "        10      198508.4142            0.68s\n",
      "        60       23440.9079        -847.2026            1.37s\n",
      "        20      196968.0066            0.62s\n",
      "        30      196919.2023            0.52s\n",
      "        40      196917.3114            0.46s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=quantile, max_depth=None, max_features=sqrt, max_leaf_nodes=20, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.2, verbose=1, warm_start=True; total time=   0.1s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        70       26735.0632        3625.5844            1.24s\n",
      "         1      454609.8475            0.49s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=squared_error, init=zero, learning_rate=0.01, loss=huber, max_depth=7, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   1.6s\n",
      "         2      311485.5775            0.80s\n",
      "         3      245015.6554            0.92s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         4      215518.8222            0.92s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=squared_error, init=zero, learning_rate=0.1, loss=quantile, max_depth=5, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   0.7s\n",
      "         1      458810.3342            0.85s\n",
      "        70       22554.6599        -396.0140            1.30s\n",
      "         5      201063.5944            0.98s\n",
      "         2      315871.1713            0.77s\n",
      "         3      249530.0616            0.67s\n",
      "         6      193697.8029            0.96s\n",
      "         4      219897.5698            0.62s\n",
      "         5      205904.8988            0.58s\n",
      "         7      189620.0724            0.91s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         8      187363.5275            0.87s\n",
      "         6      198882.0170            0.66s\n",
      "         9      186055.5948            0.86s\n",
      "         1 225139794035.5300 -244760234652.8008            1.58s\n",
      "        10      185300.4589            0.82s\n",
      "         7      194988.0963            0.72s\n",
      "         8      192756.3709            0.71s\n",
      "         2 195836191047.7487 50365450238.4551            1.49s\n",
      "         9      191414.0548            0.70s\n",
      "        10      190602.7294            0.67s\n",
      "         3 186348481909.9724 57761683951.4072            1.73s\n",
      "         4 146878648979.3758 -2407175066.5243            1.53s\n",
      "        20      184073.4710            0.64s\n",
      "         5 124401118901.8651 16015602805.8355            1.76s\n",
      "         6 111601824206.9446 22500341269.6335            1.75s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=quantile, max_depth=None, max_features=sqrt, max_leaf_nodes=20, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.2, verbose=1, warm_start=True; total time=   0.1s\n",
      "        20      189252.0324            0.63s\n",
      "         7 84721496684.7121 -9170462524.1787            1.66s\n",
      "         8 80616836985.4368 24238214764.6152            1.56s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         9 70941949808.8787  8188651186.8798            1.66s\n",
      "        30      189243.8880            0.55s\n",
      "         1 201211209470.9507 -222679144839.5817            3.14s\n",
      "        10 68037750199.0977 15834579541.8180            1.64s\n",
      "         2 183852130317.1600 50157771553.6155            2.15s\n",
      "         3 162324832783.1658 35271387283.4517            1.74s\n",
      "         4 145867518342.2317 22226719260.4532            1.54s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=quantile, max_depth=None, max_features=sqrt, max_leaf_nodes=20, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.2, verbose=1, warm_start=True; total time=   0.1s\n",
      "         5 116340371058.6044 -2660939874.9946            1.47s\n",
      "         6 98826007280.7634  8133215262.6703            1.40s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         7 91093850458.7463 18230891318.9442            1.39s\n",
      "        80       22498.8018        2171.7877            1.24s\n",
      "         1 216352216897.9001 -209487102343.1668            1.88s\n",
      "         8 81439391129.6151 17363642892.5915            1.42s\n",
      "         2 173414922688.5832  8189299439.8386            1.45s\n",
      "         9 72685495378.1064  8110568531.9652            1.38s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=squared_error, init=zero, learning_rate=0.1, loss=quantile, max_depth=5, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   1.4s\n",
      "        10 60509297265.3213  -246624260.1580            1.36s\n",
      "         3 172413239993.4955 60054183222.9735            1.68s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         4 139669932205.7507  3080468492.3605            1.68s\n",
      "         1      210419.3077       20497.1882            1.34s\n",
      "         5 123046357617.6576 27545539643.0159            1.59s\n",
      "         2      189568.3631        6694.5854            1.31s\n",
      "         6 112166172869.5836 19528642422.3748            1.55s\n",
      "         3      180594.0714       31324.3663            1.16s\n",
      "         4      164678.5606      -13606.2683            1.15s\n",
      "         5      154722.2594       26447.3465            1.25s\n",
      "         7 84125001502.0503 -16012456198.0658            1.77s        20 32010290144.8164  1179565650.5983            1.60s\n",
      "\n",
      "         6      145987.9483       18791.5375            1.29s\n",
      "         8 89171799187.5816 31866589192.2374            1.81s\n",
      "         7      135592.0125      -25215.9581            1.27s\n",
      "         8      133960.8838        9304.7310            1.20s\n",
      "         9      125666.3559       -3447.8221            1.21s\n",
      "         9 77729271755.6184  7458006781.9737            1.92s\n",
      "        10      127744.8209       21933.3464            1.18s\n",
      "        10 62849058289.0104 -1983753363.9422            1.83s\n",
      "        20 27717476833.5983  1331917707.7820            1.34s\n",
      "        90       22226.7889         -85.6642            1.14s\n",
      "        30 21759348888.2098  1709730216.9773            1.45s\n",
      "        20      112575.9835      -17130.5148            1.00s\n",
      "        20 32834151331.3549   828282569.6235            1.55s\n",
      "        30 20772147081.2614  -473065904.0168            1.28s\n",
      "        30      107203.3874      -21086.4116            0.88s\n",
      "       100       21082.8388       -1176.9076            1.03s\n",
      "        40      108123.4801         976.4920            0.77s\n",
      "        30 22151943099.8877   754189440.1403            1.33s\n",
      "        40 15417300331.0277  1300868640.2725            1.20s\n",
      "        50      104889.6130       -4237.9846            0.68s\n",
      "        40 17469564021.6948  4294143029.7242            1.48s\n",
      "        60      108385.6640       20218.7380            0.62s\n",
      "        40 14851087058.2386 -1487446081.0857            1.20s\n",
      "        50 15252936542.1019  1737279811.0771            1.08s\n",
      "        70      106785.1121        2747.4791            0.56s\n",
      "        50 14600621022.1262 -1828402815.8110            1.40s\n",
      "        80      103152.2487      -27708.8434            0.51s\n",
      "        60 13757561486.5937   671937860.5346            0.97s\n",
      "        50 13767528227.5051 -4250409830.1268            1.14s\n",
      "        90      105103.3171       -1647.6602            0.46s\n",
      "        60 15963162582.1749  4819165931.3388            1.27s\n",
      "        60 14027944943.4493  2646274500.0039            1.00s\n",
      "        70 12092365907.2694   403047688.1600            0.89s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.1, loss=quantile, max_depth=7, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.4s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      194918.6407       21894.8089            1.16s\n",
      "         2      181583.1071       37751.7195            1.01s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=squared_error, init=zero, learning_rate=0.1, loss=quantile, max_depth=5, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   1.3s\n",
      "         3      171365.4633       27355.8969            0.91s\n",
      "         4      157392.0709       -4787.7965            0.86s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         5      150459.3739       11089.3738            0.83s\n",
      "         6      146646.8970        9278.4981            0.82s\n",
      "         1      199784.2444       19431.8587            1.45s\n",
      "         7      134912.5740       -1465.0944            0.89s\n",
      "        80 11756256279.9685 -1194264113.5078            0.79s\n",
      "         2      175969.0289       -6131.0770            1.78s\n",
      "         8      127733.6231        2907.3906            0.93s\n",
      "         9      126491.6890       10418.9635            0.92s\n",
      "         3      174039.2453       59561.1040            1.66s\n",
      "        70 13397227175.8639  2082290886.3295            0.92s\n",
      "        10      118396.3952       -7666.1137            0.92s\n",
      "         4      159308.4878       -8017.5044            1.53s\n",
      "         5      150687.8630       16290.5538            1.44s\n",
      "         6      141640.9793       12032.7371            1.38s\n",
      "         7      130472.5079      -24391.3614            1.33s\n",
      "         8      130195.9971       32305.4589            1.27s\n",
      "         9      126042.4131       10390.6596            1.20s\n",
      "        10      117207.3485      -23645.4890            1.16s\n",
      "        70 13514254964.4571  2051552996.8607            1.19s\n",
      "        20      102473.0529      -16018.7633            0.84s\n",
      "        90 11380601115.5520   177482656.0872            0.73s\n",
      "        80 12481666607.2618  1383420988.9111            0.84s\n",
      "        20      111308.1743       23162.2309            1.00s\n",
      "        30      100328.8716       -4934.1122            0.76s\n",
      "        80 12197940620.8172 -2809225802.6456            1.08s\n",
      "       100 11087120586.3432  1235145814.0417            0.65s\n",
      "        30      101718.4919        9468.0649            0.85s\n",
      "        90 10940955095.7056   947474338.3929            0.76s\n",
      "        40       94146.1286       -6984.1827            0.72s\n",
      "        40      101964.3968        5001.9938            0.73s\n",
      "        50       97246.6343       11179.5685            0.63s\n",
      "        90 12395282517.7022  -400049103.7339            0.96s\n",
      "        60       97565.1911       11183.0689            0.55s\n",
      "        50       95838.4504      -28980.1447            0.67s\n",
      "       100 11880701211.2194  -193578576.6480            0.71s\n",
      "        70       96068.3539       12302.0926            0.50s\n",
      "        60       96842.1645       17415.5497            0.61s\n",
      "       100 12252474255.1014  2922561104.0420            0.86s\n",
      "        70       99006.5488       28100.6143            0.54s\n",
      "        80       93362.2475      -21845.3427            0.48s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.1, loss=quantile, max_depth=7, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.3s\n",
      "        90       96973.3100        8544.0151            0.42s\n",
      "       100       96090.4103       -1380.3333            0.38s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.1, loss=quantile, max_depth=7, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.5s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.1, loss=huber, max_depth=5, max_features=sqrt, max_leaf_nodes=30, min_impurity_decrease=0.1, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   1.3s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.1, loss=huber, max_depth=5, max_features=sqrt, max_leaf_nodes=30, min_impurity_decrease=0.1, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   1.3s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      729351.8892            0.34s\n",
      "         2      680146.3802            0.38s\n",
      "         3      636690.2077            0.46s\n",
      "         4      598781.2287            0.49s\n",
      "         5      565902.5482            0.48s\n",
      "         6      535851.5282            0.52s\n",
      "         7      507522.6989            0.64s\n",
      "         8      481631.7601            0.73s\n",
      "         9      460464.9417            0.75s\n",
      "        10      438137.9677            0.75s\n",
      "        20      307204.1631            0.69s\n",
      "        30      245014.0927            0.65s\n",
      "        40      225632.0915            0.60s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.1, loss=huber, max_depth=5, max_features=sqrt, max_leaf_nodes=30, min_impurity_decrease=0.1, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   1.7s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      718237.2188            0.20s\n",
      "        50      204687.2993            0.54s\n",
      "         2      668119.2854            0.27s\n",
      "         3      623831.4515            0.29s\n",
      "         4      585336.2430            0.37s\n",
      "         5      552064.0037            0.41s\n",
      "         6      522502.9253            0.47s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.05, loss=huber, max_depth=None, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   2.0s\n",
      "         7      494005.4284            0.56s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      718995.3677            0.21s\n",
      "         8      468305.8545            0.56s\n",
      "         2      668852.7650            0.27s\n",
      "         3      624595.2307            0.26s\n",
      "         4      586011.1638            0.36s\n",
      "         9      447220.8411            0.61s\n",
      "         5      550791.9887            0.36s\n",
      "         6      521545.2119            0.35s\n",
      "        10      425170.6939            0.61s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.05, loss=huber, max_depth=None, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   2.0s\n",
      "        60      196276.4882            0.50s\n",
      "         7      492865.0229            0.43s\n",
      "         8      467147.8945            0.44s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      746613.6167       27750.0000            0.07s\n",
      "         2      716453.8584       21152.1739            0.07s\n",
      "         9      446061.7078            0.46s\n",
      "         3      700841.1563       38370.8327            0.07s\n",
      "         4      670970.5455       13500.4883            0.07s\n",
      "        10      423375.9351            0.47s\n",
      "         5      668190.8062       51868.5002            0.09s\n",
      "         6      610291.5760      -35402.0746            0.11s\n",
      "         7      620969.9906       66617.0016            0.11s\n",
      "         8      610998.8266       30331.6493            0.11s\n",
      "         9      580149.4573       -4235.6870            0.11s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.05, loss=huber, max_depth=None, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   1.9s\n",
      "        10      563376.8903       16305.0492            0.12s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "        20      285492.7009            0.50s\n",
      "         1      743959.4419       27500.0000            0.17s\n",
      "        70      191611.0913            0.48s\n",
      "         2      718650.8220       28588.2949            0.16s\n",
      "         3      697791.7654       31793.7297            0.14s\n",
      "         4      663043.6734        6812.5239            0.13s\n",
      "         5      648625.0332       34559.1154            0.14s\n",
      "         6      604630.3522      -14010.0065            0.14s\n",
      "         7      601518.7663       46034.5593            0.14s\n",
      "        20      301059.7462            0.88s\n",
      "         8      587993.9877       25266.2792            0.13s\n",
      "         9      564432.7358        8206.1456            0.13s\n",
      "        80      191332.7283            0.42s\n",
      "        10      539761.4201        4370.8189            0.13s\n",
      "        20      433032.4225        7240.2600            0.10s\n",
      "        30      232479.4012            0.54s\n",
      "        90      190536.4831            0.39s\n",
      "        40      212147.2393            0.52s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=squared_error, init=zero, learning_rate=0.1, loss=absolute_error, max_depth=3, max_features=sqrt, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.3s\n",
      "        30      246605.9657            0.93s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      760785.2801       27550.0000            0.07s\n",
      "         2      722920.6743        8762.5584            0.06s\n",
      "         3      700047.7694       29506.5572            0.06s\n",
      "         4      660294.7536       -1228.5926            0.06s\n",
      "        30      339086.6657        1995.3933            0.09s\n",
      "         5      642793.7407       28773.1314            0.07s\n",
      "         6      607551.7040        -599.5272            0.07s\n",
      "         7      590197.6083       25007.2927            0.08s\n",
      "        20      428638.0291       17563.3061            0.16s\n",
      "         8      580765.9760       32509.0202            0.09s\n",
      "        50      193085.7084            0.50s\n",
      "         9      571445.2330       31296.3498            0.10s\n",
      "        10      552235.6207       13410.3668            0.10s\n",
      "        40      222541.1650            0.82s\n",
      "        60      187532.7301            0.46s\n",
      "        50      200311.5407            0.73s\n",
      "        40      288034.5746      -10391.7910            0.05s\n",
      "        30      324003.5629         130.0731            0.12s\n",
      "        70      176239.5830            0.43s\n",
      "        20      408759.0743       19189.3006            0.13s\n",
      "        60      192454.7562            0.62s\n",
      "        80      176078.6790            0.39s\n",
      "        70      181067.8364            0.54s\n",
      "        40      275328.5513      -14249.0841            0.05s\n",
      "        30      345523.0728       39274.7150            0.08s\n",
      "        50      266579.5804        8304.3551            0.00s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=friedman_mse, init=zero, learning_rate=0.05, loss=absolute_error, max_depth=None, max_features=log2, max_leaf_nodes=20, min_impurity_decrease=0.1, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.3s\n",
      "        90      175699.3331            0.35s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=squared_error, init=zero, learning_rate=0.1, loss=absolute_error, max_depth=3, max_features=sqrt, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.3s\n",
      "         1      645531.6123            0.32s\n",
      "        80      179248.0315            0.48s\n",
      "        40      286809.0048       -2785.8491            0.04s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2      552132.7102            0.57s\n",
      "         1      637085.5756            0.37s\n",
      "         3      464620.9289            0.55s\n",
      "         2      545605.4503            0.44s\n",
      "        50      253475.2467       15259.4894            0.00s\n",
      "        90      178691.6336            0.43s\n",
      "        50      249157.7572      -22248.2401            0.00s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=friedman_mse, init=zero, learning_rate=0.05, loss=absolute_error, max_depth=None, max_features=log2, max_leaf_nodes=20, min_impurity_decrease=0.1, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.3s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=friedman_mse, init=zero, learning_rate=0.05, loss=absolute_error, max_depth=None, max_features=log2, max_leaf_nodes=20, min_impurity_decrease=0.1, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.2s\n",
      "         4      384336.9285            0.54s\n",
      "         3      459050.6015            0.55s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         5      323566.1543            0.52s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      637795.7129            0.26s\n",
      "       100      178691.6201            0.37s\n",
      "         1 401222801917.3282            0.36s\n",
      "         2      545537.8429            0.31s\n",
      "         2 336137713179.4709            0.46s\n",
      "         4      381292.8755            0.62s\n",
      "         6      272084.9632            0.52s\n",
      "         3 285214383557.4572            0.57s\n",
      "         3      458552.1775            0.37s\n",
      "         5      321048.2897            0.59s\n",
      "         4 243447655580.3939            0.59s\n",
      "         5 208641964835.4753            0.56s\n",
      "         4      378425.5990            0.42s\n",
      "         6      269699.3085            0.57s\n",
      "         6 180867775174.9681            0.57s\n",
      "         7      232623.6958            0.59s\n",
      "         5      316640.1585            0.43s\n",
      "         7 156778845058.1516            0.63s\n",
      "         7      234833.4921            0.56s\n",
      "         8 138300364642.9473            0.61s\n",
      "         6      267203.6121            0.45s\n",
      "         9 124515920683.7313            0.61s\n",
      "         8      200685.5712            0.56s\n",
      "        10 110451783051.2688            0.58s\n",
      "         8      201948.4289            0.65s\n",
      "         7      232562.3770            0.45s\n",
      "         9      174863.5170            0.53s\n",
      "         8      201833.9454            0.43s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=squared_error, init=zero, learning_rate=0.1, loss=absolute_error, max_depth=3, max_features=sqrt, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.5s\n",
      "        10      153193.6956            0.52s\n",
      "         9      176000.3418            0.42s\n",
      "         9      176461.1393            0.68s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.1, loss=squared_error, max_depth=5, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   1.2s\n",
      "         1 386010031222.7752            0.60s\n",
      "        10      154879.4126            0.42s\n",
      "         2 322016604798.7372            0.48s\n",
      "        10      152253.9786            0.66s\n",
      "         3 272617017065.0780            0.44s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         4 231471386194.2670            0.40s\n",
      "         1 389890654680.7206            0.58s\n",
      "        20 46713025292.1096            0.46s\n",
      "         5 199928289297.7703            0.46s\n",
      "         2 325235363039.1528            0.46s\n",
      "         6 173030343907.2599            0.46s\n",
      "         3 275282169374.3981            0.53s\n",
      "         7 149618419844.2385            0.50s\n",
      "         4 234758229466.4819            0.60s\n",
      "         5 202665350743.6568            0.53s\n",
      "         8 131071159829.1534            0.49s\n",
      "         6 175055223369.8730            0.48s\n",
      "         9 117554997091.3112            0.49s\n",
      "         7 151223479095.3424            0.49s\n",
      "        10 104153569570.1991            0.47s\n",
      "         8 132957661070.1139            0.51s\n",
      "         9 119054884976.0222            0.49s\n",
      "        10 106495386236.7334            0.46s\n",
      "        30 30577621190.5945            0.38s\n",
      "        20 44139677275.8191            0.37s\n",
      "        40 24743796586.9042            0.30s\n",
      "        30 29693970313.4791            0.29s\n",
      "        20 45096545127.8631            0.45s\n",
      "        20       55877.7089            0.34s\n",
      "        20       52974.9199            0.42s\n",
      "        50 20898748169.6038            0.26s\n",
      "        40 24133249719.2644            0.27s\n",
      "        30 30143814909.6543            0.40s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.1, loss=squared_error, max_depth=5, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   1.2s\n",
      "        60 18799596982.4055            0.20s\n",
      "        50 19878666102.2753            0.23s\n",
      "        40 24861940491.5679            0.33s\n",
      "        20       58138.4134            0.59s\n",
      "        70 17370206925.1518            0.15s\n",
      "        30       32960.6519            0.25s\n",
      "        60 17782571665.5331            0.18s\n",
      "        50 21164161975.4034            0.27s\n",
      "        80 16434946192.3852            0.10s\n",
      "        30       30029.8092            0.30s\n",
      "        70 16520103582.4374            0.14s\n",
      "        60 19132550138.0935            0.21s\n",
      "        90 15950951844.5797            0.05s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.1, loss=squared_error, max_depth=5, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   1.2s\n",
      "        80 15639693017.5773            0.14s\n",
      "        70 17848766977.8694            0.23s\n",
      "        30       33796.8659            0.49s\n",
      "       100 15207378337.2450            0.00s\n",
      "        40       26415.3919            0.18s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.1, loss=huber, max_depth=3, max_features=log2, max_leaf_nodes=30, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   0.7s\n",
      "        80 16784989775.2485            0.15s\n",
      "        90 14999888592.5673            0.07s\n",
      "       100 14581384871.3074            0.00s\n",
      "        40       23901.1376            0.21s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.1, loss=huber, max_depth=3, max_features=log2, max_leaf_nodes=30, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   0.7s\n",
      "        90 15941982451.9713            0.08s\n",
      "        40       26928.7777            0.23s\n",
      "       100 15319042193.2322            0.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.1, loss=huber, max_depth=3, max_features=log2, max_leaf_nodes=30, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   0.8s\n",
      "        50       23857.9542            0.00s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.05, loss=quantile, max_depth=7, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   1.0s\n",
      "        50       20907.0438            0.00s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.05, loss=quantile, max_depth=7, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   1.0s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 165349105916.5787 -131032419797.9755            1.09s\n",
      "         2 129645368884.3083 -12416349435.4072            1.41s\n",
      "         3 114211959100.2165 16837768951.9157            1.32s\n",
      "         4 93629399008.5889 -4998520181.0779            1.25s\n",
      "         5 89956299889.0926 50196020998.1263            1.37s\n",
      "         6 76263631303.5877 -3564127454.5753            1.40s\n",
      "        50       24434.9754            0.00s\n",
      "         7 70153992312.9963 15424684857.7745            1.50s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.05, loss=quantile, max_depth=7, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   1.1s\n",
      "         8 65025185085.7588 13399965075.4975            1.60s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         9 55959338842.3671 -5480600293.5490            1.62s\n",
      "         1 153291542110.3281 -137156478615.5544            1.14s\n",
      "        10 51723045883.0021  7568614780.6762            1.59s\n",
      "         2 126853064578.9050  6669599406.0735            1.20s\n",
      "         3 108695042480.9393 19701206221.5069            1.15s\n",
      "         4 90409418344.0790  1321578374.4063            1.27s\n",
      "         5 82464988353.4712 33505777845.1091            1.28s\n",
      "         6 70256431503.4936 -3580426605.4022            1.32s\n",
      "         7 64492694423.3910 17042224839.1691            1.40s\n",
      "         8 59423249860.3827 12047262810.8720            1.35s\n",
      "         9 52004849867.8761 -9581992306.1657            1.39s\n",
      "        10 46444411105.2372  2756347755.2995            1.35s\n",
      "        20 28780350727.2912  3055865785.3142            1.45s\n",
      "        20 27043549419.1554  4452518327.7535            1.32s\n",
      "        30 19090805064.6329  2423717359.6537            1.36s\n",
      "        30 19228973226.6683  -863725814.6255            1.31s\n",
      "        40 18415604302.3216   727880822.9766            1.31s\n",
      "        40 17067921688.7878  4263245113.2771            1.24s\n",
      "        50 16560036071.2708 -1336338071.7880            1.25s\n",
      "        50 15440655276.4954 -3072181496.1351            1.17s\n",
      "        60 15308281045.0368 -1283860408.4388            1.16s\n",
      "        60 14328737949.2518    86870068.4753            1.06s\n",
      "        70 14833612602.2585  -492102024.6774            1.08s\n",
      "        70 13815028458.6299   604905213.9326            0.98s\n",
      "        80 13740897524.5399   374954173.2374            0.89s\n",
      "        90 13424546261.4339  2055386120.5240            0.80s\n",
      "        80 15402259318.9526   768478101.1246            1.16s\n",
      "       100 12322342774.3986   161745449.3542            0.73s\n",
      "        90 13343030309.3365 -4040402903.4998            1.06s\n",
      "       100 13181013289.1760 -1855066680.9224            0.96s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.1, loss=huber, max_depth=3, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=True; total time=   1.4s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 155465637697.3054 -153521756504.2948            4.82s\n",
      "         2 133154325738.3975 27590015784.1271            3.47s\n",
      "         3 110879189652.0152  7822485650.4861            2.67s\n",
      "         4 97810756184.1885 18460366603.9705            2.60s\n",
      "         5 85985062626.7100 16952248814.7038            2.70s\n",
      "         6 78458238708.2116 18703528362.6747            2.59s\n",
      "         7 65033994729.8869 -10864912693.7936            2.49s\n",
      "         8 60397987530.7829 16095319223.6890            2.56s\n",
      "         9 53678832841.9090  5064266259.1289            2.48s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.2, loss=squared_error, max_depth=5, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   2.1s\n",
      "        10 49323783764.7858  7396718241.8712            2.40s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      390193.2187        9788.7735            1.11s\n",
      "         2      373752.4375      -16822.7693            1.12s\n",
      "         3      375522.3066       49392.8890            0.99s\n",
      "         4      362344.6798        -725.0256            1.06s\n",
      "         5      360505.4612       38215.1384            1.02s\n",
      "         6      353061.9974        7047.5066            0.97s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.2, loss=squared_error, max_depth=5, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   2.4s\n",
      "         7      344821.0367        -260.7361            0.93s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         8      342401.0866       22731.9651            0.92s\n",
      "         1      382509.3202        9917.5535            0.82s\n",
      "        20 27124521806.1305  3446385084.8539            2.02s\n",
      "         2      376120.5597       24464.7875            0.79s\n",
      "         9      331470.2586      -15079.1567            0.95s\n",
      "         3      365369.5619        -682.4416            0.84s\n",
      "        10      321524.5047        4889.8495            0.93s\n",
      "         4      353003.2852       -5759.0553            0.97s\n",
      "         5      348586.6846       20172.6757            0.96s\n",
      "         6      329721.6618      -28426.5247            0.96s\n",
      "         7      331765.7712       44490.1543            0.99s\n",
      "         8      322852.6059         226.7157            0.96s\n",
      "         9      309543.9702      -24451.8340            1.04s\n",
      "        30 20718177830.6614  1042320904.2946            1.80s\n",
      "        10      307393.6356       22674.7705            0.99s\n",
      "        20      259697.4882       -4584.7148            0.83s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.2, loss=squared_error, max_depth=5, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   2.2s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      392270.0681        9313.6187            0.66s\n",
      "         2      378031.9521      -11631.8645            0.64s\n",
      "        40 18524494480.1963  2850023084.8795            1.64s\n",
      "         3      371785.8783       20442.4852            0.65s\n",
      "         4      353138.5254      -33735.8576            0.89s\n",
      "         5      346368.1128       25356.8564            0.95s\n",
      "        20      254714.8846       21162.3082            0.99s\n",
      "        30      215732.0139      -29184.1630            0.76s\n",
      "         6      338441.7122        9215.8585            1.07s\n",
      "         7      334386.5504       17423.5548            1.15s\n",
      "         8      329106.1676       15527.5673            1.08s\n",
      "         9      317342.0219      -12684.2372            1.02s\n",
      "        50 17395371023.8851  1844523560.9225            1.50s\n",
      "        10      308272.8320        -517.5193            0.98s\n",
      "        40      201128.5938       10470.1773            0.71s\n",
      "       200 11809169435.9311   385481454.4137            0.00s\n",
      "        60 14875068168.7439 -5249682044.6216            1.49s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.1, loss=huber, max_depth=3, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=True; total time=   2.2s\n",
      "        30      206174.0838      -11905.5755            1.00s\n",
      "        20      253152.6581       45961.2909            1.08s\n",
      "        70 16132978014.8724  4849368389.8102            1.33s\n",
      "        50      186924.2663        9310.1177            0.61s\n",
      "        40      187404.0084       -7401.1751            0.81s\n",
      "        30      218108.3014       69517.1902            0.94s\n",
      "        80 14887229761.9778  5735252385.3380            1.21s\n",
      "        60      177530.8248       20555.5684            0.48s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=squared_error, init=zero, learning_rate=0.01, loss=absolute_error, max_depth=5, max_features=sqrt, max_leaf_nodes=30, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=5, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   0.2s\n",
      "        50      175492.9609       11929.0627            0.66s\n",
      "        90 14749650473.9981  1079657873.9230            1.09s\n",
      "        40      192774.2659      -10757.7962            0.80s\n",
      "        70      164741.2411        6466.9316            0.34s\n",
      "       100 14479923558.7655   991568539.3532            0.96s\n",
      "        60      162743.4336        6672.8168            0.50s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=squared_error, init=zero, learning_rate=0.01, loss=absolute_error, max_depth=5, max_features=sqrt, max_leaf_nodes=30, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=5, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   0.1s\n",
      "        80      166198.3879       -8130.0384            0.22s\n",
      "        50      172220.3201      -22502.0515            0.65s\n",
      "        70      157479.3269       23373.8296            0.36s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=squared_error, init=zero, learning_rate=0.01, loss=absolute_error, max_depth=5, max_features=sqrt, max_leaf_nodes=30, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=5, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   0.1s\n",
      "        90      158583.6947       -9693.8784            0.10s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      644051.4350            0.39s\n",
      "         2      585702.0785            0.37s\n",
      "         3      531459.2182            0.35s\n",
      "         4      481794.2510            0.50s\n",
      "         5      437656.6399            0.45s\n",
      "         6      399206.4842            0.51s\n",
      "         7      366607.6533            0.55s\n",
      "         8      339068.5856            0.62s\n",
      "         9      316251.9352            0.64s\n",
      "        10      297177.4462            0.64s\n",
      "       100      159375.1208      -18240.7801            0.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=squared_error, init=None, learning_rate=0.05, loss=absolute_error, max_depth=None, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   1.0s\n",
      "        80      155503.9248       -2934.7004            0.23s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      634497.5249            0.24s\n",
      "         2      575684.0500            0.29s\n",
      "         3      521054.8730            0.30s\n",
      "         4      471160.1253            0.30s\n",
      "         5      426888.1093            0.29s\n",
      "        60      164934.1179       15002.9532            0.49s\n",
      "         6      388427.5329            0.29s\n",
      "         7      355565.4472            0.30s\n",
      "         8      327765.4097            0.30s\n",
      "         9      304613.9380            0.31s\n",
      "        10      285317.2997            0.30s\n",
      "        20      218485.7215            0.77s\n",
      "        20      206527.1684            0.31s\n",
      "        90      154796.6551       16725.5250            0.11s\n",
      "        30      203648.7977            0.66s\n",
      "        30      191254.1462            0.33s\n",
      "        40      199680.3440            0.52s\n",
      "        40      187299.2201            0.40s\n",
      "        50      198386.1291            0.50s\n",
      "       100      153650.8377      -10285.8960            0.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=squared_error, init=None, learning_rate=0.05, loss=absolute_error, max_depth=None, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   1.0s\n",
      "        70      156361.3736       19229.6342            0.37s\n",
      "        50      186102.5545            0.39s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=squared_error, init=zero, learning_rate=0.05, loss=quantile, max_depth=3, max_features=sqrt, max_leaf_nodes=10, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.2s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      635176.5555            0.25s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=squared_error, init=zero, learning_rate=0.05, loss=quantile, max_depth=3, max_features=sqrt, max_leaf_nodes=10, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.1s\n",
      "         2      576369.1574            0.44s\n",
      "         3      521698.0894            0.47s\n",
      "         4      471885.0931            0.44s\n",
      "         5      427908.5108            0.42s\n",
      "         6      389610.1528            0.40s\n",
      "         7      356895.1946            0.47s\n",
      "         8      329075.4308            0.51s\n",
      "         9      305950.2848            0.58s\n",
      "        10      286688.3753            0.58s\n",
      "        20      207697.6277            0.65s\n",
      "        80      157487.0119       26745.8092            0.24s\n",
      "        90      153846.0887       -3853.3353            0.11s\n",
      "        30      193068.2879            0.89s\n",
      "        40      189200.1257            0.73s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=squared_error, max_depth=3, max_features=log2, max_leaf_nodes=None, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=5, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.2s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.05, loss=huber, max_depth=5, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   3.2s\n",
      "        50      187981.7358            0.61s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 582250712339.6859 182553947806.9373            0.38s\n",
      "         2 475908831643.5063 48550983307.5522            0.49s\n",
      "       100      153885.4350        -205.3497            0.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=squared_error, init=zero, learning_rate=0.05, loss=quantile, max_depth=3, max_features=sqrt, max_leaf_nodes=10, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.2s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=squared_error, max_depth=3, max_features=log2, max_leaf_nodes=None, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=5, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.2s\n",
      "         3 625576571543.8027 1118635563078.4519            0.49s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=squared_error, init=None, learning_rate=0.05, loss=absolute_error, max_depth=None, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   1.1s\n",
      "         4 364732659449.1526 -690468085490.2910            0.51s\n",
      "         5 488783685326.6166 845245284667.4741            0.52s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         6 426840303281.3884 35022623964.5927            0.50s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 526637539560.5461 91194221302.3414            0.53s\n",
      "         7 372887282929.0101 35271961872.8117            0.48s\n",
      "         1 746509419098.3230 76161356243.4938            0.43s\n",
      "         8 327241621431.7170 30018126948.4262            0.45s\n",
      "         2 615865507123.7219 -84717575443.6210            0.38s\n",
      "         2 462701926800.4205 180273073103.4977            0.81s\n",
      "         9 271575939965.4860 -18010501346.9649            0.45s\n",
      "         3 544777239104.8008 161029415487.6483            0.46s\n",
      "        10 239076858279.3991 19419549822.4960            0.44s\n",
      "         4 451291324312.5875 -55349309072.8911            0.49s\n",
      "         3 391189545234.5501 49222771228.4763            0.87s\n",
      "         5 390170108997.4016 45517147219.2148            0.47s\n",
      "         6 239592395094.2572 -406999675439.0079            0.45s\n",
      "         4 334766260585.1616 59989282636.1779            0.86s\n",
      "         7 316557615146.2693 521193561166.3278            0.42s\n",
      "         5 283811100864.2536 36172277061.3128            0.79s\n",
      "         6 228989790716.7174 -27264475652.5301            0.71s         8 280223668278.2936 47959905966.2154            0.48s\n",
      "\n",
      "         9 245843585432.6204 -7993685915.2539            0.49s\n",
      "         7 204360098535.7911 79795273827.3291            0.73s\n",
      "         8 179107410187.7066 24346211132.2642            0.68s\n",
      "        10 213446964743.9125 -8708329218.8114            0.55s\n",
      "         9 144418395874.2107 -42907498935.8871            0.67s\n",
      "        10 127829768396.0597 41306788830.8665            0.66s\n",
      "        20 56696382606.5976 -146475463539.1926            0.45s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=squared_error, max_depth=3, max_features=log2, max_leaf_nodes=None, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=5, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.1s\n",
      "        20 52924967771.8978 21085697803.8609            0.50s\n",
      "        20 97601680604.5489 203449198061.4846            0.57s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.05, loss=huber, max_depth=5, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   3.2s\n",
      "        30 29187930698.2995 -111945003632.0614            0.46s\n",
      "        30 59906388490.2677 101317235390.7224            0.53s\n",
      "        30 28858779578.0731 -6713910679.5586            0.58s\n",
      "        40 37954571159.5280  6793840767.6553            0.46s\n",
      "        40 41343117906.0455 -9420002568.8814            0.45s\n",
      "        40 22378850151.7567  2763187705.4355            0.49s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.05, loss=huber, max_depth=5, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   3.5s\n",
      "        50 34354936562.1033  5048325366.0197            0.38s\n",
      "        50 27784216495.9834 -18290078065.6002            0.37s\n",
      "        50 19819455942.3798  6399373523.7013            0.39s\n",
      "        60 26599244464.3441  1302639270.5398            0.30s\n",
      "        60 26242083188.9216  8364986714.0480            0.29s\n",
      "        60 16623810043.6509 -2015691582.5309            0.30s\n",
      "        70 21510633876.2316 -2005294935.3987            0.22s\n",
      "        70 22632799259.2931 11978747660.6636            0.24s\n",
      "        70 16182680963.0699  3065631445.4027            0.25s\n",
      "        80 18977047115.9386   -22409791.0237            0.16s\n",
      "        80 21289412466.4614  8583313578.3565            0.17s\n",
      "        80 15013723156.6842  -644019340.4378            0.17s\n",
      "        90 15813531606.8809 -5014324684.7646            0.08s\n",
      "        90 13573304161.0689  1104231788.7766            0.08s\n",
      "        90 19096331337.5796  2892950936.0319            0.09s\n",
      "       100 14951717504.5562 -2318636675.4338            0.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.1, loss=squared_error, max_depth=7, max_features=log2, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.9s\n",
      "       100 16889389322.1837  3907427516.5235            0.00s\n",
      "       100 12504541956.2418 -2458594893.0352            0.00s\n",
      "       200 12715601591.9197  1391823748.4457            0.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.1, loss=squared_error, max_depth=7, max_features=log2, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.9s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.1, loss=squared_error, max_depth=7, max_features=log2, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.9s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.1, loss=huber, max_depth=3, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=True; total time=   2.4s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.0, criterion=squared_error, init=None, learning_rate=0.1, loss=quantile, max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.7s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.0, criterion=squared_error, init=None, learning_rate=0.1, loss=quantile, max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.6s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.0, criterion=squared_error, init=None, learning_rate=0.1, loss=quantile, max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.7s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 905474488192.4496            2.40s\n",
      "         2 891219868643.8284            1.76s\n",
      "         3 877248915821.2488            1.65s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         4 863560623547.6439            2.11s\n",
      "         5 850139440214.2866            1.89s\n",
      "         6 836988553684.7468            1.69s\n",
      "         1 630854357689.8832            2.89s\n",
      "         2 620008844938.1344            2.12s\n",
      "         3 609377072008.6487            1.79s\n",
      "         7 824095099092.3141            2.06s\n",
      "         8 811461233135.1990            2.04s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.2, loss=huber, max_depth=7, max_features=log2, max_leaf_nodes=30, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   1.7s\n",
      "         4 599036968586.9911            2.45s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 778970839775.1902            0.89s\n",
      "         5 588814575909.0000            2.45s\n",
      "         9 799073475195.1587            2.21s\n",
      "         6 578778532319.9873            2.20s\n",
      "         2 767284546402.2902            2.67s\n",
      "        10 786937431794.0509            2.29s\n",
      "         7 569027186204.0332            2.50s\n",
      "         3 755822300603.5914            3.19s\n",
      "         8 559388162445.6912            2.64s\n",
      "         4 744595540809.9567            3.11s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.2, loss=huber, max_depth=7, max_features=log2, max_leaf_nodes=30, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   1.8s\n",
      "         9 549938936300.3530            2.56s\n",
      "        10 540740596158.2021            2.38s\n",
      "         5 733591276170.4994            2.99s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         6 722796846409.8345            2.88s\n",
      "         1 336962570826.9252 -327003440700.6871            5.03s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.2, loss=huber, max_depth=7, max_features=log2, max_leaf_nodes=30, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   1.8s\n",
      "         7 712222471729.0171            2.69s\n",
      "         2 273863643670.5219 47200133433.9382            3.90s\n",
      "         3 234083439085.0824 71134715513.9216            3.34s\n",
      "         8 701847978754.7018            2.56s\n",
      "         4 190767302522.2104  5323836405.8106            3.01s\n",
      "         5 166467847817.3778 40755685286.3829            2.64s\n",
      "         9 691680855796.4587            2.38s\n",
      "         6 143304169108.4008 32589681952.7299            2.45s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 319180064881.5800 -347626241523.6256            1.08s\n",
      "        10 681710166247.8009            2.28s\n",
      "         2 268730311379.5026 87867158618.7425            1.10s\n",
      "         7 117529822321.9904  4852212620.7385            2.65s\n",
      "         3 228399998028.6780 61723685907.1242            1.21s\n",
      "         8 101476720200.3982 22517915038.4471            2.52s\n",
      "         4 187963446142.6042 19830535149.5194            1.35s\n",
      "        20 677909187012.7183            1.88s\n",
      "         5 159852001947.7964 20531715846.4996            1.29s\n",
      "         6 136700469009.7182 32601770459.8130            1.25s\n",
      "         9 84096692850.2252  3717512079.9542            2.64s\n",
      "         7 113400840598.8186 11455199434.4112            1.22s\n",
      "         8 97543971282.4642 18067648955.4312            1.40s\n",
      "        10 74343457812.3121 19681785611.3393            2.63s\n",
      "         9 84684333858.0985 16295290041.0119            1.38s\n",
      "        10 70221515815.3643  1470719388.4749            1.33s\n",
      "        20 457211251477.3180            1.75s\n",
      "        20 25017914341.7796 -2153816022.5471            2.46s\n",
      "        20 592071127352.9117            1.78s\n",
      "        30 586104487020.4498            1.56s\n",
      "        20 21776316773.1438 -3252623028.3630            2.16s\n",
      "        30 388243685920.9652            1.47s\n",
      "        30 16477591918.1959 -3632253087.6100            2.86s\n",
      "        30 14907214417.4704 -1432205187.7204            2.71s\n",
      "        30 517416018537.3536            1.70s\n",
      "        40 508068740767.4005            1.45s\n",
      "        40 331295990179.5427            1.34s\n",
      "        40 13426190676.1096  1296658211.2166            2.48s\n",
      "        40 11520210936.1564  -704873980.3747            2.44s\n",
      "        50 11558015378.5422  -665103640.9136            2.26s\n",
      "        50 11450139678.2259  1407279937.8953            2.08s\n",
      "        40 455633263794.9934            1.41s\n",
      "        50 444006233396.1475            1.22s\n",
      "        50 283866398935.2841            1.15s\n",
      "        60 11519494989.5364  3435134378.9132            2.11s\n",
      "        60 10808904300.4523  1178920039.3223            2.00s\n",
      "        70 11007349135.1385   500044253.8686            1.89s\n",
      "        50 404775083965.6896            1.19s\n",
      "        60 390089275570.8367            0.95s\n",
      "        70 10090126812.6949  1252352168.2857            1.84s\n",
      "        60 243827765029.6021            0.90s\n",
      "        80 10412616088.0575 -1415662618.5472            1.67s\n",
      "        60 362023609482.0182            0.91s\n",
      "        80  9723811000.1867 -1043214915.5339            1.68s\n",
      "        70 344577708236.8554            0.69s\n",
      "        70 210681738805.1882            0.67s\n",
      "        90 10192585342.1531   205742427.7536            1.59s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=squared_error, init=zero, learning_rate=0.1, loss=squared_error, max_depth=5, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   2.3s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 324875562999.0038 -329803881577.4429            6.94s\n",
      "         2 264427455940.0281 28944436428.2897            5.80s\n",
      "       100  9977231447.5430  3603671518.1477            1.44s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=squared_error, init=zero, learning_rate=0.1, loss=squared_error, max_depth=5, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   2.4s\n",
      "        70 325281805681.8986            0.70s\n",
      "         3 232238189930.2551 95979026040.0551            5.72s\n",
      "        90  9646965140.8206  1098803585.3063            1.73s\n",
      "         4 189538985700.5979 11157606170.3523            5.66s\n",
      "         5 163843718610.8539 36146691400.4206            4.94s\n",
      "         6 139204116531.8198 34784078137.4003            4.48s\n",
      "         7 112935736678.9618 -4495638103.4918            3.95s\n",
      "        80 307236665574.2030            0.47s\n",
      "         8 100821728821.4469 32417012820.3770            4.37s\n",
      "        80 183349413016.3628            0.47s\n",
      "         9 86948397631.1636 19310368772.7999            4.71s\n",
      "        10 71618120959.2274   581212837.4307            4.31s\n",
      "       100  9388383223.1862   950338183.3588            1.58s\n",
      "        20 25242762033.0588  9259133038.1412            2.87s\n",
      "        80 294381031800.9582            0.47s\n",
      "        90 276305305470.1284            0.23s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=squared_error, init=zero, learning_rate=0.1, loss=squared_error, max_depth=5, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   2.6s\n",
      "        90 160636136456.4352            0.23s\n",
      "        30 15458768987.3303  1568403708.3780            2.38s\n",
      "       100 249671331417.6293            0.00s\n",
      "        40 13149778408.1526  -400557356.3878            2.21s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.1, criterion=friedman_mse, init=None, learning_rate=0.01, loss=squared_error, max_depth=5, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.1, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   2.3s\n",
      "       100 141493469607.0684            0.00s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.1, criterion=friedman_mse, init=None, learning_rate=0.01, loss=squared_error, max_depth=5, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.1, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   2.3s\n",
      "        90 268373250617.8224            0.24s\n",
      "        50 11536803869.5349 -3602879001.3644            2.09s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      371200.6490       41917.6805            1.68s\n",
      "         2      329344.0779       38370.5596            1.61s\n",
      "         3      294066.0681       41425.8070            1.66s\n",
      "         4      246320.9389       14819.2311            1.81s\n",
      "         5      222943.8706       30075.6873            1.82s\n",
      "        60 10786167056.7132  2614299856.3196            1.90s\n",
      "         6      206960.7155       10805.0620            1.78s\n",
      "       100 246553962722.1334            0.00s\n",
      "         7      191115.8660       12245.8563            1.73s\n",
      "         8      196681.2114       55009.1763            1.58s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.1, criterion=friedman_mse, init=None, learning_rate=0.01, loss=squared_error, max_depth=5, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.1, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   2.4s\n",
      "         9      166863.3542      -14137.7452            1.51s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      359332.3959       43677.2101            0.69s\n",
      "        10      161085.8025       14858.1762            1.45s\n",
      "         2      313449.6917       31160.3244            0.70s\n",
      "         3      275900.5192       32745.3852            1.30s\n",
      "         4      236983.8365        8618.1638            1.34s\n",
      "        70 10663934587.7025  3574511055.8641            1.70s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=friedman_mse, init=zero, learning_rate=0.01, loss=squared_error, max_depth=5, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.8s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         5      220796.5929       39074.6379            1.49s\n",
      "         1      359175.1217       41359.5296            1.31s\n",
      "         6      196233.6277       14064.6733            1.46s\n",
      "         7      186256.8930       23525.0154            1.37s\n",
      "         2      309429.0644       29557.5507            1.51s\n",
      "         8      183535.4489       26183.4822            1.31s\n",
      "         9      159851.1731       -8016.0563            1.24s\n",
      "         3      282229.8994       41371.6951            1.46s\n",
      "        10      149301.8850        4548.0295            1.19s\n",
      "         4      249022.6332       25249.1419            1.47s\n",
      "        20      125448.8106      -17165.3148            1.17s\n",
      "        80 10215087953.6117   262130913.1266            1.50s\n",
      "         5      222602.9010       27598.2192            1.62s\n",
      "         6      216413.1852       44637.0334            1.53s\n",
      "         7      187627.3642       -4862.8489            1.43s\n",
      "         8      177998.9891       20572.0557            1.35s\n",
      "         9      163661.0242        3804.5830            1.38s\n",
      "        10      154786.3224        4068.6026            1.36s\n",
      "        90  9479197792.9209 -4055228459.2491            1.38s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=friedman_mse, init=zero, learning_rate=0.01, loss=squared_error, max_depth=5, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.8s\n",
      "        30      106861.3249       -3441.2619            1.04s\n",
      "        20      122126.1214       -7077.3601            1.19s\n",
      "       100  9949696100.3889  -615922178.8546            1.21s\n",
      "       200  8498351726.6150  2946835759.1754            0.00s\n",
      "        20      134186.5180       27868.1511            1.17s\n",
      "        30      114354.0590        2814.0455            0.90s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=squared_error, init=zero, learning_rate=0.1, loss=huber, max_depth=5, max_features=sqrt, max_leaf_nodes=None, min_impurity_decrease=0.1, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   2.6s\n",
      "        40      118940.7095        6541.1890            0.85s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=friedman_mse, init=zero, learning_rate=0.01, loss=squared_error, max_depth=5, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.8s\n",
      "       200  7891186680.2107   191462900.2202            0.00s\n",
      "        30      122039.7536       17910.1232            0.98s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=squared_error, init=zero, learning_rate=0.1, loss=huber, max_depth=5, max_features=sqrt, max_leaf_nodes=None, min_impurity_decrease=0.1, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   2.7s\n",
      "        40      108096.8224        5567.9944            0.81s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      401392.9327         999.3872            1.17s\n",
      "         2      408988.4361       13866.7758            1.12s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=squared_error, init=None, learning_rate=0.2, loss=absolute_error, max_depth=5, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   0.8s\n",
      "         3      427375.9904       30001.7854            1.03s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      387399.6394        1019.8270            0.63s\n",
      "         4      408353.7631      -24803.9195            1.06s\n",
      "         5      400560.2276       -9410.5914            1.13s\n",
      "        50      103322.8790      -14454.1312            0.67s\n",
      "         2      398700.8928       19497.2888            1.29s\n",
      "        40      118996.8600        1981.5423            0.90s\n",
      "         6      407490.0330       12690.0951            1.03s\n",
      "         3      402877.9322        8769.9000            1.06s\n",
      "         7      390216.6360      -23272.9809            0.94s\n",
      "         8      401587.7643       19337.1799            0.87s\n",
      "         4      409378.4970       12291.8744            1.11s\n",
      "         9      401231.4868        1721.7807            0.84s\n",
      "         5      384305.2143      -34502.7081            1.04s\n",
      "        10      395770.2408       -6058.9576            0.78s\n",
      "         6      379778.7185       -4497.4651            1.01s\n",
      "         7      387992.4252       14697.1502            0.97s\n",
      "         8      394845.3145       12520.5641            0.91s\n",
      "        50      117672.9823       -1204.3495            0.73s\n",
      "        60      102224.7027       -7472.8733            0.54s\n",
      "         9      396691.7660        4920.1344            0.89s\n",
      "        10      378084.7344      -25403.1203            0.86s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=squared_error, init=None, learning_rate=0.2, loss=absolute_error, max_depth=5, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   0.8s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      397541.8313         982.2062            0.95s\n",
      "         2      386651.2264      -13840.9802            0.70s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=squared_error, init=zero, learning_rate=0.1, loss=quantile, max_depth=7, max_features=log2, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   0.6s\n",
      "         3      409080.4741       36158.3329            0.62s\n",
      "         4      388810.7931      -27930.3658            0.62s\n",
      "         5      398075.4634       16273.3979            0.58s\n",
      "         6      403455.8508       10477.2457            0.58s\n",
      "         7      377906.1422      -35965.5931            0.59s\n",
      "         8      402303.1745       38810.1383            0.58s\n",
      "        20      395639.2452        3126.1194            0.69s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=squared_error, init=None, learning_rate=0.2, loss=absolute_error, max_depth=5, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   1.0s\n",
      "         9      398972.6425       -2842.6616            0.58s\n",
      "        10      384623.7816      -19475.2465            0.57s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=squared_error, init=zero, learning_rate=0.1, loss=quantile, max_depth=7, max_features=log2, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   0.7s\n",
      "        20      374574.7717        6707.1090            0.70s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=squared_error, init=zero, learning_rate=0.1, loss=quantile, max_depth=7, max_features=log2, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   0.7s\n",
      "        30      378331.4805       -9275.2401            0.43s\n",
      "        20      382742.8477       -4461.9451            0.48s\n",
      "        30      360577.8870      -10988.0369            0.42s\n",
      "       200  8598826935.9973   451514102.6387            0.00s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=squared_error, init=zero, learning_rate=0.1, loss=huber, max_depth=5, max_features=sqrt, max_leaf_nodes=None, min_impurity_decrease=0.1, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   2.2s\n",
      "        30      364738.0682       -5802.8668            0.31s\n",
      "        40      369567.2803       33217.2417            0.20s\n",
      "        40      351094.5675       19163.7249            0.20s\n",
      "        40      353824.3136       -2578.8345            0.16s\n",
      "        50      361613.7450        8349.7473            0.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=friedman_mse, init=None, learning_rate=0.01, loss=absolute_error, max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.01, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   1.0s\n",
      "        50      348113.2926       10683.1634            0.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=friedman_mse, init=None, learning_rate=0.01, loss=absolute_error, max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.01, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   1.0s\n",
      "        50      347658.9532      -29145.9729            0.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=friedman_mse, init=None, learning_rate=0.01, loss=absolute_error, max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.01, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.8s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.1, loss=huber, max_depth=None, max_features=sqrt, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   0.6s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.1, loss=huber, max_depth=None, max_features=sqrt, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   0.6s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.1, loss=huber, max_depth=None, max_features=sqrt, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   0.5s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=squared_error, init=None, learning_rate=0.1, loss=absolute_error, max_depth=5, max_features=log2, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   1.3s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.01, loss=absolute_error, max_depth=None, max_features=None, max_leaf_nodes=10, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   0.4s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 923654980930.3630            0.41s[CV] END alpha=0.95, ccp_alpha=0.0, criterion=squared_error, init=None, learning_rate=0.1, loss=absolute_error, max_depth=5, max_features=log2, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   1.3s\n",
      "\n",
      "         2 878175475163.4861            0.58s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         3 834879900428.7332            0.59s\n",
      "         1 619538147627.9945            0.37s\n",
      "         2 581955449800.5233            0.31s\n",
      "         3 544539273402.7665            0.28s\n",
      "         4 794961273900.3132            0.60s\n",
      "         4 511945768017.9788            0.27s\n",
      "         5 482948944829.8760            0.26s\n",
      "         5 771101967472.8916            0.57s\n",
      "         6 736231652458.3916            0.51s\n",
      "         6 456796523351.6262            0.27s\n",
      "         7 706864650614.8302            0.52s\n",
      "         7 428613676045.9277            0.33s\n",
      "         8 680649580394.0782            0.49s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=squared_error, init=None, learning_rate=0.1, loss=absolute_error, max_depth=5, max_features=log2, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   1.3s\n",
      "         8 407308255661.2825            0.34s\n",
      "         9 658472505674.1079            0.46s\n",
      "         9 388259257202.7861            0.33s\n",
      "        10 633317119214.8096            0.45s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 826991391032.8055            0.19s\n",
      "        10 368684571342.0211            0.35s\n",
      "         2 786836873892.3253            0.20s\n",
      "         3 755390884953.7933            0.21s\n",
      "         4 724023319187.8400            0.22s\n",
      "         5 697035141316.5665            0.21s\n",
      "         6 668149282582.3866            0.21s\n",
      "         7 638953988770.6350            0.21s\n",
      "         8 615568293828.8098            0.21s\n",
      "         9 596985693247.3705            0.21s\n",
      "        10 575397852201.6271            0.21s\n",
      "        20 469383021011.0750            0.36s\n",
      "        20 247493087209.4381            0.32s\n",
      "        20 432531163730.1564            0.22s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.01, loss=absolute_error, max_depth=None, max_features=None, max_leaf_nodes=10, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   0.4s\n",
      "        30 384957810474.3555            0.30s\n",
      "        30 174287935279.8594            0.27s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 476833483781.7957 -391290171545.8293            0.73s\n",
      "        30 361032514639.2456            0.21s\n",
      "         2 375480496156.6753 -15270390501.5298            0.70s\n",
      "         3 311214019626.0753 58294432393.0618            0.88s\n",
      "        40 138817248399.8173            0.22s\n",
      "        40 343092741720.5764            0.24s\n",
      "         4 249199277890.6665  4236550381.6771            0.78s\n",
      "         5 230756078208.7223 134182208736.0873            0.70s\n",
      "         6 188552968373.8905  2796483265.6206            0.66s\n",
      "         7 164127718694.1157 51073770082.7860            0.63s\n",
      "        40 324070937450.1295            0.20s\n",
      "         8 142036401516.7697 34414633799.0861            0.60s\n",
      "         9 115017094304.3935 -2793732478.4121            0.58s\n",
      "        50 316648238169.7618            0.19s\n",
      "        50 117356010840.8781            0.18s\n",
      "        10 99499009720.5217 23425466798.5177            0.58s\n",
      "        50 301359336451.6024            0.16s\n",
      "        60 299322354929.8156            0.15s\n",
      "        60 102476894840.6951            0.15s\n",
      "        60 287792639618.7886            0.13s\n",
      "        20 34810222664.6963  2929238379.6309            0.52s\n",
      "        70 289857206098.1481            0.11s\n",
      "        70 96182664358.1771            0.11s\n",
      "        70 279924996916.9142            0.10s\n",
      "        80 283072841808.3467            0.08s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.01, loss=absolute_error, max_depth=None, max_features=None, max_leaf_nodes=10, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False; total time=   0.4s\n",
      "        80 90579529260.9291            0.08s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "        30 17083140755.7845  2753136313.3164            0.47s\n",
      "        80 273306922753.0561            0.07s\n",
      "         1 437953821878.2963 -399005413164.5621            1.25s\n",
      "         2 355601293050.2055 38004695052.3090            0.87s\n",
      "         3 295435524319.2334 59142772537.0766            0.75s\n",
      "        90 277251555981.0004            0.04s\n",
      "        90 85760950592.1507            0.04s\n",
      "         4 242161907448.7719 12448742287.1821            1.11s\n",
      "         5 216279972094.3324 86437557728.1100            1.03s\n",
      "       100 272755474583.8918            0.00s\n",
      "        90 267332103467.1008            0.04s\n",
      "         6 175422944237.5174  6169272662.7604            1.08s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.05, loss=squared_error, max_depth=None, max_features=sqrt, max_leaf_nodes=20, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   0.4s\n",
      "         7 151548381461.4557 49443232084.2800            1.06s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 445938039091.0681 -446468953310.0160            0.57s\n",
      "       100 82959485298.2795            0.00s\n",
      "         8 133441780677.2122 30952221306.9133            1.04s\n",
      "         2 373418370108.4028 91315463337.5527            0.53s\n",
      "        40 15494556498.4745  1355527503.8729            0.44s\n",
      "         3 303068225838.9634 30286721008.0234            0.51s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.05, loss=squared_error, max_depth=None, max_features=sqrt, max_leaf_nodes=20, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   0.4s\n",
      "         9 107106818565.1713 -11228436792.3697            1.00s\n",
      "         4 259264232900.7416 67559626065.2470            0.51s\n",
      "         5 220067716624.3856 43607689257.6783            0.50s\n",
      "        10 92148558981.6498 12626809228.4175            0.98s\n",
      "         6 191684979532.9391 58090554371.8150            0.51s\n",
      "         7 151931807467.2664 -14739270145.3853            0.50s\n",
      "       100 262261974305.4422            0.00s\n",
      "         8 132514616556.1529 39784698889.0407            0.51s\n",
      "         9 114273160631.2059 14930306469.8464            0.53s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.05, loss=squared_error, max_depth=None, max_features=sqrt, max_leaf_nodes=20, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   0.4s\n",
      "        10 99564153061.1533 20866525676.8412            0.54s\n",
      "        50 13020702151.5653 -1466876169.8908            0.36s\n",
      "        20 32031949844.1610  7571360704.0826            0.84s\n",
      "        20 31741917382.3042  7085165844.9477            0.59s\n",
      "        60 11093775592.2764  -186402725.4973            0.29s\n",
      "        30 16893569305.4860  -601384490.8801            0.71s\n",
      "        30 19139087665.0755   222544200.8991            0.55s\n",
      "        70 10553857921.8680    39765085.7012            0.23s\n",
      "        40 13409575496.8141  2847575082.3465            0.53s\n",
      "        40 15138112792.6132  1611354974.1009            0.47s\n",
      "        50 10782221019.7294 -3481166543.6548            0.41s\n",
      "        80 10379005891.2066  1299363551.4965            0.16s\n",
      "        50 13359686350.5015   847382054.6365            0.39s\n",
      "        60  9670882945.7617   506848512.5850            0.32s\n",
      "        90  8166239037.0096 -4504346529.3107            0.08s\n",
      "        60 10354037103.0826 -5043681765.3877            0.31s\n",
      "        70  9062103421.7072   -64308215.0758            0.24s\n",
      "       100  8544206433.2168  -471227585.8970            0.00s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=friedman_mse, init=zero, learning_rate=0.1, loss=huber, max_depth=7, max_features=sqrt, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=True; total time=   0.8s\n",
      "        70 11615833906.2364  3610027367.7695            0.24s\n",
      "        80  8760495825.7857  1225896471.2708            0.17s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=squared_error, init=None, learning_rate=0.2, loss=huber, max_depth=7, max_features=sqrt, max_leaf_nodes=10, min_impurity_decrease=0.1, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.6s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=squared_error, init=None, learning_rate=0.2, loss=huber, max_depth=7, max_features=sqrt, max_leaf_nodes=10, min_impurity_decrease=0.1, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.6s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "        90  8189493602.4809   240054009.2096            0.08s\n",
      "         1 106543928359.3743 -112539209842.9000            1.16s\n",
      "         1 102273982796.5379 -110388912806.7636            1.18s\n",
      "         2 108543351437.7708  6552473958.4174            0.87s\n",
      "         2 106168531677.3747  9720602543.3265            1.00s\n",
      "         3 114502810432.2362 12663997150.6313            0.93s\n",
      "         4 106601217094.0006 -9533679994.7894            0.85s\n",
      "        80 10464090770.7046  6848899351.5251            0.17s\n",
      "         3 106741670266.7653  4195747760.4472            1.19s\n",
      "         5 103414310909.7953 -1260739846.9540            0.78s\n",
      "         6 104746588852.3962  5276158590.5310            0.74s\n",
      "         4 107553789440.3472  4941292306.6484            1.20s\n",
      "         7 96720084156.0326 -8238304669.3719            0.77s\n",
      "         8 99617269857.4027  8049784099.0361            0.73s\n",
      "         5 98044115819.6772 -10843108572.2495            1.19s\n",
      "         9 98281849754.0951  1513125979.0629            0.70s\n",
      "         6 95085953360.2884  -964996072.2337            1.16s\n",
      "        10 96268147563.0658   112594607.5527            0.71s\n",
      "         7 96723204395.6068  6269361318.7465            1.07s\n",
      "         8 98030762100.4789  5198047283.0571            0.99s\n",
      "         9 97630290105.0083  2987875225.2911            0.92s\n",
      "        10 90247872725.0918 -7976998787.7076            0.98s\n",
      "        90  9877012130.4799   331164272.9283            0.09s\n",
      "        20 85839784963.9978  1554616358.6057            0.70s\n",
      "        20 80317550902.5376  3140488858.4046            0.73s\n",
      "       100  9401889527.8765  1224869828.3237            0.00s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=friedman_mse, init=zero, learning_rate=0.1, loss=huber, max_depth=7, max_features=sqrt, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=True; total time=   0.9s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "       100  7441332265.0649  -214254298.1304            0.00s\n",
      "         1 104735218496.3016 -103827780567.0138            1.05s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=friedman_mse, init=zero, learning_rate=0.1, loss=huber, max_depth=7, max_features=sqrt, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=True; total time=   1.0s\n",
      "         2 99260475158.7861 -4822682686.0321            1.24s\n",
      "         3 106618969774.9509 14975950079.6033            1.07s\n",
      "         4 98289416164.5771 -9123235939.0762            1.09s\n",
      "        30 73677067786.2118  -575050896.5649            0.64s\n",
      "         5 100677866751.8538  6893541107.4172            0.98s\n",
      "         6 101598631795.2264  4586077122.4845            0.95s\n",
      "        30 69451291885.3190 -2014008425.6209            0.67s\n",
      "         7 91176822325.6687 -12090128185.6123            0.91s\n",
      "         8 98954069388.4173 15224035972.8077            0.84s\n",
      "         9 96633863999.7044  -212887726.1961            0.85s\n",
      "        10 90484770351.5285 -5710025475.9596            0.84s\n",
      "        40 64382789933.6495 10056989212.6914            0.54s\n",
      "        40 60052013875.9032  5998653920.4854            0.58s\n",
      "        20 82439906284.0967  -752420992.6794            0.71s\n",
      "        50 54784849871.3084  2648191546.8866            0.47s\n",
      "        50 57389329202.6555  2361814374.6617            0.49s\n",
      "        30 69454900396.7786 -1029995796.7230            0.65s\n",
      "        60 49052316932.5507  2169412129.0078            0.38s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=squared_error, init=None, learning_rate=0.2, loss=huber, max_depth=7, max_features=sqrt, max_leaf_nodes=10, min_impurity_decrease=0.1, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.7s\n",
      "        60 55922672878.2238  5605939974.0544            0.39s\n",
      "        40 60238973585.9214   613057487.1676            0.58s\n",
      "        70 43068019109.9646 -1011179804.1578            0.28s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=squared_error, init=zero, learning_rate=0.01, loss=quantile, max_depth=7, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   2.4s\n",
      "        70 46741315389.3818   -70087035.7508            0.29s\n",
      "        50 53118633358.4252 -7725703806.7866            0.48s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=squared_error, init=zero, learning_rate=0.01, loss=quantile, max_depth=7, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   2.4s\n",
      "        80 38711158919.1062 -3315519535.9464            0.20s\n",
      "        80 39174957455.2066 -7655543072.1057            0.21s\n",
      "        60 48205448333.8631  3942647792.0079            0.43s\n",
      "        90 36420046842.9618  1186762598.7445            0.10s\n",
      "        90 37738490374.0917 -1658979144.7440            0.11s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=squared_error, init=zero, learning_rate=0.01, loss=quantile, max_depth=7, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   2.7s\n",
      "        70 44785567572.4615  4575701022.2309            0.33s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.05, loss=squared_error, max_depth=3, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.8s\n",
      "       100 32627221287.4026  1173108556.5839            0.00s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=squared_error, init=None, learning_rate=0.01, loss=huber, max_depth=None, max_features=log2, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   1.1s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      676931.1339      109593.3670            0.07s\n",
      "         2      588425.8158       66678.4427            0.07s\n",
      "       100 35650405420.5242  4393712182.1784            0.00s\n",
      "         3      536500.0541      106287.0944            0.07s\n",
      "         4      480975.5255       -7935.4001            0.07s\n",
      "         5      459305.8922       48904.3980            0.07s\n",
      "        80 38422241777.6114 -1379583848.4226            0.23s\n",
      "         6      438613.3593       45362.9843            0.07s\n",
      "         7      406133.3358       -4445.0969            0.07s\n",
      "         8      401158.7419       10638.4675            0.07s\n",
      "         9      379679.5871       -6521.3834            0.07s\n",
      "        10      376550.7441       -2819.3978            0.07s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=squared_error, init=None, learning_rate=0.01, loss=huber, max_depth=None, max_features=log2, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   1.2s\n",
      "        20      306088.9788      -16629.3078            0.08s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      656455.9221      110686.9142            0.07s\n",
      "         2      580480.7509      124327.1269            0.14s\n",
      "         3      525019.7810       93105.0613            0.20s\n",
      "         4      476963.5199       19925.2257            0.18s\n",
      "         5      444829.6841        3359.5340            0.16s\n",
      "         6      424991.3990       50421.2492            0.14s\n",
      "         7      394589.3500        4142.5504            0.14s\n",
      "         8      389499.1038       13492.9820            0.15s\n",
      "         9      376415.2383       31366.8403            0.14s\n",
      "        30      287078.9004      -40582.4176            0.07s\n",
      "        10      357545.6119      -28417.0430            0.14s\n",
      "        90 34807164422.1746  1145801196.7120            0.12s\n",
      "        40      284794.5527       10145.9094            0.03s\n",
      "        20      285358.6053      -31249.3769            0.10s\n",
      "        30      288335.7834      -13009.1635            0.06s\n",
      "        50      283092.0533       19358.9288            0.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=squared_error, init=zero, learning_rate=0.2, loss=absolute_error, max_depth=3, max_features=log2, max_leaf_nodes=10, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.2s\n",
      "        40      251527.5748        3555.3319            0.03s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "       100 35289477122.1957  1428743053.1877            0.00s\n",
      "         1      662767.6640      110675.3785            0.19s\n",
      "         2      568476.1941       52251.4239            0.15s\n",
      "         3      524572.0990      133500.6200            0.12s\n",
      "         4      472481.0233        -920.7152            0.11s\n",
      "         5      436548.8121       58333.3329            0.13s\n",
      "         6      423564.3455       40025.6065            0.13s\n",
      "         7      400461.8351      -35261.4790            0.12s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=squared_error, init=None, learning_rate=0.01, loss=huber, max_depth=None, max_features=log2, max_leaf_nodes=30, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   1.2s\n",
      "         8      392088.4233       51276.0587            0.12s\n",
      "         9      376849.1176       32749.0068            0.12s\n",
      "        10      350561.1793      -16475.4571            0.12s\n",
      "        50      256300.6802       24490.1912            0.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=squared_error, init=zero, learning_rate=0.2, loss=absolute_error, max_depth=3, max_features=log2, max_leaf_nodes=10, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.2s\n",
      "        20      293498.0844       63810.2132            0.09s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.05, loss=squared_error, max_depth=3, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.8s\n",
      "        30      272399.3119        1371.7900            0.07s\n",
      "        40      279719.1760       11279.9392            0.03s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=squared_error, init=zero, learning_rate=0.2, loss=absolute_error, max_depth=3, max_features=log2, max_leaf_nodes=10, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.2s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.05, loss=squared_error, max_depth=3, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.8s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      409056.8956         827.6352            0.51s\n",
      "         2      405249.3165      -10149.5230            0.46s\n",
      "         3      413869.5017       34483.0911            0.41s\n",
      "         1      389202.4788         821.3771            0.76s\n",
      "         4      402648.3090      -44868.2008            0.42s\n",
      "         2      396171.6980       33037.9211            0.69s\n",
      "         3      402857.4552       26737.9585            0.56s\n",
      "         5      408566.6058       23669.2042            0.45s\n",
      "         4      397856.1837      -20002.2720            0.49s\n",
      "         5      393927.8140      -15709.4439            0.44s\n",
      "         6      410837.0290       11385.8299            0.48s\n",
      "         6      398649.8545       21101.7377            0.48s\n",
      "         7      403536.2084      -24111.5947            0.48s\n",
      "         8      404871.4541        5348.1863            0.45s\n",
      "         9      399143.2630      -18014.2933            0.43s\n",
      "        10      398163.5214       -3915.7442            0.41s\n",
      "         7      392326.7293      -20114.1401            0.59s\n",
      "         8      393908.8546        6327.6893            0.59s\n",
      "         9      396243.2773       14423.9340            0.61s\n",
      "        10      387063.8643      -34829.1400            0.60s\n",
      "        20      400174.3718      -19916.0745            0.46s\n",
      "        20      381040.6273      -32504.9652            0.57s\n",
      "        30      393820.5502      -57026.5935            0.47s\n",
      "        30      385172.5799       -9738.3869            0.55s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=squared_error, init=None, learning_rate=0.1, loss=squared_error, max_depth=None, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   0.8s\n",
      "        40      388216.1451        4543.0955            0.50s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.1, loss=absolute_error, max_depth=3, max_features=sqrt, max_leaf_nodes=20, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.2s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "        40      368850.0616        2227.8394            0.60s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      396709.7693         826.8349            1.62s\n",
      "         2      388461.1707      -32994.4398            1.06s\n",
      "        50      385766.2891       28921.1123            0.48s\n",
      "         3      404119.8008       65983.1461            0.97s\n",
      "         4      394263.8595      -37476.3902            0.91s\n",
      "         5      397038.4344       16142.2043            0.81s\n",
      "         6      401119.5847       18202.8284            0.74s\n",
      "         1 386694076949.2383 -414251479996.8145            1.80s\n",
      "         7      389986.7271      -41074.3807            0.86s\n",
      "        50      372529.6494       36930.5748            0.54s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.1, loss=absolute_error, max_depth=3, max_features=sqrt, max_leaf_nodes=20, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.3s\n",
      "         2 277050380438.9180 133268996450.4521            1.32s\n",
      "         8      397669.3353       33134.2350            0.84s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         9      400496.4401       16622.2038            0.84s\n",
      "        10      390018.8247      -36986.0535            0.79s\n",
      "         3 211833255596.1251 118794200268.4773            1.19s\n",
      "         1 359198583345.5557 -387766074502.7405            1.17s\n",
      "        60      383964.5000        5831.3514            0.47s\n",
      "         2 261719139164.1154 138440081560.6744            1.06s\n",
      "         4 146310261438.9249 25961985549.4632            1.29s\n",
      "         3 193185125906.5283 85033537353.4622            0.84s\n",
      "        60      373053.6098       15898.0576            0.49s\n",
      "        20      394874.8538       54569.2250            0.63s\n",
      "         4 148120979301.7287 44700742023.7728            0.79s\n",
      "         5 108415423180.9464 26493405657.6637            1.26s\n",
      "         5 105248387476.2181 13187138889.5599            0.78s\n",
      "         6 80250889203.6610 19689892334.5184            0.71s\n",
      "         6 87445012137.8740 29876328924.7931            1.21s\n",
      "         7 66161661206.1248 20037933235.3216            0.66s\n",
      "        70      365922.4013       13570.9425            0.44s\n",
      "         7 60190706547.3874   -18992777.6570            1.15s\n",
      "        70      381090.4675      -10068.1694            0.45s\n",
      "         8 57334568480.8512 15345251464.8658            0.74s\n",
      "        30      380146.5013        4829.1742            0.61s\n",
      "         8 54371816158.7671 20968639948.7474            1.17s\n",
      "         9 48098965510.0648  6746458212.2991            0.73s\n",
      "        80      355535.7138      -41025.3501            0.39s\n",
      "        10 40102678851.8746  1878396605.1554            0.69s\n",
      "        80      369277.9765      -47660.9620            0.40s         9 44315363409.4472  6573725856.2623            1.11s\n",
      "\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.1, loss=absolute_error, max_depth=3, max_features=sqrt, max_leaf_nodes=20, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.3s\n",
      "        10 44699300729.5222 12111141513.5898            1.07s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "        90      358141.1010       30156.0881            0.34s\n",
      "        40      380260.6085        4242.1385            0.57s\n",
      "         1 389867297547.2844 -376004878167.9760            0.87s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=squared_error, init=None, learning_rate=0.1, loss=squared_error, max_depth=None, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   0.8s\n",
      "        90      369091.4372         917.9181            0.36s\n",
      "         2 261368906691.0891 79988919967.9497            0.63s\n",
      "         3 214193255877.1913 117193794448.5205            0.56s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.01, loss=absolute_error, max_depth=7, max_features=log2, max_leaf_nodes=20, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.3s\n",
      "         4 151171358761.6692 31174432056.9830            0.52s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         5 115428860144.4185 43397744035.2888            0.55s\n",
      "         1      586097.5000       36000.0000            0.42s\n",
      "       100      351188.1601      -11815.9765            0.31s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "        50      370668.8787      -27369.6730            0.51s\n",
      "         2      556972.0192       43140.4831            0.61s\n",
      "         6 93606864907.8192 29684258369.0832            0.55s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=squared_error, init=None, learning_rate=0.1, loss=squared_error, max_depth=None, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   0.8s\n",
      "         1      575807.3077       36000.0000            0.67s\n",
      "         2      553012.8846       53650.6399            0.58s\n",
      "         3      524934.5252       41239.0898            0.52s\n",
      "         3      541939.2308       61749.7170            1.00s\n",
      "         4      496677.8377       36029.7529            0.53s\n",
      "         7 61744826704.6381 -6988625383.4571            0.59s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         5      448226.8455       -3070.2597            0.58s\n",
      "         8 59403623117.7906 28896541574.0018            0.56s\n",
      "         1      585207.7866       36000.0000            0.31s\n",
      "         4      494488.0907        6202.6615            1.05s\n",
      "         2      540378.4487       19641.1319            0.30s\n",
      "         6      420869.2027       23283.0809            0.60s\n",
      "         3      525407.1113       60441.3446            0.30s\n",
      "        20 23283280109.0602  -556431442.0878            0.59s\n",
      "         9 50421155610.8100  7955020027.7229            0.55s\n",
      "         4      477298.1106        3015.2778            0.37s\n",
      "        10 39701920939.7591 -1390908578.0303            0.52s\n",
      "         7      404910.8217       36901.0545            0.68s\n",
      "         5      463473.5831       49806.7675            0.47s\n",
      "        20 28213930761.0052   951040982.5207            0.75s\n",
      "        60      362079.5838        9088.5625            0.46s\n",
      "         6      440337.1051       32107.2170            0.52s\n",
      "         8      389128.6507       31163.2956            0.72s\n",
      "         7      394239.8488      -10666.6232            0.57s\n",
      "         9      372856.2476       24686.9405            0.75s\n",
      "         8      397494.7601       59186.2586            0.56s\n",
      "        10      337788.4650      -10535.4756            0.71s\n",
      "         5      459809.2547       17668.5029            1.75s\n",
      "         9      372782.1269       11362.7335            0.55s\n",
      "        10      345949.8344        3086.9921            0.54s\n",
      "        70      363434.7164       30283.4072            0.41s\n",
      "         6      444526.4647       43377.5814            2.13s\n",
      "         7      405253.6577        1341.4774            1.93s\n",
      "         8      392172.3612       33795.3639            1.80s\n",
      "         9      373180.3293       19040.0639            1.65s\n",
      "        20      258613.0339       -1281.3421            0.54s\n",
      "        10      350311.9242        7370.0740            1.57s\n",
      "        20      251976.2776        9266.5751            0.70s\n",
      "        30 24217508545.2274  2128939631.6131            0.57s\n",
      "        30 21342693245.7311 -1026810932.1870            0.50s\n",
      "        80      352423.0777      -33619.1335            0.39s\n",
      "        20 29169367118.8225   458997109.6308            0.51s\n",
      "        30      224441.8522       -8185.9780            0.61s\n",
      "        20      268793.5444        6723.2679            1.05s\n",
      "        30      224984.1435       -6190.2490            0.58s\n",
      "        90      353081.0887      -37342.8405            0.35s\n",
      "        40      215406.2103       15719.8863            0.52s\n",
      "        40 21736526202.2023  6329612523.1363            0.45s\n",
      "       100      358194.2658      -19135.4779            0.31s\n",
      "        40 17472557183.0284  2284668136.1181            0.42s\n",
      "        30 23874795283.8369  1505299625.9091            0.44s\n",
      "        50      217583.4165        8282.7116            0.48s\n",
      "        30      238839.1176       -4832.0746            0.94s\n",
      "        40      216411.2357       -1476.7347            0.60s\n",
      "        40      230198.7265       26190.9090            0.78s\n",
      "        60      217266.2463        2690.6756            0.45s\n",
      "        50      214713.1349      -22333.7061            0.54s\n",
      "        50 19195502367.8383 -1834163950.3949            0.36s\n",
      "        50 18120920232.5124   354129919.2397            0.34s\n",
      "        50      227383.7346        6872.8695            0.66s\n",
      "        40 17893028167.9537 -1324505564.1634            0.39s\n",
      "        60      215792.6660       10128.3368            0.50s\n",
      "        70      215244.9074       -7922.5327            0.43s\n",
      "        70      221413.7840       15182.4093            0.45s\n",
      "        60      242662.4745       14665.6955            0.62s\n",
      "        60 17266238469.4470   826966030.2180            0.26s\n",
      "       200      330097.3492       -7500.4971            0.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.01, loss=absolute_error, max_depth=7, max_features=log2, max_leaf_nodes=20, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.6s\n",
      "        80      213713.9722      -10461.7793            0.40s\n",
      "        80      214158.6208      -15808.2917            0.42s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.05, loss=quantile, max_depth=None, max_features=log2, max_leaf_nodes=20, min_impurity_decrease=0.1, min_samples_leaf=4, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.3s\n",
      "        60 21756896201.3688  7183722517.6749            0.31s\n",
      "        70      229998.2455       -5178.9782            0.58s\n",
      "        50 17832810316.7012 -5358297438.4312            0.35s\n",
      "        90      214128.4049        3204.8000            0.38s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.05, loss=quantile, max_depth=None, max_features=log2, max_leaf_nodes=20, min_impurity_decrease=0.1, min_samples_leaf=4, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.3s\n",
      "        70 15747761483.1421   360381191.3499            0.20s\n",
      "        80      217981.6265      -31926.0474            0.52s\n",
      "        60 17830830938.2928  3124180300.0749            0.27s\n",
      "        90      226690.6147       -9793.0753            0.46s\n",
      "        70 18819234268.2337  3003205742.1338            0.23s\n",
      "       100      231455.2710       15732.3345            0.41s\n",
      "        80 15418525675.4414 -2611423317.2173            0.13s\n",
      "        70 18263787925.1696  5025373947.4879            0.20s\n",
      "       200      328943.8084        4125.8683            0.00s\n",
      "        80 17188022408.0368 -4123688879.8611            0.15s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.01, criterion=squared_error, init=None, learning_rate=0.01, loss=absolute_error, max_depth=7, max_features=log2, max_leaf_nodes=20, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.6s\n",
      "        90 15433452609.4809  -777630097.4107            0.07s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.05, loss=quantile, max_depth=None, max_features=log2, max_leaf_nodes=20, min_impurity_decrease=0.1, min_samples_leaf=4, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=10, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.5s\n",
      "        90 17171966772.7326 -1819550085.6978            0.07s\n",
      "        80 17018564799.7017  1327102069.9346            0.14s\n",
      "       100 15533139233.5586  3021062916.0414            0.00s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=huber, max_depth=5, max_features=sqrt, max_leaf_nodes=20, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.7s\n",
      "       100 17144646016.1874  5067425695.6832            0.00s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=huber, max_depth=5, max_features=sqrt, max_leaf_nodes=20, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.7s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 530074821724.4203 213196600724.6720            0.20s\n",
      "         2 402871095139.9154 97000470330.7697            0.13s\n",
      "         3 632571389545.7982 591363100569.8137            0.11s\n",
      "         4 324785594887.3192 -285219975572.2073            0.12s\n",
      "         5 461279182302.1614 407859638968.6436            0.11s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.2, loss=absolute_error, max_depth=5, max_features=sqrt, max_leaf_nodes=None, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   0.3s\n",
      "         6 197660426643.3247 -327040933327.2366            0.10s\n",
      "         7 374103744888.1625 368045120080.8008            0.10s\n",
      "         8 365696854540.1401 25235058513.0941            0.09s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "        90 14182421950.7622  -618423714.2930            0.07s\n",
      "         9 307722033724.2550 -20424158865.0000            0.09s\n",
      "        10 294697184518.5571 18936994872.8949            0.08s\n",
      "         1 485366679467.5378 137759521437.4882            0.23s\n",
      "         2 394008122831.8826 115834599602.4390            0.16s\n",
      "         3 343972551061.7994 103209696058.6506            0.13s\n",
      "         4 272979515612.4467 35843862012.4292            0.12s\n",
      "         5 224730272562.4407 43655788090.8898            0.10s\n",
      "         6 153807999255.6784 -41073401946.1739            0.10s\n",
      "         7 161387539875.7387 68387162010.6223            0.09s\n",
      "         8 173914202776.1902 35430437448.3199            0.09s\n",
      "         9 132098825379.8003 -40582397792.8616            0.08s\n",
      "        10 132036165754.5223 29037868360.9782            0.08s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.2, loss=absolute_error, max_depth=5, max_features=sqrt, max_leaf_nodes=None, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   0.3s\n",
      "        20 120130181888.0427 16617397904.3266            0.07s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 800687121454.1713 95393026214.1942            0.08s\n",
      "         2 609076917495.2815 21132314546.5757            0.07s\n",
      "         3 539733707466.4739 96169363242.4152            0.08s\n",
      "         4 240605670110.1383 -315774729343.6875            0.08s\n",
      "         5 387561796269.8082 343021567635.2794            0.08s\n",
      "        20 103162219260.3976 21818624586.8801            0.07s\n",
      "         6 184932704834.6794 -245649811585.6227            0.10s\n",
      "         7 162126623057.9845 24310078861.9062            0.09s\n",
      "         8 331857597677.6288 306486487907.2916            0.09s\n",
      "         9 315846153798.3124  7417021120.9952            0.09s\n",
      "        30 91726733745.8564 -10902485094.2047            0.05s\n",
      "        10 274687808373.1126 -25391317725.3698            0.10s\n",
      "        30 69002817996.7215 -7082493241.7672            0.04s\n",
      "       100 17073003785.3881   301878486.7958            0.00s\n",
      "        40 87724468003.5475 -69433573693.8831            0.02s\n",
      "        40 67436666937.4958 -3337822433.9719            0.02s\n",
      "        20 204695920521.7380 162147591112.5253            0.07s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=huber, max_depth=5, max_features=sqrt, max_leaf_nodes=20, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=10, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.7s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.2, loss=absolute_error, max_depth=5, max_features=sqrt, max_leaf_nodes=None, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   0.4s\n",
      "        50 130252233130.3018  5977480751.7953            0.00s\n",
      "        30 91930051578.4185 -5333763771.4902            0.04s\n",
      "        50 60889802916.4954  6788803301.3360            0.00s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=friedman_mse, init=None, learning_rate=0.2, loss=squared_error, max_depth=3, max_features=log2, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.1s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=friedman_mse, init=None, learning_rate=0.2, loss=squared_error, max_depth=3, max_features=log2, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.1s\n",
      "        40 114106140907.0273 -9059422565.1805            0.02s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      777167.5860            0.13s\n",
      "         2      771673.0860            0.12s\n",
      "         3      766233.5310            0.11s\n",
      "         4      760848.3715            0.10s\n",
      "         5      755517.0637            0.09s\n",
      "         6      750239.0689            0.09s\n",
      "         7      745013.8541            0.12s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=squared_error, init=None, learning_rate=0.01, loss=absolute_error, max_depth=3, max_features=sqrt, max_leaf_nodes=20, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.3s\n",
      "         8      739840.8914            0.12s\n",
      "         9      734720.4365            0.14s\n",
      "        10      729654.1209            0.16s\n",
      "        50 104083186139.9991 -6996161517.4254            0.00s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=friedman_mse, init=None, learning_rate=0.2, loss=squared_error, max_depth=3, max_features=log2, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.1s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      763283.6957            0.11s\n",
      "         2      757739.6957            0.15s\n",
      "         3      752251.1357            0.15s\n",
      "         4      746817.4613            0.14s\n",
      "         5      741438.1236            0.13s\n",
      "         6      736112.5793            0.12s\n",
      "         7      730840.2905            0.10s\n",
      "         8      725620.7245            0.10s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      762942.3118            0.06s\n",
      "         2      757428.0118            0.05s\n",
      "         3      751968.8548            0.06s\n",
      "         9      720454.2447            0.11s\n",
      "         4      746564.2894            0.05s\n",
      "         5      741213.7696            0.06s\n",
      "         6      735916.7551            0.05s\n",
      "        10      715341.7169            0.13s\n",
      "         7      730672.7106            0.09s\n",
      "         8      725481.1066            0.09s\n",
      "         9      720341.4187            0.08s\n",
      "        10      715253.9735            0.07s\n",
      "        20      681931.3953            0.20s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=squared_error, init=None, learning_rate=0.01, loss=absolute_error, max_depth=3, max_features=sqrt, max_leaf_nodes=20, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.4s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 594969345963.7075 143821461589.0100            1.11s\n",
      "         2 492990456274.5582 66961073090.6165            0.78s\n",
      "         3 661641597686.7341 1110142476519.7500            0.69s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=squared_error, init=None, learning_rate=0.01, loss=absolute_error, max_depth=3, max_features=sqrt, max_leaf_nodes=20, min_impurity_decrease=0.1, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.4s\n",
      "         4 393664511461.7983 -713471139860.5076            0.72s\n",
      "         5 519347448119.4310 869177775398.2400            0.68s\n",
      "         6 461470278385.9479 38469481023.7110            0.68s\n",
      "         7 408716791154.0135 48592205184.1777            0.65s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         8 366638965495.9507   -88956379.7762            0.64s\n",
      "         9 318090763161.7928 -14249665811.9032            0.62s\n",
      "         1 533172475697.8091 91526598598.5732            1.20s\n",
      "        20      667281.7618            0.13s\n",
      "         2 482488702014.3082 162591173866.4770            1.05s\n",
      "        10 282476310801.1636 16237349507.6955            0.73s\n",
      "         3 420612262442.7305 54155765949.1402            1.04s\n",
      "         4 361770697889.9376 52661622015.5991            1.01s\n",
      "        20      667167.4847            0.17s\n",
      "         5 319419880191.3448 37513808210.6014            0.89s\n",
      "         6 264890922542.8849 -50201963515.3329            0.93s\n",
      "         7 247635709406.1203 83711279445.9122            0.95s\n",
      "         8 223037108141.1542 21368489294.4588            0.95s\n",
      "         9 177853906909.9726 -60158542228.4368            0.89s\n",
      "        10 163955428409.6001 46452274307.9904            0.84s\n",
      "        20 81585927262.6422 -143220730367.7296            0.71s\n",
      "        30      639519.4439            0.16s\n",
      "        20 84908390644.6116 31945647017.5457            0.72s\n",
      "        30 49182179210.2400 -108087785940.1619            0.67s\n",
      "        30 49611551290.8659 -5283009221.2912            0.63s\n",
      "        30      624612.2590            0.13s\n",
      "        40 51720121987.5723  -209776098.1973            0.60s\n",
      "        30      624301.1533            0.15s\n",
      "        40 39534224386.3733  2839726757.5615            0.61s\n",
      "        50 43502977546.3012  4199624679.4763            0.58s\n",
      "        50 33910031653.6266  8359191595.1019            0.56s\n",
      "        40      602250.5421            0.09s\n",
      "        60 34882734475.5842    39883305.3509            0.55s\n",
      "        60 28228757211.0282  -754950594.0716            0.53s\n",
      "        40      587078.8446            0.08s\n",
      "        70 26399979077.6787  4299506170.4305            0.47s\n",
      "        70 29282393877.2079   962283003.4122            0.53s\n",
      "        40      586726.0098            0.09s\n",
      "        80 24507325590.7012   415264187.8158            0.44s\n",
      "        80 25990861228.3481   213733495.5711            0.49s\n",
      "        50      569660.2324            0.00s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.01, loss=absolute_error, max_depth=7, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.1, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   0.5s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 757358645614.3004 61775591771.2158            1.06s\n",
      "        90 21805488977.5334 -5031512425.0265            0.44s\n",
      "         2 629675340957.8016 -103145640086.0544            0.79s\n",
      "         3 561343841687.4989 166242305767.2355            0.68s\n",
      "         4 467183406983.7283 -44931161793.9119            0.61s\n",
      "        90 21475860640.9600   435280683.4240            0.42s\n",
      "         5 403899181360.3882 61603124165.1306            0.62s\n",
      "         6 259385317006.0854 -420478943723.0316            0.59s\n",
      "         7 333382759207.6915 524683292570.0641            0.67s\n",
      "         8 298415450097.1021 51238109857.6187            0.72s\n",
      "         9 260516730517.5002 -24941693358.4250            0.71s\n",
      "        50      554463.5818            0.00s\n",
      "        10 228895668898.3162  2467763708.7245            0.77s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.01, loss=absolute_error, max_depth=7, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.1, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   0.4s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "       100 20614489243.5480 -4735908928.5683            0.40s\n",
      "       100 19626363796.8615 -4596771711.2193            0.38s\n",
      "         1 1004855169538.5652 96971896797.6143            0.50s\n",
      "         2 791332625324.4564 16821868228.6171            0.42s\n",
      "        20 106980127549.5542 190340252308.2621            0.64s\n",
      "         3 654369050003.3568 65043879971.6559            0.37s\n",
      "         4 363474544158.6635 -267945255011.0456            0.35s\n",
      "        50      554007.6370            0.00s\n",
      "         5 322744760070.4761 98378110337.5653            0.36s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.1, criterion=friedman_mse, init=zero, learning_rate=0.01, loss=absolute_error, max_depth=7, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.1, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   0.5s\n",
      "         6 388963426891.0253 257780312054.4423            0.33s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         7 245828899905.9068 -108360774285.7625            0.32s\n",
      "         1 627866045654.0706 80627030707.1707            1.58s\n",
      "        30 64118859829.5632 83203024629.2577            0.76s\n",
      "         8 362201661431.1104 295676238624.4805            0.45s\n",
      "         2 477661422031.2921    65007065.0900            1.11s\n",
      "         9 263132534910.7364 -56687373183.9158            0.45s\n",
      "         3 409930596065.3506 77082525452.3097            0.81s\n",
      "        10 245744044395.0176 57029657095.5295            0.42s\n",
      "         4 298611822394.4186 -26780681888.7200            0.67s\n",
      "         5 294641982574.5488 116291665524.1844            0.60s\n",
      "         6 218317649576.1236 -20869378923.4285            0.56s\n",
      "         7 215490758694.7320 77708562301.9612            0.51s\n",
      "        40 44732378368.2752 -8801825220.9050            0.73s\n",
      "         8 210498290769.1842 64093431265.2278            0.49s\n",
      "         9 150689309763.5473 -34757553817.2682            0.45s\n",
      "        10 137491445344.2595 27989980888.6568            0.41s\n",
      "        20 69675099841.8361 -28666966503.4755            0.25s\n",
      "        50 34346331239.4646 -11218322864.7010            0.69s\n",
      "        20 56402897513.4821 -5819698174.0275            0.26s\n",
      "        60 30364380492.0622  8089931916.1045            0.66s\n",
      "        30 38313281229.0845 -3015552933.2984            0.16s\n",
      "        70 25706409121.5596  7781456935.0931            0.61s\n",
      "        30 35472637132.1719  1770446427.5813            0.17s\n",
      "        40 39041508419.8356  5077932565.8420            0.08s\n",
      "        80 24423922033.4495  6385310331.7128            0.55s\n",
      "        90 22593317632.2265  5378084694.8571            0.49s\n",
      "       200 13047816217.4074  1677158481.9091            0.00s\n",
      "       200 13605767168.5680   694386551.8831            0.00s\n",
      "        40 30630923856.2774  1913308742.1907            0.08s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=squared_error, init=None, learning_rate=0.1, loss=squared_error, max_depth=None, max_features=log2, max_leaf_nodes=20, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   0.8s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=squared_error, init=None, learning_rate=0.1, loss=squared_error, max_depth=None, max_features=log2, max_leaf_nodes=20, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   0.8s\n",
      "        50 31678833832.7330 -2896588130.4452            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.1, loss=squared_error, max_depth=5, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   0.4s\n",
      "         1 844967022196.2761 100802272944.7888            0.59s\n",
      "       100 20145738418.9758  2357216468.0304            0.45s\n",
      "         2 464445331790.3424 -326636826371.8297            0.44s\n",
      "         3 404631428092.5038 110204425062.0741            0.36s\n",
      "         4 535187551553.2657 425899470866.3878            0.40s\n",
      "         5 451040390704.2291 54547926594.0424            0.36s\n",
      "         6 406586174446.5919 78558029592.5394            0.33s\n",
      "         7 313385240342.6102 -20949774489.6090            0.31s\n",
      "        50 26093210160.2376 -4363177143.2104            0.00s\n",
      "         8 283990773369.7578 54828703247.1381            0.32s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.1, loss=squared_error, max_depth=5, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   0.4s\n",
      "         9 238117954530.1139 10550588353.0985            0.30s\n",
      "        10 199839207011.1758  8362855761.8777            0.30s\n",
      "        20 87038518982.3629 50999666970.4601            0.21s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=squared_error, init=zero, learning_rate=0.05, loss=huber, max_depth=3, max_features=sqrt, max_leaf_nodes=None, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   1.1s\n",
      "        30 44539222653.0232  8190680148.8107            0.14s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=squared_error, init=zero, learning_rate=0.05, loss=huber, max_depth=3, max_features=sqrt, max_leaf_nodes=None, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   1.2s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=squared_error, init=zero, learning_rate=0.05, loss=huber, max_depth=3, max_features=sqrt, max_leaf_nodes=None, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   1.2s\n",
      "        40 36873246995.9387  4204845934.6502            0.07s\n",
      "       200 12950654088.3221  1368644659.7865            0.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=squared_error, init=None, learning_rate=0.1, loss=squared_error, max_depth=None, max_features=log2, max_leaf_nodes=20, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   0.8s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 1688066392506.7659 112591487541.0880            0.40s\n",
      "        50 32621059806.8469   586705005.8231            0.00s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.1, loss=squared_error, max_depth=5, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.2, verbose=1, warm_start=False; total time=   0.4s\n",
      "         2 1501447431975.0237 21374222031.8954            0.50s\n",
      "         3 1377093381346.2126 86418862874.8392            0.47s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         4 905420418240.5173 -473063054759.5717            0.41s\n",
      "         1 1240706748970.7832 97697841345.9050            0.37s\n",
      "         2 1064430207145.5850 -5957777476.9409            0.29s\n",
      "         5 849080138640.0902 125825209346.9705            0.42s\n",
      "         3 979748928360.1322 104349719444.9142            0.27s\n",
      "         6 1026594575360.2279 466457526604.3202            0.41s\n",
      "         4 812042834293.6333 -43113993400.7380            0.29s\n",
      "         5 805467894803.4092 179208059213.8268            0.27s\n",
      "         7 712152915167.4406 -295900345966.1420            0.40s\n",
      "         6 673651568853.3870 -30953417970.2661            0.25s\n",
      "         8 1020710851137.5205 642883583069.1523            0.38s\n",
      "         7 670697990286.1416 149698161988.8071            0.28s\n",
      "         9 843645735354.2749 -118980043094.7864            0.37s\n",
      "        10 833404473547.6387 129516040048.8347            0.35s\n",
      "         8 650783325079.0278 113099751052.5182            0.28s\n",
      "         9 511139364433.1625 -84314730721.1540            0.26s\n",
      "        10 488620717605.6033 84311123618.0483            0.25s\n",
      "        20 234270613974.5156 -351543905496.1735            0.25s\n",
      "        20 227810112958.6504 -6770140294.3849            0.21s\n",
      "        30 100375066307.1720 -10149965867.0950            0.16s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.01, loss=squared_error, max_depth=None, max_features=sqrt, max_leaf_nodes=20, min_impurity_decrease=0.01, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.4s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "        30 101550947011.7068 -18352072152.6025            0.15s\n",
      "         1 1496456028057.0710 116367450101.4751            0.37s\n",
      "         2 1030551724832.0087 -436782075950.9929            0.45s\n",
      "         3 968780761327.1235 147191761472.1885            0.37s\n",
      "         4 1171596798064.3621 538428929317.9288            0.35s\n",
      "         5 1091679176211.2118 90087413204.0197            0.44s\n",
      "         6 1077407515467.4963 164734265738.7617            0.40s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.01, loss=squared_error, max_depth=None, max_features=sqrt, max_leaf_nodes=20, min_impurity_decrease=0.01, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.4s\n",
      "        40 60755276979.7622  8780826518.9472            0.07s\n",
      "        40 231898777622.2535 -10594203871.8684            0.08s\n",
      "         7 936943428719.8345 -42438751952.1160            0.41s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 376246838308.4315 -416684497840.9915            0.21s\n",
      "         8 893664883555.3895 96024498230.7571            0.40s\n",
      "         2 257822579441.3296 102483689803.8040            0.22s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.01, criterion=friedman_mse, init=None, learning_rate=0.01, loss=squared_error, max_depth=None, max_features=sqrt, max_leaf_nodes=20, min_impurity_decrease=0.01, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=100, n_iter_no_change=5, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=True; total time=   0.4s\n",
      "         9 822667463224.2206 36596396711.0250            0.39s\n",
      "         3 207162254908.5667 112319596679.5767            0.20s\n",
      "        10 754667450660.7982 22770453510.2692            0.36s\n",
      "         4 148012712370.5649 40987511103.6816            0.25s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         5 126663552204.7469 53803013511.0747            0.27s\n",
      "         1 366690643791.6781 -351629341099.3942            0.42s\n",
      "         6 82659244662.3356 -6401656264.9449            0.26s\n",
      "         2 259404348447.4315 95279227282.0742            0.41s\n",
      "         3 192692343639.6964 84451009938.5436            0.34s\n",
      "         7 79027812957.3527 37140969077.6733            0.28s\n",
      "         4 134783258408.1833 37024200359.0476            0.34s\n",
      "         8 72124328350.6567 15923042051.6287            0.30s\n",
      "        50 41562891426.4586 -12199847127.2911            0.00s\n",
      "         5 104873712426.3119 37924563985.7790            0.34s\n",
      "         9 61205120315.4880 -1303775495.9024            0.28s\n",
      "         6 74493275678.3131   225319321.6687            0.31s\n",
      "        10 53643045261.0242  3209028852.6417            0.26s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.0, criterion=squared_error, init=zero, learning_rate=0.05, loss=squared_error, max_depth=7, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.2, verbose=1, warm_start=True; total time=   0.4s\n",
      "         7 70313262200.6142 22897443315.1641            0.32s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 400285130641.6583 -319829678799.0233            0.37s\n",
      "         8 64620780403.3681 13273076852.9557            0.33s\n",
      "        50 203869979094.0920 208831182642.4269            0.00s\n",
      "         2 271885377065.6562 73884637159.9655            0.29s\n",
      "         9 52226027170.5159  1397943079.2935            0.30s\n",
      "        20 466970562698.7294 431803748824.8439            0.24s\n",
      "         3 198853049745.1125 76776677178.7940            0.25s\n",
      "        10 44018156960.3655  2546203566.9616            0.28s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.0, criterion=squared_error, init=zero, learning_rate=0.05, loss=squared_error, max_depth=7, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.2, verbose=1, warm_start=True; total time=   0.4s\n",
      "         4 132354157184.8835 23028770614.2458            0.27s\n",
      "         5 104232047897.5614 38353165078.6772            0.27s\n",
      "         6 76218552226.6053 13333873916.2772            0.29s\n",
      "        20 28954278117.9950  1652507602.9523            0.18s\n",
      "         7 63642308558.5795 17623792053.2462            0.30s\n",
      "         8 58686542319.5194 23851343265.6949            0.29s\n",
      "         9 52534145084.9902  9053872625.5865            0.27s\n",
      "        20 30076113828.4638  5185546325.6274            0.17s\n",
      "        10 43524316408.1470  2385441575.0052            0.25s\n",
      "        30 310222522038.6635 318098839770.6495            0.16s\n",
      "        30 21434501369.0566  -615070213.2063            0.12s\n",
      "        20 28195161845.2610  6219908541.4268            0.17s\n",
      "        30 21319803991.6110    -6576886.9870            0.12s\n",
      "        40 19551407312.2633 -6097895971.0872            0.06s\n",
      "        40 258808903003.3398 11514440705.0135            0.08s\n",
      "        30 22038706878.8443  1914095719.5229            0.11s\n",
      "        40 19688320695.3825 -3471394253.3287            0.06s\n",
      "        50 21524612555.7046   665011614.8882            0.00s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=huber, max_depth=7, max_features=sqrt, max_leaf_nodes=10, min_impurity_decrease=0.1, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.3s\n",
      "        40 20624956024.0445  -799142605.0941            0.06s\n",
      "        50 19676445778.6828  3292167739.3436            0.00s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=huber, max_depth=7, max_features=sqrt, max_leaf_nodes=10, min_impurity_decrease=0.1, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.3s\n",
      "        50 235478269859.7420 27878931903.9058            0.00s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.0, criterion=squared_error, init=zero, learning_rate=0.05, loss=squared_error, max_depth=7, max_features=None, max_leaf_nodes=20, min_impurity_decrease=0.01, min_samples_leaf=4, min_samples_split=5, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=0.6, tol=0.0001, validation_fraction=0.2, verbose=1, warm_start=True; total time=   0.4s\n",
      "        50 20452874200.6952 -2244758427.9978            0.00s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.2, loss=huber, max_depth=7, max_features=sqrt, max_leaf_nodes=10, min_impurity_decrease=0.1, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=False; total time=   0.3s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=squared_error, init=zero, learning_rate=0.2, loss=huber, max_depth=5, max_features=sqrt, max_leaf_nodes=10, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   1.2s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=squared_error, init=zero, learning_rate=0.2, loss=huber, max_depth=5, max_features=sqrt, max_leaf_nodes=10, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   1.3s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=squared_error, init=zero, learning_rate=0.2, loss=huber, max_depth=5, max_features=sqrt, max_leaf_nodes=10, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=True; total time=   1.2s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.1, loss=absolute_error, max_depth=7, max_features=sqrt, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.6s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.1, loss=absolute_error, max_depth=7, max_features=sqrt, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.7s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 936392910800.9025            0.73s\n",
      "         2 921580282274.6349            0.55s\n",
      "         3 906789487323.7928            0.46s\n",
      "         4 892735822002.8024            0.48s\n",
      "         5 878503450282.7542            0.43s\n",
      "         6 864989297725.8353            0.40s\n",
      "         7 851293218833.2164            0.41s\n",
      "         8 838297445411.9077            0.41s\n",
      "         9 825513261891.7177            0.39s\n",
      "        10 812574334967.4701            0.39s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.1, loss=absolute_error, max_depth=7, max_features=sqrt, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=None, random_state=42, subsample=0.6, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.7s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 652868968097.6893            0.34s\n",
      "         2 642647146120.7041            0.47s\n",
      "         3 632628738401.0607            0.50s\n",
      "         4 622764656525.3656            0.44s\n",
      "        20 698151839052.1046            0.30s\n",
      "         5 613140062838.5795            0.42s\n",
      "         6 603651956876.2853            0.44s\n",
      "         7 594395078059.8328            0.41s\n",
      "         8 585343594790.0251            0.41s\n",
      "         9 576434103603.5066            0.40s\n",
      "        10 567723883827.3949            0.42s\n",
      "        30 602817251106.4417            0.20s\n",
      "        20 488432917389.1301            0.30s\n",
      "        40 524435155501.0724            0.10s\n",
      "        30 421714454261.0251            0.20s\n",
      "        50 459450863673.4755            0.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=friedman_mse, init=None, learning_rate=0.01, loss=squared_error, max_depth=None, max_features=None, max_leaf_nodes=10, min_impurity_decrease=0.1, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   0.5s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 822295940950.1008            0.34s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=squared_error, init=zero, learning_rate=0.2, loss=huber, max_depth=7, max_features=sqrt, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.9s\n",
      "         2 809555774493.0642            0.39s\n",
      "         3 797066802958.6665            0.55s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=squared_error, init=zero, learning_rate=0.2, loss=huber, max_depth=7, max_features=sqrt, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   1.0s\n",
      "         4 784824354445.5758            0.58s\n",
      "        40 366291963288.1738            0.11s\n",
      "         5 772822250809.6365            0.54s\n",
      "         6 761066382437.0126            0.56s\n",
      "         7 749542537994.9277            0.52s\n",
      "         8 738233907830.2703            0.50s\n",
      "         9 727157404387.4180            0.48s\n",
      "        10 716180191056.3197            0.44s\n",
      "        50 319646599020.2062            0.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=friedman_mse, init=None, learning_rate=0.01, loss=squared_error, max_depth=None, max_features=None, max_leaf_nodes=10, min_impurity_decrease=0.1, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   0.5s\n",
      "        20 617198068156.4205            0.29s\n",
      "        30 534549620869.9149            0.20s\n",
      "        40 465387144036.9567            0.10s\n",
      "        50 406960471544.2777            0.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.1, criterion=friedman_mse, init=None, learning_rate=0.01, loss=squared_error, max_depth=None, max_features=None, max_leaf_nodes=10, min_impurity_decrease=0.1, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   0.5s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 1545048972738.1553            0.09s\n",
      "         2 1523018781867.8938            0.15s\n",
      "         3 1501394207641.9651            0.15s\n",
      "         4 1480194533946.1162            0.17s\n",
      "         5 1462800454744.9507            0.17s\n",
      "         6 1442475301901.3396            0.16s\n",
      "         7 1423213634214.6279            0.17s\n",
      "         8 1404606990449.9883            0.16s\n",
      "         9 1386268521001.4998            0.15s\n",
      "        10 1367285377140.9541            0.15s\n",
      "        20 1201922490618.4885            0.10s\n",
      "        30 1060149361288.2832            0.06s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.0, criterion=squared_error, init=zero, learning_rate=0.1, loss=squared_error, max_depth=None, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.6s\n",
      "        40 946618714986.6826            0.03s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 1242728168857.6709            0.15s\n",
      "         2 1224199546804.1543            0.18s\n",
      "         3 1204400769068.0417            0.16s\n",
      "        50 845176632090.6852            0.00s\n",
      "         4 1185363071061.2795            0.17s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.01, loss=squared_error, max_depth=None, max_features=sqrt, max_leaf_nodes=10, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   0.2s\n",
      "         5 1168339531599.4224            0.17s\n",
      "         6 1150343694554.6350            0.17s\n",
      "         7 1133081289019.7839            0.16s\n",
      "         8 1116230508031.4170            0.15s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         9 1099230243155.6232            0.14s\n",
      "         1 1415928802893.8562            0.10s\n",
      "        10 1082129710334.9792            0.13s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.0, criterion=squared_error, init=zero, learning_rate=0.1, loss=squared_error, max_depth=None, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.7s\n",
      "         2 1395848850424.5642            0.12s\n",
      "         3 1377016193298.3706            0.11s\n",
      "         4 1358196704181.8804            0.15s\n",
      "         5 1340829743692.8174            0.15s\n",
      "         6 1322079407199.7742            0.14s\n",
      "         7 1304615095443.1345            0.15s\n",
      "         8 1287003663328.1775            0.15s\n",
      "        20 931629542020.1329            0.09s\n",
      "         9 1269845853471.2698            0.14s\n",
      "        10 1254054811633.2605            0.14s\n",
      "        30 804404042985.3248            0.06s\n",
      "        20 1097814168164.1592            0.10s\n",
      "[CV] END alpha=0.8, ccp_alpha=0.0, criterion=squared_error, init=zero, learning_rate=0.1, loss=squared_error, max_depth=None, max_features=None, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   0.6s\n",
      "        40 705052460569.7748            0.03s\n",
      "        30 969350513679.4694            0.06s\n",
      "        50 614445382184.8578            0.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.01, loss=squared_error, max_depth=None, max_features=sqrt, max_leaf_nodes=10, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   0.2s\n",
      "        40 867660175693.3026            0.03s\n",
      "        50 773806157845.3356            0.00s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=zero, learning_rate=0.01, loss=squared_error, max_depth=None, max_features=sqrt, max_leaf_nodes=10, min_impurity_decrease=0.01, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.01, n_estimators=50, n_iter_no_change=10, random_state=42, subsample=1.0, tol=0.001, validation_fraction=0.1, verbose=1, warm_start=True; total time=   0.2s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.05, loss=huber, max_depth=None, max_features=log2, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   2.3s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.05, loss=huber, max_depth=None, max_features=log2, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   2.1s\n",
      "[CV] END alpha=0.9, ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.05, loss=huber, max_depth=None, max_features=log2, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=None, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   2.2s\n",
      "[CV] END alpha=0.95, ccp_alpha=0.0, criterion=squared_error, init=zero, learning_rate=0.2, loss=huber, max_depth=7, max_features=sqrt, max_leaf_nodes=30, min_impurity_decrease=0.0, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.01, n_estimators=200, n_iter_no_change=5, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=False; total time=   1.6s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 698684452861.8657 79798065227.2780            0.98s\n",
      "         2 590829606903.9656 25981691849.2924            0.86s\n",
      "         3 533577347998.1824 135018172465.1630            0.76s\n",
      "         4 456987770305.0688 30497036957.0623            0.70s\n",
      "         5 416940966080.1575  3446352914.3047            0.68s\n",
      "         6 365469945813.8248 37541365360.1400            0.67s\n",
      "         7 242516804753.2683 -297568472000.4445            0.65s\n",
      "         8 217358968169.0939 48389190230.4686            0.62s\n",
      "         9 274108089756.5590 379125289933.4692            0.60s\n",
      "        10 165981823502.8315 -319129656005.8152            0.59s\n",
      "        20 74430728046.2376 -5831070530.8143            0.55s\n",
      "        30 70133958624.9165 91001860721.6591            0.51s\n",
      "        40 47976114794.6492 40265589434.0415            0.50s\n",
      "        50 37775720732.5433 -1765835887.9003            0.51s\n",
      "        60 32602193544.1265  4588376928.3451            0.49s\n",
      "        70 30868228757.2028    22334331.4024            0.45s\n",
      "        80 27157076994.7494  2839010475.1517            0.42s\n",
      "        90 25160457461.4255  3603389936.5447            0.38s\n",
      "       100 23036281446.3741  1380205003.4139            0.35s\n",
      "       200 14152960710.9495 -4039049750.6754            0.00s\n",
      "---------------- Best Params for GB -------------------\n",
      "{'warm_start': True, 'verbose': 1, 'validation_fraction': 0.1, 'tol': 0.001, 'subsample': 0.8, 'random_state': 42, 'n_iter_no_change': None, 'n_estimators': 200, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 2, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 20, 'max_features': 'log2', 'max_depth': None, 'loss': 'squared_error', 'learning_rate': 0.1, 'init': None, 'criterion': 'squared_error', 'ccp_alpha': 0.0, 'alpha': 0.9}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "model_param = {}\n",
    "for name, model, params in randomcv_models:\n",
    "    random = RandomizedSearchCV(estimator=model,\n",
    "                                   param_distributions=params,\n",
    "                                   n_iter=100,\n",
    "                                   cv=3,\n",
    "                                   verbose=2,\n",
    "                                   n_jobs=-1)\n",
    "    random.fit(X_train, y_train)\n",
    "    model_param[name] = random.best_params_\n",
    "\n",
    "for model_name in model_param:\n",
    "    print(f\"---------------- Best Params for {model_name} -------------------\")\n",
    "    print(model_param[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2aa0c74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor\n",
      "RandomForestRegressor(max_features=5, n_estimators=1000, n_jobs=-1)\n",
      "Model Performance for Training Set:\n",
      "MSE: 14897149332.59353\n",
      "MAE: 38280.827116169625\n",
      "RMSE: 122053.8788101121\n",
      "R2 Score: 0.9810063428993965\n",
      "------------------------------------------------------------------------------------------\n",
      "Model Performance for Testing Set:\n",
      "MSE: 92439945866.3378\n",
      "MAE: 102350.53960627284\n",
      "RMSE: 304039.3820976779\n",
      "R2 Score: 0.8905586718143809\n",
      "===================================\n",
      "\n",
      "\n",
      "K-Neighbors Regressor\n",
      "KNeighborsRegressor(n_jobs=-1, n_neighbors=2)\n",
      "Model Performance for Training Set:\n",
      "MSE: 38355302340.30271\n",
      "MAE: 63689.90634192767\n",
      "RMSE: 195845.09782045276\n",
      "R2 Score: 0.9510975258167156\n",
      "------------------------------------------------------------------------------------------\n",
      "Model Performance for Testing Set:\n",
      "MSE: 113826258370.10123\n",
      "MAE: 115921.87905528159\n",
      "RMSE: 337381.4730688412\n",
      "R2 Score: 0.8652390286290759\n",
      "===================================\n",
      "\n",
      "\n",
      "Adaboost Regressor\n",
      "AdaBoostRegressor()\n",
      "Model Performance for Training Set:\n",
      "MSE: 163422258369.25748\n",
      "MAE: 282126.94119249674\n",
      "RMSE: 404255.1896627395\n",
      "R2 Score: 0.7916389056206414\n",
      "------------------------------------------------------------------------------------------\n",
      "Model Performance for Testing Set:\n",
      "MSE: 239762607288.39954\n",
      "MAE: 292885.92357669794\n",
      "RMSE: 489655.6006913426\n",
      "R2 Score: 0.716140701457888\n",
      "===================================\n",
      "\n",
      "\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 698684452861.8657 79798065227.2780            1.49s\n",
      "         2 590829606903.9656 25981691849.2924            1.09s\n",
      "         3 533577347998.1824 135018172465.1630            0.90s\n",
      "         4 456987770305.0688 30497036957.0623            0.80s\n",
      "         5 416940966080.1575  3446352914.3047            0.76s\n",
      "         6 365469945813.8248 37541365360.1400            0.73s\n",
      "         7 242516804753.2683 -297568472000.4445            0.70s\n",
      "         8 217358968169.0939 48389190230.4686            0.66s\n",
      "         9 274108089756.5590 379125289933.4692            0.64s\n",
      "        10 165981823502.8315 -319129656005.8152            0.62s\n",
      "        20 74430728046.2376 -5831070530.8143            0.56s\n",
      "        30 70133958624.9165 91001860721.6591            0.51s\n",
      "        40 47976114794.6492 40265589434.0415            0.49s\n",
      "        50 37775720732.5433 -1765835887.9003            0.48s\n",
      "        60 32602193544.1265  4588376928.3451            0.46s\n",
      "        70 30868228757.2028    22334331.4024            0.43s\n",
      "        80 27157076994.7494  2839010475.1517            0.41s\n",
      "        90 25160457461.4255  3603389936.5447            0.37s\n",
      "       100 23036281446.3741  1380205003.4139            0.34s\n",
      "       200 14152960710.9495 -4039049750.6754            0.00s\n",
      "GradientBoostingRegressor\n",
      "GradientBoostingRegressor(criterion='squared_error', max_depth=None,\n",
      "                          max_features='log2', max_leaf_nodes=20,\n",
      "                          min_samples_leaf=2, n_estimators=200, random_state=42,\n",
      "                          subsample=0.8, tol=0.001, verbose=1, warm_start=True)\n",
      "Model Performance for Training Set:\n",
      "MSE: 14834118410.202372\n",
      "MAE: 80849.478012919\n",
      "RMSE: 121795.395685561\n",
      "R2 Score: 0.9810867064441192\n",
      "------------------------------------------------------------------------------------------\n",
      "Model Performance for Testing Set:\n",
      "MSE: 91705500266.70143\n",
      "MAE: 102848.12216862135\n",
      "RMSE: 302829.1601987851\n",
      "R2 Score: 0.8914281952779765\n",
      "===================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retraining the models with best parameters\n",
    "models = {\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(n_estimators=1000, min_samples_split=2, max_features=5, max_depth=None, \n",
    "                                                     n_jobs=-1),\n",
    "     \"K-Neighbors Regressor\": KNeighborsRegressor(n_neighbors=2, n_jobs=-1),\n",
    "     \"Adaboost Regressor\":AdaBoostRegressor(n_estimators=50, loss=\"linear\"),\n",
    "     \"GradientBoostingRegressor\":GradientBoostingRegressor( warm_start= True, verbose= 1, validation_fraction= 0.1, tol= 0.001, subsample= 0.8, random_state= 42, n_iter_no_change= None, n_estimators= 200, min_weight_fraction_leaf= 0.0, min_samples_split= 2, min_samples_leaf= 2, min_impurity_decrease= 0.0, max_leaf_nodes= 20, max_features= 'log2', max_depth= None, loss= 'squared_error', learning_rate= 0.1, init= None, criterion= 'squared_error', ccp_alpha= 0.0, alpha= 0.9)\n",
    "    \n",
    "}\n",
    "for i in range(len(list(models))):\n",
    "    model = list(models.values())[i]\n",
    "    model.fit(X_train, y_train) # Train model\n",
    "\n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    model_train_mae, model_train_mse, model_train_rmse, model_train_score = evaluate_model(true=y_train, predicted=y_train_pred)\n",
    "    model_test_mae, model_test_mse, model_test_rmse, model_test_score = evaluate_model(true=y_test, predicted=y_test_pred)\n",
    "    \n",
    "    print(list(models.keys())[i])\n",
    "    print(list(models.values())[i])\n",
    "    print(\"Model Performance for Training Set:\")\n",
    "    print(f\"MSE: {model_train_mse}\")\n",
    "    print(f\"MAE: {model_train_mae}\")\n",
    "    print(f\"RMSE: {model_train_rmse}\")\n",
    "    print(f\"R2 Score: {model_train_score}\")\n",
    "    print('------------------------------------------------------------------------------------------')\n",
    "    print(\"Model Performance for Testing Set:\")\n",
    "    print(f\"MSE: {model_test_mse}\")\n",
    "    print(f\"MAE: {model_test_mae}\")\n",
    "    print(f\"RMSE: {model_test_rmse}\")\n",
    "    print(f\"R2 Score: {model_test_score}\")\n",
    "    \n",
    "    \n",
    "    print('='*35)\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
